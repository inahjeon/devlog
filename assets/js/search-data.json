{
  
    
        "post0": {
            "title": "코로나로 인해 재택근무를 하며 느낀점",
            "content": "코로나 19 사태로 인해 갑작스럽게 이번주 화요일부터 다음주까지 전사 재택근무를 하게 되었다. 재택근무를 처음 해보기도 하고 갑작스럽게 결정되어 약간의 걱정 반, 기대 반으로 재택근무를 시작하게 되었다. . 이번주는 아직 첫 주라 그런지 대체로 만족스러웠던 것 같다. 😀 . 앞으로 회사에서도 점차 리모트 근무를 도입할 계획도 가지고 있어, 이번주 재택근무를 해보면서 느꼈던 점들을 잘 기록해두기로 했다. . 재택근무를 하며 달라진 점들 . 1. 출/퇴근 . 재택근무의 장점은 뭐니뭐니해도 출퇴근 시간이 사라진다는 게 가장 큰 장점인 것 같다. . 리모트 근무 시 출퇴근 관리는 출퇴근 관리 앱을 통해서 이루어진다. 현재 우리 회사에서는 출퇴근, 업무 시간, 야간 근무 요청 등은 출/퇴근 관리 앱인 시프티를 사용하고 있다. 시프티앱을 켜서 출근하기 / 퇴근하기 버튼을 누르면 출퇴근 표시가 되는 방식이다. 이건 사무실에 출근할 때도 동일하다. . 장점 👍: 돈과 시간이 절약된다. . 1초 만에 출/퇴근이 가능하다. 집에서 회사까지 door-to-door 로 대략 40분정도 걸리고, 출근 준비하는데 대략 40분 정도 걸리는데, 출/퇴근이 없어지니 하루 2시간 정도가 절약된다. | 점심을 집에서 먹을 수 있어 돈과 시간이 절약된다. 집에서 요리 해먹는 걸 좋아해서 이 부분은 행복감 200% 다. | . 단점 👎: 딱히 없다. . 아래 단점들은 현재 사용하고 있는 출/퇴근 앱의 문제이다. . 앱에서 출퇴근을 따로 요청하는 작업이 은근 귀찮다. 정책 상 회사 와이파이가 아닌 경우 출퇴근 요청을 따로 해야하는데, 요청 란에 필수로 의미 없는 출퇴근 사유도 써야 한다. | 나는 출퇴근 요청만 하지만, 매번 팀원들의 출퇴근 요청을 승인하는 사람들은 매우 귀찮지 않을까 싶다. | . 2. 회의 방식 . 회의는 기본적으로 행아웃 화상 미팅으로 이루어진다. 회의를 잡을 때 캘린더에 참석자들을 초대하고, 화상 미팅을 활성화하여 행아웃 링크를 공유하면 끝이다. . 원래 잡혀있던 회의 및 면접 등은 행아웃 화상 미팅으로 대체하고, 매일 업무 체크를 위해 스탠드업 회의가 활성화 되었다. 오전에 팀끼리 15분 정도 간단히 진행할 업무, 어려운 점 등을 공유한다. 중간에 급하게 구두 논의가 필요할 때도 빠르게 미팅을 잡아서 논의할 수 있다. . 장점 👍: 장소에 구애받지 않고, 바로 조용한 곳에서 회의를 할 수 있음. . 회사에서 종종 회의실을 구할 수 없어 여러 사람이 사용하는 열린 공간에서 미팅하게 되었을 때 회의에 집중하기 어려웠던 적이 있는데, 지금은 행아웃으로 필요한 때 당장 조용한 곳에서 미팅이 가능히다. | 심지어 이번주에는 Tech 인원들이 모두 모여서 한 달간 성과를 공유하는 Tech all hands 행사가 있었는데, 행아웃을 통해 매우 원활하게 진행되었다. (채팅창에 깨알같은 드립들이 난무하며 지난 번 오프라인 행사 때 보다 더 분위기가 좋았다칸다.) | . 단점 👎: 슬랙을 자주 확인하게 됨. . 모든 커뮤니케이션을 슬랙으로 하고, 리모트를 할 때는 왠지 더 눈치가 보여서 바로 답장을 하지 않으면 안될 것 같아 슬랙을 너무 자주 확인하게 되는 것 같다. 집중하는 동안은 팀에 양해를 구하고 알림을 꺼두어야 겠다. | 기본적으로 화상 미팅을 하는데, 카메라에 집안 풍경(?)이 일부 잡히게 되어 신경쓰여서 청소를 자주하게 된다. | . 사실 이번주는 팀에서 개발한 기능을 배포일정이 잡혀있던 주였어서 재택근무를 한다는 소식을 들었을 때 살짝 멘붕하기도 했었다. 성공적으로 배포하기 위해서 팀원 간 의사소통, 손발이 착착 잘 맞아야 하는데, 재택근무를 하게되면 이번주에 무사히 배포를 할 수 있을까 하는 걱정이 있었다. . 리모트를 위한 업무 시스템과, 서비스 배포 환경이 잘 갖춰진 탓인지, 무사히 사내 배포를 완료할 수 있었다. 배포 이후 버그도 있었지만, 버그 픽스도 시간 하루 정도로 생각보다 빠르게 해결될 수 있었다. 다음주에도 유저에게 배포되는 실서비스 배포도 안정적으로 진행해 볼 수 있을 것 같다. . 3. 업무 환경 . 코로나 때문에 시행하는 재택 근무라 아쉽게도 카페 리모트는 안되고 집에서만 일할 수 있다. . 업무 환경은 확실히 기존에 비해 아쉬운 점이 많은 것 같다. 사무실의 드넓은 책상과 편한 의자, 모니터, 커피, 간식 등이 다 사라졌다. . 장점 👍: 편안함. 스피커로 노래 틀기 . 노래 들을 때 에어팟을 장시간 끼고 있으면 귀가 아픈데, 집에서는 블루투스 스피커로 노래를 막 재생할 수 있어서 좋다. | 원래 집순이라 그런지 밖에 나가면 지하철 타면서 기빨리고 에너지가 쭉쭉 빠지는 스타일인데, 집 안에서 근무하니 매우 편안하고 쾌적하게 일했다. | . 단점 👎: 사무실의 그 훌륭한 장비들을 사용하지 못한다. 그리고 커피가 없다. . 이번 주는 거의 복층 2층 책상에서 양반다리를 하고 일 했었는데, 몇 시간 앉아서 일하다보면 다리가 저려오고 허리가 아팠다. 일할 때는 확실히 편한 의자와 책상이 필요한 것 같다. | 일할 때 커피를 내내 달고 있어서 하루에 보통 커피를 3잔 정도 마시는데, 집에는 커피가 없어 금방 금단 증상이 왔다. 얼른 집 앞 스타벅스에 커피를 사러 갔다 왔다. | . 4. 업무 효율 . 일단 무엇보다 업무 집중도는 2배 이상 올라간 것 같다. 그리고 회사에 출근했을 때는 출근해서 사무실에 있으니까 일 인정! 이런 느낌인데, 집에 있으니 조금이라도 쉬면 안될 것 같다는 생각에 오히려 눈치보여서 중간에 덜 쉬고 더 일하게 되었던 것 같다. . 장점 👍: 몰입이 잘 된다. . 사무실보다 몰입이 매우 잘 된다. 사무실에서는 여러 사람들이 움직이고, 말을 걸게되는 데, 혼자 집안에서 있으니 조용해서 일에 몰입이 잘되어 평소보다 빠르게 일을 쳐냈던 것 같다. | 사무실에서는 누군가 말을 걸면 그 즉시 집중이 깨진다. 리모트를 할 때는 슬랙에서 누군가 말을 걸더라도 내가 하던 업무를 마무리하고 볼 수 있어서 몰입을 깨는 상황이 적은 것 같다. | 집에서 일하는 습관이 드니, 주말에도 좀 더 부지런해진 것(?) 같다. | . 단점 👎: 몰입이 너무 잘 된다. (?) . 예전에는 집과 회사가 분리되어 퇴근하고 집에오면 푹 퍼져서 쉬었는데, 지금은 퇴근시간을 넘기고도 자연스럽게(?) 계속 일하게 되는 것 같다. 지금은 집 == 회사가 되어버려 의식적인 출/퇴근 구분이 필요할 것 같다. | . 5. 건강 . 아직은 그럭저럭 괜찮은 컨디션 유지중. . 장점 👍: 피로감이 덜하다. 건강한 식사! . 이번 주는 배포가 있어 어쩔 수 없이 거의 내내 밤 11시~12시 까지 야근을 했다. 그래도 출퇴근 시간이 없다보니 잠은 충분히 잘 수 있었어서, 평소 야근 했던 날에 비해 피로감이 덜하고 컨디션이 좋았다. | 밥먹기 귀찮으면 편의점 김밥, 샌드위치 등으로 간단히 때우는 때가 많았는데, 점심/저녁 식사로 집밥을 먹게 된 점은 좋다. | . 단점 👎: 작은 외로움과 갑갑함. 그리고 허리 아픔. . 원래 외로움을 잘 타지 않는 성격인데, 다른 사람과 말할 일이 줄어들다보니 살짝 외로움이 생긴다. | 집안에만 갇혀 있으니 갑갑하다. 이건 코로나 사태가 진정되면 해소될 것 같다. | 집의 책상과 의자가 그리 좋지 않아 장시간 컴퓨터로 일하면 허리가 아프다. 집이 좁아 새로 구매하기는 어려우므로 의식적인 바른 자세와 틈틈히 스트레칭하는 것으로 이겨내야겠다. 💪 | . 다음주를 위한 업무 환경 개선 . 코로나 사태가 얼마나 지속될 지 모르겠다. 일단 우리 회사는 다음주까지 재택근무를 시행하기로 했으니, 다음주 쾌적한 근무를 위해 약간의 노력으로 업무 환경을 개선해보았다. . 약간의 리모델링(?)을 통해 업무 공간을 1층으로만 한정하도록 분리했다. 2층은 온전히 쉬는 공간으로 남겨두어야지 🤐 | 일하기 편한 책상과 의자는 조그만 집에 둘 데가 없어서 못샀다. 대신 일하는 책상을 높이가 낮아 허리 아팠던 바 테이블 대신 원래 집에 딸려 있던 좀 더 높이가 높은 책상으로 바꿨다. 내년에 투룸으로 이사하면 새로 사야겠다. 😞 | 이번 기회에 찜해두었던 네스프레소 커피 머신과 스타벅스 캡슐(개인적으로 Pike Place 맛이 제일 맛있는 것 같다)을 구매했다! 😎 | 퇴근과 동시에 슬랙 끄고 노트북 덮기. 그 외 시간에는 개인 맥북을 사용하도록 서랍장에 쳐박혀있던 개인 맥북을 부활 시켰다. 👩‍💻 | 개인적 취향으로 작은 맥북 화면에서 코딩할 때 집중이 잘 되기도 해서 모니터는 굳이 구매하지 않았다. 🙃 | . 첫 주 재택근무를 해 본 결과, 단점들보다는 압도적인 장점(시간, 집중)들이 많아 아직은 재택근무에 대해서 만족스럽다. . 외로움 및 갑갑함 해소를 위해 딱 하루 정도는 사무실에 출근해도 괜찮을 것 같다. 회사의 다른 분들은 이번 주 재택근무를 해보면서 어떻게 느꼈는지도 궁금하다. . +) 그리고 뒤늦게 올려보는 깨알같은 회사 홍보 . 코로나에도 당황하지않고 하루만에 스무스한 재택근무 &amp; 리모트 근무가 가능한 뱅크샐러드 ! 에 지원하실 때 추천인 코드는 @전인아로 해주시면 채용보상금 반띵해드립ㄴ… .",
            "url": "https://inahjeon.github.io/devlog/diary/2020/03/01/work-from-home.html",
            "relUrl": "/diary/2020/03/01/work-from-home.html",
            "date": " • Mar 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "글또 4기를 시작하며",
            "content": "글또 4기를 시작하며. . 지난 글또 회고 . 재작년 11월 부터해서 2, 3기 글또에 참여했고, 덕분에 꾸준히 글쓰기를 하는 습관이 생겼습니다 :) . 작년에는 총 21편의 글을 썼고, 거의 매달 한 두편의 글을 꼭 작성했어서 개인적으로 매우 뿌듯했습니다. 매번 글 쓰기 주제를 고민하면서 강제로 새로운 주제에 대해서 공부하고 성장한 점도 좋았습니다. . 2기에 비해서 3기 때는 스터디 모임등 글또 내 소모임도 생기고 중간에 오프라인 모임도 있어서 사람들끼리 더 화기애애했던 것 같습니다. 이번 4기에도 오프라인 모임을 많이 하고 사람들끼리도 더 친해졌으면 좋겠습니다. (지금은 코로나 때문에 ㅠㅠ) 글또 시스템이 매 기수 마다 점점 발전하는 것도 재밌네요. . 4기 글쓰기 계획 . 예치금 모두 돌려받기 . 항상 세우는 계획이지만 2기, 3기 모두 실패했습니다. 이번에는 꼭…! . 개발 / 비개발글로 매주 글쓰기 (개인적 목표) . 올해 계획에도 올렸지만, 작년보다 2배로 글 더 쓰기를 목표로 잡았습니다. 그래서 이번에는 매주 글쓰기를 해보려고 하는데, 한 주는 기존과 동일한 데이터 / 개발 관련 글, 한 주는 독후감, 어떤 주제에 대한 개인적인 생각 등 자유 주제의 짧은 글을 써보려고 합니다. . 회사일도 그렇고, 글 쓰는 주제도 데이터 쪽이 많다보니 너무 매몰된 것 같아, 스스로 좀 지루해지는 감이 있어서 다양한 주제로 제한없이 글을 써보면 재밌을 것 같습니다. . 사이드 프로젝트 1개 이상 진행하고 글쓰기 . 2기, 3기를 할 때도 역시나 매번 진행해보려 마음먹었던 재밌는 사이드 프로젝트 진행하고 글쓰기. . 사이드 프로젝트 주제는 아직 아이디에이션 중입니다. . 몇 개월째 벼르고 있는 개인 블로그 직접 개발하기 | 관심있는 주제 뉴스/페북글/소식 스크래핑해서 요약해주는 봇 만들기 | 가상화폐 거래소 API 활용해서 시세 예측하고 자동매매 봇 만들기 (생계탈출 가즈아!) | . 매일 조금씩 글쓰기 . 2주마다 1편씩 쓰긴 했지만, 사실 2주 내내 쓰지는 않고 보통 하루, 몇시간 정도만 집중해서 글을 썼던 적이 많았습니다. 그러다보니 빠르게 글을 쓸 수 있는 주제를 선택하게되고, 좀 더 퀄리티 있는 글을 못 썼던 것 같아서 아쉬움이 많이 남았습니다. . 이번에는 주제를 미리 생각하고, 매일매일 꾸준히 글을 써보려합니다. 저녁에 1시간 정도씩만 투자해서 1일 1 commit에 도전! . 이번 글또 4기 에서는… . 4기 들어서 글또 노션 페이지가 생겼어요 -&gt; 글또 😀 글또 모임의 규칙, 지난 활동들, 글쓰기 자료, 로고 등에 대해 잘 소개되어있어요. . | 글또 로고(3기 어느 금손님이 만드신)도 생겼어요. 짱이쁨. 😍 . | . . 글또 4기에서는 필요한 자동화 툴을 개발하는 자동화팀과 내부 오프라인 모임, 행사를 기획하는 운영팀이 생겼습니다. 🛠 4기 때는 더 재미있을 것 같네요! | . 깨알자랑 .",
            "url": "https://inahjeon.github.io/devlog/diary/2020/03/01/start-geultto4.html",
            "relUrl": "/diary/2020/03/01/start-geultto4.html",
            "date": " • Mar 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://inahjeon.github.io/devlog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Hidden Technical Debt in Machine Learning Systems",
            "content": "ML 서비스를 Production 에 적용하기 시작하면서, CD4ML, ML Ops 에도 관심이 많이 생겨서 차근차근 공부해보려고 합니다. . 나온지 꽤 되긴 했는데 MLOps 관련 많은 article 들에서 읽어볼 것을 추천하고 있는, Hidden Technical Debt in Machine Learning Systems (2015)(ML system 에서의 기술 부채에 대해 Google 에서 작성한 유명한 논문) 를 찬찬히 읽어보고 적당히 의역 및 요약해보았습니다. (정확한 내용은 원문을 참고해주세요.) . 영어가 짧아서 논문을 읽고 저만의 언어(?)로 번역해보는 일은 시간이 꽤 걸렸지만, 그냥 슥 읽어보는 것보다 더 꼼꼼하고 깊게 읽어 보게 되어서 공부하는데 도움은 많이 되었습니다. :) . 1. Introduction - ML 시스템에서의 기술적 부채 . Technical Debt (기술 부채) 란? . Ward Cunningham 이 제시한 비유적 표현으로, software engineering 관점에서 기존의 시스템에 축적되어서 새로운 기능을 추가하거나, 변경 / 유지보수 할 때 이를 어렵게 만드는 요소들로, 금융에서의 부채와 비슷한 개념으로 바라볼 수 있습니다. . 기술 부채는 대략 다음의 한 짤로 요약됩니다… . . Software engineering에서와 마찬가지로 라이브 시스템에 ML 을 적용하기 시작하면서 ML system 에서도 개발과 배포에 걸리는 시간은 상대적으로 빠르고 저렴하지만, 이를 유지보수하는 것은 어렵고 비용이 많이 들게되는 현상이 광범위하게 발생하고 있습니다. . 특히 ML system 에서는 전통적인 코드를 유지 보수하는 문제에 추가로 ML에 특정된 이슈들로 인한 기술적 부채들도 발생합니다. 그런데 이런 기술 부채들은 코드 레벨보다는 시스템 레벨에 존재하기 때문에 감지하기 어렵습니다. ML 시스템에서 코드 외 시스템에서 흘러가는 데이터 때문에 전통적인 추상화 경계(abstraction boundaries)가 깨지기 쉽고, 코드 레벨의 기술부채를 갚는 전형적인 방법들 (e.g. 리팩토링, 테스트코드 작성 등)로는 ML system의 부채를 해결하는데 충분하지 않습니다. . 이 논문에서는 ML 기술 부채가 빠르게 축적될 수 있는 ML system 내의 시스템 레벨의 상호작용과 인터페이스에 집중하여 ML 기술 부채를 발생시키는 요소들에 대해 제시하고 있습니다. . 2. Complex Models Erode Boundaries . 전통적인 software engineering에서는 캡슐화(encapsulation)와 모듈 디자인(modular design)를 사용한 강한 추상화 경계(abstraction boundaries)를 통해 유지보수가 가능한 코드를 만들고, 독립된 작은 변화와 개선을 만들기 쉽게 합니다. 엄격한 추상화 경계는 인풋과 아웃풋에 대한 일관성을 가지게 합니다. . 그러나 ML 시스템에서는 설계했던 동작이 외부의 데이터 의존성을 제외하고 소프트웨어 로직만으로는 효과적으로 표현 될 수 없기 때문에 엄격한 추상화 경계를 적용하기 어렵고, 이로 인해 심각한 기술 부채를 만들 수 있습니다. . Entanglement . ML 시스템에서는 입력값들이 서로 복잡하게 얽혀있기 때문에, 독립적으로 개선 하기 어렵습니다. 예를 들어 x_1, … , x_n 의 feature를 사용하는 모델에서 만약 x_1 의 입력 분포를 바꾸는 경우, 나머지 모든 n-1개의 feature 들의 가중치, 사용 여부 등이 모두 바뀌게 됩니다. 새로운 n+1 번째 feature 를 추가하거나, 특정 feature x_j 를 제거하는 경우도 마찬가지 입니다. . 이 논문에서는 이러한 경우를 CACE (Changing Anything Changes Everything) 원칙으로 명명했습니다. CACE 원칙은 입력값에 대해서 뿐아니라, 하이퍼파라미터, 학습에 필요한 설정들, 샘플링 방법등 ML system 의 모든 가능한 작업에 적용될 수 있습니다. . 이에 대한 해결책으로 한 가지 방법은 모델을 독립적으로 구성하고 ensemble한 결과를 제공하는 것입니다. Ensemble 은 문제가 sub task 들로 자연스럽게 잘 나뉘어질 수 있을 때 적용하기 좋고, 많은 케이스에서 ensemble은 잘 동작합니다. 그렇지만 ensemble 역시 모델이 얽혀있기 때문에, 개별 모델의 정확도 개선이 전체적인 시스템의 정확도를 낮추는 결과를 초래할 수 도 있습니다. . 두 번째 가능한 전략으로는 발생할 수 있는 예측에 대한 변화를 감지하는 것에 주력하는 것입니다. 고차원의 정보를 시각화 할 수 있는 도구를 활용하여, 영향들을 다양한 차원에서 나누어서 분석해볼 수 있습니다. . Correction Cascades . 문제 A 를 풀기 위한 모델 m_a 가 존재하고 문제 A와 약간만 다른 문제 A’에 대한 모델이 필요할 때, 빠른 해결책으로 모델 m_a의 결과를 입력으로 사용하고 일부분만 재학습하여 m_a&#39;를 만들 수 있습니다. . 그러나, 이렇게 만들어진 모델의 경우 모델 m_a에 대한 의존성을 가지고 있어 차후에 모델을 개선하기 위해 분석하는 비용이 많이 들게됩니다. 그리고 m_a’ 모델을 활용하여 다시 문제 A’‘에 적용하는 식으로 correction 모델들이 쌓여서 비용이 증가하게 됩니다. 또한 이런 correction cascade 들은 deadlock 을 발생시킬 수 있는데, 개별 모델의 정확도 개선이 전체 시스템 레벨의 정확도를 오히려 낮추게 될 수 있습니다. 이에 대해서는 feature 를 추가하여 직접적으로 correction 을 학습시키거나, 또 다른 모델 A&#39;를 직접 개발하는 비용을 들여서 완화할 수 있습니다. . 대략 문제 별로 그냥 모델을 따로 만들어라 하는 얘기 같음. . Undeclared Consumers . SE에서도 물론 동일하게 발생하는 문제이지만, batch prediction 같이 결과값이 파일로 저장되어 사용될 수 있는 ML system의 경우 특히 주의해야 할 이슈인 것 같다. . 모델 m_a를 통한 예측 결과값이 다른 시스템에서 쉽게 접근가능한 경우, 접근 제한이 없으면 선언하지 않은 소비자들이 모델의 결과값을 사용하게 되는 경우가 발생할 수 있습니다. 전통적인 software engineering 에서는 이 문제를 visibility debt 라고 부릅니다. . 선언하지 않은 consumer들은 모델 m_a와 특정 부분과의 숨겨진 밀접한 결합을 만들기 때문에, 작게는 (시스템 유지보수에 대한) 비용을 높이고, 최악의 경우는 위험할 수도 있습니다. 모델 m_a의 변화가 다른 어떤 부분에 의도하지 않은 영향을 끼칠 수 있습니다. 일반적으로 이러한 밀접한 결합은 비용을 증가시키고, 개선점을 m_a 에 적용하기 어렵게 만듭니다. 또한 이런 consumer 들은 4장에서 설명하는 숨겨진 피드백 루프를 만들 수도 있습니다. . Undeclared consumer 들은 엄격한 service-level agreements (SLAs) 같은 강력한 가드 가드 없이는 감지하기 어렵습니다. . 3. Data Dependencies Cost More than Code Dependencies . 전통적인 소프트웨어 엔지니어링에서 코드를 복잡하게 하고 기술 부채를 발생시키는 주요 요소로 dependency dept를 꼽고 있습니다. ML 시스템에서도 이와 비슷하게 기술 부채를 발생시키면서 감지하는 것도 더 어려운 data dependency 가 존재합니다. 코드 의존성의 경우 compiler, linker 등 static analysis 를 통해 확인할 수 있습니다. 데이터 의존성의 경우도 의존성을 파악할 수 있는 도구가 없다면, 대규모의 해결하기 어려운 데이터 의존성을 쌓기 쉽습니다. . Unstable Data Dependencies . 모델에서 다른 시스템에서 만든 출력값을 모델의 입력값으로 사용하게 되는 경우가 많은데, 이런 경우 시간의 흐름에 따라 질적으로, 양적으로 변화하는 unstable 한 입력값이 될 수 있습니다. 예를 들어 다른 모델의 출력 값을 입력값으로 사용하는 경우 해당 모델이 변경됨에 따라 출력값이 암묵적으로 바뀔 수 있습니다. 이러한 입력값의 변화는 이를 사용하는 시스템에서 감지하기 어렵기 때문에 위험합니다. . 이에 대한 한 가지 해결 전략으로는 데이터에 대해 versioned copy 를 만들어 데이터 의존성을 해결하는 방법이 있습니다. . 예전에 데이터 버전 관리 툴 DVC 사용해보고 작성한 글 참고 . Underutilized Data Dependencies . 코드에서 underutilized dependencies는 사용되지 않는 package들을 의미합니다. 이와 유사하게, underutilized data dependencies 는 모델에서의 이득이 거의 없는 입력값들을 의미합니다. 이러한 입력값들은 ML system 에서 불필요하게 변화에 취약하게 만들고, 치명적으로 만들 수 있습니다. . Underutilized Data Dependencies는 다음의 경우에서 발생할 수 있습니다. . Legacy Features: 가장 흔한 경우로써, feature F가 모델의 개발단계에서 포함되었다가 시간이 지나면서 새로운 feature에 의해 불필요하게 된 경우입니다. | Bundled Features: 보통 마감 일정에 쫓겨서 모든 feature 를 bundle로 모델에 한꺼번에 추가하는 경우 발생하는데, 이때 특정 feauture 들은 적은 효과가 있거나 가치가 없는 feature 일 수 있습니다. | ϵ-Features: 모델의 정확도를 개선하기 위해 아주 작은 효과를 가지지만, 시스템의 복잡도를 크게 증가시킬 수 있는 feature를 포함시키려는 경우가 있습니다. | Correlated Featuress: 두 feature 가 강한 상관관계를 보이지만, 한 가지 feature가 직접적인 원인이 되는 경우가 많습니다. 많은 ML 방법들에서 이러한 특성을 감지하는 것은 어렵고, 두 feature를 동일하게 평가하거나 심지어는 직접적인 원인이 아닌 feature 를 택하는 경우가 있습니다. | . Underutilized Data Dependencies 는 leave-one-feature-out 평가를 통해 감지할 수 있고, 이러한 평가는 정기적으로 수행하고 불필요한 feature 들을 제거해야 합니다. . Static Analysis of Data Dependencies . 전통적인 코드에서는 컴파일러와 빌드 시스템에서 의존성 그래프에 대해 정적 검사를 수행할 수 있습니다. 데이터 의존성에 대한 정적 분석 도구는 자동화된 feature 관리 같은 도구를 예를 들 수 있고, 아직 그리 많이 있지는 않습니다. . 4. Feedback Loops . Live ML 시스템에서 한 가지 중요한 특성은 시간이 흐름에 따라 시스템의 동작이 다시 모델에 영향을 끼치게 되는 것입니다. 이러한 특성은 모델이 배포되기 전에는 시스템이 어떻게 동작할 지 예측하기 어려운 analysys debt 를 발생시킬 수 있습니다. . 모델이 데이터에 영향을 끼치고, 다시 그 데이터가 모델에 영향을 끼치는 건가 봉가 . Direct Feedback Loops: 어떤 모델이 해당 모델에 사용할 미래의 학습 데이터를 선택하는데 직접적으로 영향을 끼질 수 있습니다. 이러한 문제는 supervised algorithm 에서 보통 bandit algorithm 을 통해 해결합니다. | . 뭔 말인지 모르겠음. . Hidden Feedback Loops: 직접적인 피드백 루프는 분석하기 어렵지만, 적어도 통계학적으로 해결책을 찾아볼 수는 있습니다. 더욱 더 어려운 케이스는 실세계에서 간접적으로 서로 영향을 주고 있는 시스템들에서 발생하는 숨어있는 피드백 루프입니다. 예를 들어 웹 페이지의 facet을 결정하는 두 시스템 (보여줄 제품을 선택하는 시스템과 제품에 관련된 리뷰를 선택하는 시스템) 에서 한 시스템에서의 개선 사항은 다른 시스템에 영향 (유저가 특정 제품을 더 많이/적게 클릭하는 등)을 끼칠 수 있습니다. 이러한 숨은 피드백 루프가 완전히 독립된 시스템 사이에서 발생한다는 사실을 유념해야 합니다. | . 5. ML-System Anti-Patterns . . Real-world ML system 에서 실제 학습이나 예측에 사용되는 “ML 코드”가 차지하는 비중은 매우 적고, ML system 을 위해 요구되는 인프라는 매우 복잡합니다. . 매우 진리이다. 한 백 번쯤 강조해야 한다. . 아래에서는 이러한 ML 시스템에서 반드시 피하거나, 리팩토링 되어야하는 몇 가지 system-design anti-pattern 들에 대해 다룹니다. . Glue Code . ML 연구자들은 독립된 package 형식으로 솔루션을 개발하려는 경향이 있고, 오픈소스 형태로 다양하게 존재하고 있습니다. 이러한 package들을 사용하게 될 경우, 보통 이 package를 사용하기 위해 데이터 입출력을 위한 대량의 지원 코드를 작성하게 됨으로써 glue code system design pattern 으로 빠지게 합니다. Glue code 는 특정 package를 사용하도록 시스템을 종속시키게 되어 장기적으로는 비용이 큽니다. . Glue code 문제를 해결하기 위한 한 가지 전략은 이러한 black-box package 를 common API로 wrapping 하는 방법 이 있습니다. Common api 로 wrapping 하면 package의 변화에 따른 비용을 줄이고, 재사용성을 더 높일 수 있습니다. . Pipeline Jungles . Glue code의 특수한 케이스로, 데이터 전처리 단계에서 pipeline jungles 가 발생할 수 있습니다. Pipeline jungles 는 새로운 입력값이 식별되거나, 새로운 데이터 소스가 추가되면서 점차 발생하게 되는데, 여기에 주의를 기울이지 않으면 중간 파일 생성, join, 샘플링 등으로 점철된 데이터 전처리 정글을 만들게 됩니다. . 데이터 파이프라인들을 관리하고, 에러를 감지하고, 장애로부터 복구하는 작업은 모두 어렵고 비용이 많이 듭니다. 파이프라인들을 테스트할 때에도 보통은 비용이 많이 드는 통합 테스트까지 필요로 하는 경우가 많습니다. 이런 모든 것들이 기술적인 부채를 쌓고, 혁신을 만드는 비용을 더 크게 만듭니다. . 데이터를 수집하고 feature 를 추출하는 데 전체적으로 잘 생각해야만 Pipeline jungles 을 피할 수 있습니다. 파이프라인을 재설계하고 백지상태에서 출발하는 방식은 실제로 현재 발생중인 비용을 줄이고, 혁신을 가속화하는데 효과적인 방식입니다. . Glue code 와 pipeline jungle 현상은 research 와 engineering 의 역할이 분리되어 있다는 점이 근본 원인일 수 있습니다. Researcher 와 engineer 들이 같은 팀에서 하이브리드로 연구하는 방식은 이러한 문제를 해결하는데 도움이 될 수 있습니다. . Dead Experimental Codepaths . 일반적으로 Glue code나 pipeline jungle 로 인해서 발생하는 결과는 단기적으로 main production code의 조건분 분기로 실험 코드를 구현하여 실험을 진행하게 되는 것입니다. 어떤 개별 변동사항에 대해 이러한 접근방식은 전체 인프라를 건드리지 않기 때문에 상대적으로 적은 비용이 듭니다. 그렇지만, 시간이 흐르면서 이러한 분기 코드들은 이전 버전과의 호환성을 유지하는데 어려움을 증가시키거나, cyclomatic complexity 를 기하급수적으로 증가시킵니다. . 일반적인 소프트웨어 개발의 dead flag 와 같이 주기적으로 이러한 실험용 branch 들을 제거해야합니다. 실험들 중 매우 적은 실험 셋들만 실제로 사용되고, 대부분은 한번 테스트하고 버려지게 됩니다. . 단기적으로 실험한 코드 branch들 째깍째깍 잘 정리해야함. . Abstraction Debt . 위에서 언급한 이슈들은 ML system 을 지원하기 위한 강력한 abstraction 이 없다는 사실을 강조하고 있습니다. 데이터의 흐름이나 모델, 예측을 설명하기 위한 적합한 인터페이스는 무엇일까요? . 논문에서 MR, Parameter-server 방식을 언급하긴 했지만, 특별한 해결책은 아직 없는 것 같음. . Common Smells . 소프트웨어 엔지니어링에서 흔히 특정 component 나 시스템에 존재하는 문제를 smell 이라고 표현합니다. 본 논문에서는 몇 가지 ML system 에서의 smell 들을 정의했습니다. . Plain-Old-Data Type Smell: ML system에서 사용되고 생산되는 rich 정보들은 raw float 이나 interger와 같은 타입으로 인코딩되어 있습니다. 모델에서 이 정보가 의미하는 것이 무엇인지 (decision threshold 인지, log 승수인지) 명확하게 알아야하고, 데이터가 어떻게 생산되고 소비되어야하는지 알아야합니다. . | Multiple-Language Smell: 특정 프로그래밍 언어가 특정 라이브러리나 구문을 사용하기 편리할 경우, 시스템의 한 부분만 다른 언어를 사용하려는 경우가 있습니다. 그러나 여러 개의 언어를 사용하는 것은 효과적으로 테스트 하는 비용을 증가시키고, service ownership을 넘기기 어렵게 만듭니다. . | Prototype Smell: 새로운 아이디어를 테스트 하기위해 작은 스케일로 프로토타이핑하는 것은 편리합니다. 그렇지만 프로토타이핑 환경에 자주 의존하는 것은 full-scale 시스템이 변화에 대응하기 어렵고, 더 나은 추상화나 인터페이스가 필요하다는 것을 나타내는 지표일 수 있습니다. 프로토타이핑을 위한 환경을 유지하는 것은 비용이 들고, 시간의 압박에 쫓겨 프로토타이핑 시스템을 production 시스템에 적용하려는 시도를 부추길 수도 있습니다. 게다가 작은 스케일에서의 실험은 실제 세계의 현실을 제대로 반영하지 못하는 경우가 많습니다. . | . 6. Configuration Debt . 기술 부채가 축적될 수 있는 또 하나의 영역은 ML system 의 configuration 입니다. 대부분 대규모의 시스템에서 다양한 범위의 설정가능한 옵션 값들이 존재합니다. 이러한 설정값들은 특정 feature들에 대한 사용 여부, 데이터를 어떻게 선택할 지, 다양한 특정 알고리즘에 특화된 설정값들, 전처리 또는 후처리 등에 대한 값들을 포함합니다. . 연구자들이나 엔지니어 모두 이런 설정값을 관리하는 것은 보통 나중일로 생각하는 경향이 있고, 설정값을 검증하거나 테스트하는 작업은 중요하지 않게 보일 수도 있습니다. 활발하게 개발되고 있는 성숙한 시스템에서는 설정을 위한 코드의 줄 수가, 실제 구동하는 코드의 줄 수보다 훨씬 많은 경우도 있습니다. 각각의 설정 코드들에는 실수들이 잠재적으로 포함되어 있을 수 있습니다. . 아래는 논문에서 제시하는 좋은 configuration system 에 대한 원칙들 입니다. . 설정 값을 이전 설정값에서 부터 약간의 변경만으로 쉽게 지정할 수 있어야 합니다. | 수동적인 에러, 생략, 누락을 만들기 어려운 구조여야합니다. | 두 모델 간 설정값 차이를 쉽게 시각적으로 볼 수 있어야 합니다. | 설정값이 자동으로 assert 되고, 기본적인 값들 (사용하는 feature의 개수, transitive closure of data dependencies 등)이 자동으로 검증되어야 합니다. | 사용되지 않는 불필요한 설정들을 감지할 수 있어야 합니다. | 설정값은 전체 코드 리뷰 과정에서 리뷰되고, repository 에 merge되어야 합니다. | . 7. Dealing with Changes in the External World . ML system 이 매력적인 한 가지 이유는 외부 세계와 직접적으로 상호작용한다는 것입니다. 경험적으로, 외부 세계는 안정적인 경우가 매우 드물기 때문에, 외부 환경에 대한 변화로 인한 유지보수 비용이 지속적으로 발생합니다. . Fixed Thresholds in Dynamic Systems. . ML system에서 스팸 메일인지, 정상 메일인지 예측하여 표시하거나, 특정 광고를 유저에게 표시할 지 말지 등 특정 ML 모델이 어떤 동작을 수행하게 하기 위해 decision threshold 값 를 선택하는 경우가 많습니다. 한 가지 전통적인 방법은 가능한 임계값 범위 내에서 precision / recall 과 같은 tradeoffs 를 고려하여 하나를 택하는 방법이 있습니다. . 그런데 이러한 임계값들을 대부분 수동적으로 설정하는 경우가 많습니다. 만약 모델이 새로운 데이터를 입력으로 받게되는 경우, 기존의 수동적으로 설정한 임계값은 유효하지 않을 수 있습니다. 또한, 많은 모델들의 임계값들을 수동으로 변경하는 일은 시간이 오래걸리고 다루기 힘든 작업입니다. . 이러한 문제를 해결하기 위한 한 가지 전략은, 검증 데이터 셋을 통해 임계값을 학습하게 하는 방식 을 사용할 수 있습니다. . Monitoring and Testing. . 동작하고 있는 시스템에서 개별 component 들에 대해 unit test 및 end-to-end 테스트를 수행하는 것은 중요한 작업이지만, 변화하는 실 세계 환경에서 이러한 테스트들은 해당 시스템이 의도한 대로 잘 동작하고 있다는 것을 증명하기에는 불충분할 수 있습니다. . 장기적인 시스템 안정성에 대한 관점에서 자동화된 실시간 모니터링은 매우 중요합니다. 중요한 것은 무엇을 모니터링 할 것인가? 입니다. ML system 에서는 테스트 해야하는 값이 명확하게 주어지지 않는 경우가 많습니다. 본 논문에서는 모니터링 해야하는 시작점으로 아래의 요소들에 대해 제시합니다. . Prediction Bias: ML system 이 의도한 대로 잘 동작하고 있다면, 시스템에서 예측하는 label 의 분포가 관측한 label 의 분포와 동일해야합니다. Prediction bias 를 측정하는 방안은 갑작스럽게 변경되는 외부의 변화를 감지하는데 유용하게 쓸 수 있습니다. 다양한 차원에서 prediction bias를 나눔으로써, 이슈를 빠르게 분리하고, 자동화된 알림으로 사용할 수 있습니다. . | Action Limits: ML system 에서 스팸 문자를 마킹하는 등의 특정 동작을 수행하는 경우, 동작에 대한 limit을 설정하는 방법은 sanity check 로 유용합니다. 만약 시스템에서 특정 동작에 대한 limit 을 넘어서는 경우, 자동 알림을 보내서 수동적으로 검사해볼 수 있습니다. . | Up-Stream Producers: ML system에서 입력 데이터를 다양한 up-stream producer 들로 부터 수급받게 되는 경우가 많습니다. 이러한 up-stream process 들은 ML system의 요구사항을 포함한 서비스 레벨의 목적을 달성할 수 있도록 철저하게 모니터링되고 테스트되어야 합니다. 이러한 up-stream 들의 failure에 대한 alert 또한 반드시 ML system의 control 전략에 포함되어야하고, 비슷하게 ML system 에서의 failure 역시 down-stream consumer 들에게 전파되어 down-stream consumer 들의 control 전략에 포함되어야 합니다. . | . 외부의 변화는 실시간으로 일어나기 때문에, 이에 대한 반응 또한 실시간으로 발생합니다. 사람이 직접 개입해서 대응하는 것이 보통이지만, 빠르게 해결되어야하는 문제에 대해서는 취약할 수 있습니다. 사람이 직접 개입하지 않고 자동으로 변화에 대해 대응할 수 있는 시스템을 만드는 것은 투자할만한 가치가 있습니다. . 그런 시스템을 만드는 게 가능한가? 단순한 조치는 취할 수 있을 듯 . 8. Other Areas of ML-related Debt . 그 밖에 ML 관련 기술 부채들에 대한 내용입니다. . Data Testing Debt. . 데이터가 ML system 의 코드를 변경시키는 경우, 해당 코드에 대해서 반드시 테스트하고, 입력 데이터 또한 테스트하는 것은 매우 당연합니다. 기본적인 sanity 체크 및 입력 값의 분포의 변화에 대해 모니터링하기 위한 잘 설계된 테스트 가 유용하게 쓰일 수 있습니다. . Reproducibility Debt. . Scientist 로써, 실험을 다시 수행하고 비슷한 결과물을 얻는 것은 중요합니다. 그러나 real-world ML system 에서 Randomized 알고리즘의 사용, 초기 조건에 따른 의존성 등의 문제로 strict reproducibility 를 가지도록 설계하는 것은 더욱 더 어렵습니다. . Process Management Debt. . 이 논문에 소개된 대부분의 사례들은 한 가지 모델을 유지보수 하기 위한 비용에 대해 말하고 있습니다. 그러나 성숙한 시스템의 경우는 수 십, 수 백개의 모델들이 동시에 실행됩니다. 이런 성숙한 시스템의 경우, 많은 비슷한 모델들의 congifuration 값들을 안전하게 자동적으로 변경하는 문제, 비즈니스 우선순위가 다른 모델등 사이에서 리소스를 할당하고 조율하는 문제, production 데이터 파이프라인의 흐름을 시각화하고 blocker 들을 감지하는 문제 등 넓은 범위에 걸쳐 해결해야 할 다양하고 중요한 문제들이 있습니다. 또한, 장애 시 복구를 위한 tool을 개발하는 것도 매우 중요합니다. . Cultural Debt. . ML research 와 engineering 사이에 큰 장벽이 있는 경우, 이러한 장벽은 장기적인 관점에서 생산성을 저하시킬 수 있습니다. 팀 차원에서 쓸모없는 feature 의 개수를 줄이고, 시스템의 복잡도를 낮추고, 재현성과 안정성을 개선하고, 시스템을 모니터링하는 것에 대해 보상하는 문화를 만드는 것이 중요 합니다. . 9. Conclusions: Measuring Debt and Paying it Off . 기술 부채는 유용한 표현이지만, 기술적 부채에 대해 측정할 수 있는 구체적인 metric 까지는 제시하고 있지 않습니다. 다음과 같은 질문들을 통해 기술적 부채가 얼마나 존재하는지 대략적으로 파악해 볼 수 있습니다. . 새로운 알고리즘적 접근이 얼마나 쉽게 full scale 에서 테스트 가능한가? | What is the transitive closure of all data dependencies? (무슨 말인지 모르겠음) | 시스템에 적용되는 새로운 변화에 대한 임팩트를 얼마나 정확하게 측정할 수 있는지? | 특정 모델이나 입력값에 대한 개선이 다른 것들의 성능을 저하시키는지? | 새로운 팀 멤버가 얼마나 빨리 업무에 적응할 수 있는가? | . 이 논문을 통해 유지보수 가능한 ML 시스템 개발을 위한 더 나은 추상화, 테스팅 기법, 디자인 패턴들이 나오길 바라고, 논문에서 제시하는 가장 중요한 시사점은 engineer 와 researcher 모두 기술 부채에 대해 경계하고 고민해야한다는 점 입니다. . 정확도(accuracy)에서 작은 개선점을 얻을 수 있는 특정 research 솔루션이 전체 시스템의 복잡도을 매우 높이는 경우는 좋은 practice 가 될 수 없습니다. . ML 관련된 기술 부채를 줄이기 위해서는 팀 문화를 바꾸는 차원의 특별한 노력이 필요합니다. 장기적으로 성공적인 ML팀이 되기 위해 이러한 노력에 대해 인식하고 우선순위화해서 업무를 진행하고, 이에 대해 보상하는 것이 중요합니다. .",
            "url": "https://inahjeon.github.io/devlog/ml%20/%20data%20engineering/2020/02/16/technical-dept-in-ml-systms.html",
            "relUrl": "/ml%20/%20data%20engineering/2020/02/16/technical-dept-in-ml-systms.html",
            "date": " • Feb 16, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "2020년 계획",
            "content": "2020년 새해 맞이 늦은 신년 계획 . Happy New Year, 2020 . 새해 되자 마자 유행하고 있는 지독한 A형 독감에 걸려서 병가쓰고 3일째 방구석에서 골골대고 있다. 이렇게 며칠 간 아파본 것도 오래만인 것 같다 (독감은 거의 10년만..). 가끔씩 병가쓰고 한 일주일만 쉬어보고 싶다고 입버릇처럼 말하던 게 있었는데, 실제로 아프고나니 쉬기는 커녕 아무것도 못하겠고 그냥 빨리 나았으면 싶다. 먹고 자고 누워만 있었는데 2kg가 빠졌다(…) . 한 이틀 크게 앓고 나니 조금 회복되어서 미루고 있었던 늦은 새해 계획을 작성해보았다. (새해목표 0번은 건강 챙기기..) . 1. 일찍 잠들기 + 칼퇴하기 . 11시 이전에 잠들기 . 집중해서 일하고 칼퇴하기 . 나도 아침에 일찍 못일어나고 밤늦게까지 깨어있는 저녁형 인간이다. 거기다 가끔 야근을 하게되면 늦게 집에 들어가고 새벽에 잠들고, 늦은 출근을 하고, 또 야근을 하고,…의 반복이라 생활 패턴이 깨진적이 많았다. 매번 아침에 일찍 일어나야지 알람을 맞춰보지만, 그 전날 늦게 잤으니 아무리 의지력이 높아도 일어나기 힘들다. . 올해부터는 그냥 무리하지 않고 칼퇴하고 일찍 잠드는 것을 목표로 삼았다. 일찍 잠들기만해도 그냥 일찍 잘 일어나진다. 새해 버프(?) 때문인지 모르겠지만, 한 며칠 동안 밤 11시에 자고 5시반에 일어나는 것을 시도해보았는데 성공적이었다. 계속 하다가 몸이 적응하면 9시 출근 / 6시 퇴근으로 바꿔야지. . 2. 꾸준히 운동하기 . 일주일에 운동하는 횟수 3번으로 늘리기 . 1년 간 운동 꾸준히 하기 . 술을 줄이기는 너무 어려워서 운동을 꾸준히 하기로 했다. 일주일에 2번 회당 6만원 하는 PT 수업은 돈이 아까우므로 어떻게든 잘 가게 되지만, 그 외 시간에 스스로 헬스장을 가진 않는다. 일주일 3회 운동이면 매우 도전적인 목표가 될 것 같다. 집에서 3분 거리 엎어지면 코 닿을 거리에 있는 헬스장인데, 막상 가면 열심히 하는데 가기가 너무 힘들다. 운동도 다른 사람이랑 내기하나 걸고 해야 잘 갈 것 같다. . 3. 꾸준히 글쓰기 . 글 40편 이상 작성하기 . 사이드 프로젝트 1개 이상 하기 . 작년에 이어 올해도 꾸준히 블로그에 글쓰기. 작년에는 총 21편 썼으니까 올해는 작년의 두 배정도만 쓰면 적당(?)할 것 같다. 2주마다 1편씩 쓰긴 했지만, 사실 2주 내내 쓰지는 않고 보통 하루, 몇시간 정도만 집중해서 글을 썼던 적이 많아서 좀 더 가능하지 않을까 싶다. . 글쓸 때 보통은 평소에 알고싶었던 주제를 공부한 내용이나, 새로운 ML, 데이터 분석 관련 라이브러리를 사용해본 내용, 컨퍼런스 다녀온 후기 등을 작성하곤 했었는데, 이번에는 작은 거라도 재밌는 프로젝트 하나를 진행해본 경험을 써보고 싶다. . 그 외 그냥 생각나는 하고 싶은 것들 . 아이패드로 코딩하기 | 투룸으로 이사하기. 복층은 계단이 위험하고 귀찮다. | 도로연수 받고 운전하기. 차를 사기 위한 큰 그림. | 개발 서적 외 책 읽기 | . 올해 계획은 이정도로만 세워두자. 올해는 또 올해의 새로운 일들이 많겠지. .",
            "url": "https://inahjeon.github.io/devlog/diary/2020/01/16/2020plan.html",
            "relUrl": "/diary/2020/01/16/2020plan.html",
            "date": " • Jan 16, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "2019년 회고",
            "content": "2019년의 마지막 날 올해를 돌이켜보았다. 올해도 아무것도 안하고 무난무난하게 한 해를 보냈던 것 같지만 그래도 많은 일들이 있었구나 싶다. . 바이바이 2019 . 1. 이사: 봉천역 to 증미역 . 회사가 여의도로 이사를 하게 되면서 ( 이사하고 돼지머리 화면에 띄워놓고 기념사진 찍음 ) 마침 나도 살던 곳이 계약 만료가 되어 이사를 하게 되었다. 비록 분명 두 달 전에 나간다고 했는데 집주인이 세입자 구할 때 까지 보증금을 못 빼준다고 으름장을 놓은 바람에 좀 짜증이 났지만. . 여의도 근처로 집을 알아보고 강서구로 오게되었는데, 일단 서울 중심부와 거리가 좀 있다보니 조건에 비해 가격이 저렴한 편이고 대부분 신축 건물이라서 매우 만족했다. 이번엔 대출받아서 전세로 계약했는데, 집주인(=은행)에게 바치는 월세가 이전보다 훨씬 줄어서 좋다. :) . 버킷리스트 중 하나였던 복층 오피스텔 살기 도 이룰 수 있어서 너무 좋았다. 지금은 계단 오르내리기가 귀찮아져서 다음엔 아마 투룸으로 구할 듯 싶다. . 2. 레이니스트: 4년째 다니고 있는 고인물 . 엊그제 12월 28일 입사일 기준 4년 째 회사를 다닌 날이었다. 입사 4주년 축하 공지가 올라와서 오늘이구나 했다. 회사가 7년정도 밖에 안되기도 했고, 그 동안 수 많은 사람들이 들어오고 나가고 하느라 오래된(?) 분들이 많지 않아서 고인물로 불리고 있다. . 나도 생각보다 오래다니고 있다는 생각이 든다. 보통 주변의 다른 개발자들이 2년마다 이직하는 걸 보면. 나도 회사를 나가고 다른 회사를 경험해보고 싶었던 적이 없진 않았지만 (아니 종종), 그럼에도 계속 다니고 있는 건 여기도 매번 새로운 모습으로 변화하고 있어서 그런 것 같다. . 항상 새로운 시도를 하고 일이든 문화든 더 좋게 바꾸려고 노력하고, 앞으로 더 좋아질 것 같은데 하는 기대를 하고 좀 더 지켜봐야겠다는 생각을 하게 된다. . 올해는 내가 속한 기술부문에 변화가 매우 컸어서 (이전과 차원이 다른 정도의 훌륭함. 거의 뭐 실리콘밸리를 한국에서 구현하고 계시는 분이 있다. 존멋) 또 많이 배우고 감탄했던 것 같다. 이렇게 한번 더 뱅또속. . 3. 런닝머신 엔지니어 . 올해는 또 Data Scientist 에서 Machine Learning Engineer 로 직군을 변경했다. 전공이 컴퓨터공학 이기도하고, 회사에 오래 다니다보니 본업(?) 외 개발도 이것 저것 조금씩 기웃기웃해서 배우면서 엔지니어링 쪽에 재미를 많이 느끼고 있어서 나에게 더 맞을 것 같은 포지션으로 바꿨다. (사실 서버개발자가 더 맞을지도 몰름) . 로컬에서 고정된 데이터 셋을 받아서 분석하고, 모델을 만들어서 돌려보는 작업은 어렵지 않지만, 어떤 머신러닝/딥러닝 모델이 production 제품에 적용되기까지는 매우 많은 엔지니어링 리소스가 필요하다. . 올해 초에 회사에서 유저의 금융 데이터로 서비스의 특정 기능에 필요한 모델링 업무를 주로 했었다. 그렇지만 만든 모델이 결국 서비스화까지는 되지 못했는데, 엔지니어링 쪽 인력과 인프라가 부족했던 원인이 컸었다. 그 점이 갑갑해서 아예 엔지니어링까지 다 해버리자 했던 마음이 컸던 것 같다. (=협업을 잘 못한다는 뜻) . 매일매일 새로운 힙-한 모델들이 만들어지고 퍼포먼스가 갱신되는 걸 보면서 신기하고 재밌지만, 개인적으로는 모델링/연구 보다는 이런 모델들을 잘 가져다가 어떤 재밌는 걸 만들어볼까 하는 데 관심이 많기도 하다. . 어쨌거나저쨌거나 지금 특정 기능을 위해 이번에 새롭게 만든 모델을 개발계에 배포하고 QA를 진행하고 있다. QA가 무사히 완료되고 내가 만든 모델이 처음으로 실 서비스까지 적용되어 배포되는 순간은 너무 기쁠 것 같다. (난 개발자인가봐) 그래도 한번 이렇게 만들어 본 경험으로 다음번에는 더 쉽게 진행할 수 있을 것 같고, 인프라도 잘 구축해서 더욱 더 많은 모델을 서비스화하고 데이터로 멋진 서비스들을 많이 만들 수 있으면 좋겠다. . 아직은 생소한 직군이고 회사마다 하는 일의 범위나 역할이 다 다른 것 같지만 어떤 이름이 됬건간에 ML/DL을 이용한 서비스가 점점 많아지고 보편화되면서 이런 역할은 계속 수요가 증가하지 않을까 생각하고 있다. . 4. 글또 활동: 꾸준한 블로깅 훌륭 . 작년 11월 개발자 글 쓰기 모임인 글또 2기를 모집한다는 글을 보고 글또 활동을 시작하게 되었다. 글또에서는 6개월 동안 2주마다 블로그 글을 한 편씩 강제로(?) 작성하게 한다. 글쓰기를 강제하는 시스템 덕에 꾸준히 무언가를 하는 걸 잘 못하는 내가 1년간 글을 꾸준히 쓸 수 있는데 큰 도움이 되었다. :) . 2기로 처음 글또를 시작했을 때는 시작/회고글을 제외하고 6개월 간 총 6편의 글을 썼다. (읭?) 그래도 어느정도 습관이 생겼는지 3기 활동에서는 11편이다. 2배 성장. 예! 중간 쉬는 텀에도 글을 쓰긴 해서 2019년 작성한 모든 글을 세어보니 총 21편이었다. 글을 세어보면서 11월을 빼고는 매달 한 두편의 글이 있어서 개인적으로 매우 뿌듯했다. . 3기에는 더 많은 사람들이 참여하고, 중간에 오프라인 모임도 가졌어서 더 좋았던 것 같다. 2기에서는 처음/끝에만 한번 모여서 인사만하고 주로 슬랙에서만 얘기했었는데, 이번에는 모임을 주도해주신 분들이 계셔서 직접 얼굴 보고 얘기나누고 재미있었다. . 난 절대절대 내 의지력을 믿지 못하기 때문에 아마 내년에도 모집한다면 참여하지 않을까싶다. . 5. PT: 살려고 하는 운동 . 작년에는 회사 근처 헬스장에서 1년간 PT를 끊어서 운동을 했었다. 올해는 회사가 여의도로 옮기게 되면서 자연스레 운동을 안 가게 되었는데, 운동을 안한 지 6개월 정도 지나니 서서히 살이 찌고 몸도 안 좋아졌다. 운동 다시 해야지 해야지 미루다가 11월에야 겨우 집 근처 헬스장에 새로 등록했다. . 역시나 그냥 헬스장만 등록했더니 딱 하루 가고 안 가게 되어서 그냥 PT를 질렀다. 가격은 회당 6만원. 그나마 저렴한 편이지만, 카드값 출혈이 상당했다. 이번 달은 굶어야한다. 비쌌지만, 이렇게 계속 운동 안하면 죽겠구나 싶어서 살려고 시작했다. . 그래서 지금은 꼬박꼬박 주 2회씩 운동을 나가고 있다. 근력 운동도 하지만 체형 교정도 받고 있어서 매우 만족하고 있다. 안그래도 평소 자세가 안 좋다보니 거북목, 굽은 어깨, 등등 허리도 아프고 몸이 안 좋았었는데, 조금씩 좋아지고 있다. 다이어트 목적 보다는 거의 재활운동…인 것 같다. . 6. 그 외 . 개발 서적 외 독서량이 매우 줄었다. 책 안 읽으면 바보된다. | 퇴근 후 나의 술친구 참피디 아저씨 최고임. 최고의 유투버. | 올해의 지름: 4k 프로젝터, 플스4 pro (특징: 귀찮아서 잘 안함) 닌텐도도 지르고 싶다. | 집(통영)으로 가는 가장 빠른 길 을 찾았다. 김포 공항에서 사천 공항으로 갔다가 집 가는 버스 타면 2시간 반이다. 시간이 반이다. 가격은 3배다. | 심심해서 내가 좋아하고 자주 사용하는 제품을 만드는 회사들 해외주식투자를 소액으로 했는데, 괜찮은 투자였다. 더 많이 해봐야지. | 이제 한달 택시비 거의 20만원 … 내년엔 그냥 도로 연수 받고 쏘카타자… | 1일 1커밋 도전 실패. 주말엔 쉬자. | 사워맥주 맛나다. 회사 지하에 와인앤모어 곧 오픈한다고 해서 매우 행복. | 작년 11월에 조카가 태어났는데, 하루하루 커가는 모습이 매우매우매우 사랑스럽다. | 미드 실리콘밸리 정주행 시작. 꿀잼. | 이제 만 나이로도 어쩔 수 없는 빼박 30대… :( | . 2019 회고 글을 쓰면서 올해를 돌아보고 이런 저런 생각을 하다보니 내년엔 이래야지 저래야지 여러가지 생각이 떠오른다. 하지만 새해 계획은 새해 첫 날에 쓰는 게 제 맛이므로, 지금은 남은 2019년을 즐기고 내일 써야지. .",
            "url": "https://inahjeon.github.io/devlog/diary/2019/12/31/2019review.html",
            "relUrl": "/diary/2019/12/31/2019review.html",
            "date": " • Dec 31, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "gRPC",
            "content": "tensorflow serving에서 제공하는 gRPC 통신을 사용해보기전 간단히 gRPC에 대한 개념을 공부하고, 간단한 예제를 구현해보았습니다. . gRPC . RPC는 RPC(Remote Procedure Call)는 원격 컴퓨터나 프로세스에 존재하는 함수를 호출하는데 사용하는 프로토콜입니다. 원격에 있는 함수를 동일 프로세스에 존재하는 함수를 호출하는 것 처럼 직접 호출할 수 있습니다. gRPC는 Google 의 RPC 구현체입니다. . . 기본적으로 gRPC는 서비스 인터페이스와 메시지의 구조를 모두 설명하기 위해 ProtoBuf 를 IDL (Interface Definition Language)로 사용합니다. ProtoBuf는 구글에서 만들고 사용하는 데이터 직렬화 라이브러리 입니다. IDL로 정의된 메시지를 기반으로 protoc 라는 컴파일러를 사용해서 원하는 언어의 코드를 생성할 수 있습니다. . (gRPC와 JSON을 사용 하는 HTTP Api 간 비교표) . gRPC에 대한 더 자세한 소개와 장점에 대해서는 https://medium.com/@goinhacker/microservices-with-grpc-d504133d191d 에 잘 설명되어있습니다. . Tutorial . https://github.com/grpc/grpc/tree/v1.25.0/examples/python/route_guide 을 참고하여 간단한 예제를 만들어보았습니다. . 먼저 필요한 라이브러리 설치를 위해 아래와 같이 grpcio-tools 를 설치해 줍니다. . pip install grpcio-tools . 예제에서는 gRPC 통신을 통해 두 수의 합을 구해주는 서비스를 제공하는 서버와, 클라이언트를 구현합니다. . service.proto . 먼저 protos 디렉토리 아래 service.proto라는 파일을 만들어서 두 수의 합을 구하는 서비스의 인터페이스와 message의 형태를 정의합니다. . syntax = &quot;proto3&quot;; package test; service Test { rpc GetSum(Data) returns (Sum) {} } message Data { int32 a = 1; int32 b = 2; } message Sum { int32 sum = 1; } . 다음에서 구현할 서버와 클라이언트에서 위 service.proto 에 명세 대로 gRPC를 이용하여 통신하게 됩니다. . run_codegen.py . from grpc_tools import protoc protoc.main(( &#39;&#39;, &#39;-Iprotos&#39;, &#39;--python_out=python_out&#39;, &#39;--grpc_python_out=python_out&#39;, &#39;protos/service.proto&#39;, )) . protos/service.proto 에 정의된 명세대로 python stub 코드를 python_out 이라는 디렉토리에 생성해주는 코드입니다. . run_codegen.py 을 통해 만들어지는 코드는 다음과 같습니다. . python_out/service_pb2.py | . # -*- coding: utf-8 -*- # Generated by the protocol buffer compiler. DO NOT EDIT! # source: service.proto import sys _b=sys.version_info[0]&lt;3 and (lambda x:x) or (lambda x:x.encode(&#39;latin1&#39;)) from google.protobuf import descriptor as _descriptor from google.protobuf import message as _message from google.protobuf import reflection as _reflection from google.protobuf import symbol_database as _symbol_database # @@protoc_insertion_point(imports) _sym_db = _symbol_database.Default() DESCRIPTOR = _descriptor.FileDescriptor( name=&#39;service.proto&#39;, package=&#39;test&#39;, syntax=&#39;proto3&#39;, serialized_options=None, serialized_pb=_b(&#39; n rservice.proto x12 x04test &quot; x1c n x04 x44 x61ta x12 t n x01 x61 x18 x01 x01( x05 x12 t n x01 x62 x18 x02 x01( x05 &quot; x12 n x03Sum x12 x0b n x03sum x18 x01 x01( x05 x32) n x04Test x12! n x06GetSum x12 n.test.Data x1a t.test.Sum &quot; x00 x62 x06proto3&#39;) ) _DATA = _descriptor.Descriptor( name=&#39;Data&#39;, full_name=&#39;test.Data&#39;, filename=None, file=DESCRIPTOR, containing_type=None, fields=[ _descriptor.FieldDescriptor( name=&#39;a&#39;, full_name=&#39;test.Data.a&#39;, index=0, number=1, type=5, cpp_type=1, label=1, has_default_value=False, default_value=0, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, serialized_options=None, file=DESCRIPTOR), _descriptor.FieldDescriptor( name=&#39;b&#39;, full_name=&#39;test.Data.b&#39;, index=1, number=2, type=5, cpp_type=1, label=1, has_default_value=False, default_value=0, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, serialized_options=None, file=DESCRIPTOR), ], extensions=[ ], nested_types=[], enum_types=[ ], serialized_options=None, is_extendable=False, syntax=&#39;proto3&#39;, extension_ranges=[], oneofs=[ ], serialized_start=23, serialized_end=51, ) _SUM = _descriptor.Descriptor( name=&#39;Sum&#39;, full_name=&#39;test.Sum&#39;, filename=None, file=DESCRIPTOR, containing_type=None, fields=[ _descriptor.FieldDescriptor( name=&#39;sum&#39;, full_name=&#39;test.Sum.sum&#39;, index=0, number=1, type=5, cpp_type=1, label=1, has_default_value=False, default_value=0, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, serialized_options=None, file=DESCRIPTOR), ], extensions=[ ], nested_types=[], enum_types=[ ], serialized_options=None, is_extendable=False, syntax=&#39;proto3&#39;, extension_ranges=[], oneofs=[ ], serialized_start=53, serialized_end=71, ) DESCRIPTOR.message_types_by_name[&#39;Data&#39;] = _DATA DESCRIPTOR.message_types_by_name[&#39;Sum&#39;] = _SUM _sym_db.RegisterFileDescriptor(DESCRIPTOR) Data = _reflection.GeneratedProtocolMessageType(&#39;Data&#39;, (_message.Message,), { &#39;DESCRIPTOR&#39; : _DATA, &#39;__module__&#39; : &#39;service_pb2&#39; # @@protoc_insertion_point(class_scope:test.Data) }) _sym_db.RegisterMessage(Data) Sum = _reflection.GeneratedProtocolMessageType(&#39;Sum&#39;, (_message.Message,), { &#39;DESCRIPTOR&#39; : _SUM, &#39;__module__&#39; : &#39;service_pb2&#39; # @@protoc_insertion_point(class_scope:test.Sum) }) _sym_db.RegisterMessage(Sum) _TEST = _descriptor.ServiceDescriptor( name=&#39;Test&#39;, full_name=&#39;test.Test&#39;, file=DESCRIPTOR, index=0, serialized_options=None, serialized_start=73, serialized_end=114, methods=[ _descriptor.MethodDescriptor( name=&#39;GetSum&#39;, full_name=&#39;test.Test.GetSum&#39;, index=0, containing_service=None, input_type=_DATA, output_type=_SUM, serialized_options=None, ), ]) _sym_db.RegisterServiceDescriptor(_TEST) DESCRIPTOR.services_by_name[&#39;Test&#39;] = _TEST # @@protoc_insertion_point(module_scope) . python_out/service_pb2_grpc.py | . # -*- coding: utf-8 -*- # Generated by the protocol buffer compiler. DO NOT EDIT! # source: service.proto import sys _b=sys.version_info[0]&lt;3 and (lambda x:x) or (lambda x:x.encode(&#39;latin1&#39;)) from google.protobuf import descriptor as _descriptor from google.protobuf import message as _message from google.protobuf import reflection as _reflection from google.protobuf import symbol_database as _symbol_database # @@protoc_insertion_point(imports) _sym_db = _symbol_database.Default() DESCRIPTOR = _descriptor.FileDescriptor( name=&#39;service.proto&#39;, package=&#39;test&#39;, syntax=&#39;proto3&#39;, serialized_options=None, serialized_pb=_b(&#39; n rservice.proto x12 x04test &quot; x1c n x04 x44 x61ta x12 t n x01 x61 x18 x01 x01( x05 x12 t n x01 x62 x18 x02 x01( x05 &quot; x12 n x03Sum x12 x0b n x03sum x18 x01 x01( x05 x32) n x04Test x12! n x06GetSum x12 n.test.Data x1a t.test.Sum &quot; x00 x62 x06proto3&#39;) ) _DATA = _descriptor.Descriptor( name=&#39;Data&#39;, full_name=&#39;test.Data&#39;, filename=None, file=DESCRIPTOR, containing_type=None, fields=[ _descriptor.FieldDescriptor( name=&#39;a&#39;, full_name=&#39;test.Data.a&#39;, index=0, number=1, type=5, cpp_type=1, label=1, has_default_value=False, default_value=0, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, serialized_options=None, file=DESCRIPTOR), _descriptor.FieldDescriptor( name=&#39;b&#39;, full_name=&#39;test.Data.b&#39;, index=1, number=2, type=5, cpp_type=1, label=1, has_default_value=False, default_value=0, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, serialized_options=None, file=DESCRIPTOR), ], extensions=[ ], nested_types=[], enum_types=[ ], serialized_options=None, is_extendable=False, syntax=&#39;proto3&#39;, extension_ranges=[], oneofs=[ ], serialized_start=23, serialized_end=51, ) _SUM = _descriptor.Descriptor( name=&#39;Sum&#39;, full_name=&#39;test.Sum&#39;, filename=None, file=DESCRIPTOR, containing_type=None, fields=[ _descriptor.FieldDescriptor( name=&#39;sum&#39;, full_name=&#39;test.Sum.sum&#39;, index=0, number=1, type=5, cpp_type=1, label=1, has_default_value=False, default_value=0, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, serialized_options=None, file=DESCRIPTOR), ], extensions=[ ], nested_types=[], enum_types=[ ], serialized_options=None, is_extendable=False, syntax=&#39;proto3&#39;, extension_ranges=[], oneofs=[ ], serialized_start=53, serialized_end=71, ) DESCRIPTOR.message_types_by_name[&#39;Data&#39;] = _DATA DESCRIPTOR.message_types_by_name[&#39;Sum&#39;] = _SUM _sym_db.RegisterFileDescriptor(DESCRIPTOR) Data = _reflection.GeneratedProtocolMessageType(&#39;Data&#39;, (_message.Message,), { &#39;DESCRIPTOR&#39; : _DATA, &#39;__module__&#39; : &#39;service_pb2&#39; # @@protoc_insertion_point(class_scope:test.Data) }) _sym_db.RegisterMessage(Data) Sum = _reflection.GeneratedProtocolMessageType(&#39;Sum&#39;, (_message.Message,), { &#39;DESCRIPTOR&#39; : _SUM, &#39;__module__&#39; : &#39;service_pb2&#39; # @@protoc_insertion_point(class_scope:test.Sum) }) _sym_db.RegisterMessage(Sum) _TEST = _descriptor.ServiceDescriptor( name=&#39;Test&#39;, full_name=&#39;test.Test&#39;, file=DESCRIPTOR, index=0, serialized_options=None, serialized_start=73, serialized_end=114, methods=[ _descriptor.MethodDescriptor( name=&#39;GetSum&#39;, full_name=&#39;test.Test.GetSum&#39;, index=0, containing_service=None, input_type=_DATA, output_type=_SUM, serialized_options=None, ), ]) _sym_db.RegisterServiceDescriptor(_TEST) DESCRIPTOR.services_by_name[&#39;Test&#39;] = _TEST # @@protoc_insertion_point(module_scope) . server 와 client 코드에서 위 모듈을 불러와서 사용하게 됩니다. . server.py . 서버 코드를 아래와 같이 구현합니다. . from concurrent import futures import grpc from python_out.service_pb2 import Data, Sum import python_out.service_pb2_grpc as service_pb2_grpc class TestServicer(service_pb2_grpc.TestServicer): def __init__(self): pass def GetSum(self, request: Data, context): return Sum(sum=request.a + request.b) def serve(): server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) service_pb2_grpc.add_TestServicer_to_server( TestServicer(), server ) server.add_insecure_port(&#39;[::]:50051&#39;) server.start() server.wait_for_termination() if __name__ == &#39;__main__&#39;: serve() . service_pb2_grpc.py 에 정의된 stub class TestServicer 를 상속받아서 service.proto 에 정의했던 rpc GetSum(Data) returns (Sum) {} 의 실제 동작 부분을 구현해주면 됩니다. 그리고 serve() 에서 서버를 실행시켜줍니다. server.py 코드를 실행하면 서버가 실행됩니다. . client.py . import grpc import python_out.service_pb2 as service_pb2 import python_out.service_pb2_grpc as service_pb2_grpc def get_sum(stub: service_pb2_grpc.TestStub): response = stub.GetSum(service_pb2.Data(a=1, b=2)) print(f&quot;Received sum: {response}&quot;) def run(): with grpc.insecure_channel(&#39;localhost:50051&#39;) as channel: stub = service_pb2_grpc.TestStub(channel) print(&quot;-- Get Sum --&quot;) get_sum(stub) if __name__ == &#39;__main__&#39;: run() . 다음은 server에 요청을 보내서 두 수의 합을 구하는 client 코드입니다. service_pb2_grpc 에 선언된 stub class를 이용해서 GetSum 함수를 호출하고 인자로 service_pb2 에 정의된 Data class 의 인스턴스를 넘깁니다. 그리고 response 값으로 두 수의 합을 받아옵니다. client 코드를 실행하면 다음과 같은 결과를 얻을 수 있습니다. . -- Get Sum -- Received sum: sum: 3 . 예제는 크게 어렵지 않았는데, gRPC의 개념에 대해서는 익숙하지 않아 이해하기 좀 어려웠습니다. 그렇지만 예제를 한번 작성해보고 나니, 개념과 장점에 대해서 좀 더 잘 와닿았던 것 같습니다. 다음에는 tensorflow serving 에서 제공하는 gRPC 통신으로 변경 시 성능 변화가 얼마나 있는지 테스트 해보려고 합니다. .",
            "url": "https://inahjeon.github.io/devlog/engineering/2019/12/08/grpc.html",
            "relUrl": "/engineering/2019/12/08/grpc.html",
            "date": " • Dec 8, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "Deview 2019",
            "content": "Deview2019 참석 후 작성한 세션 요약글 입니다. . 세션1: 외산 클라우드 없이 AI 플랫폼 제공하기: features, training, serving, and AI Suite. - 현동석님 . 모델을 연구하는 사람들이 모델 연구에만 집중할 수 있도록 다양한 ml 서비스들을 제공하기 위한 플랫폼 . 1. 자체 AI 플랫폼이 필요한 이유 . security - 데이터에 대한 보관, 반출 제한 등 | cost - 연산량이 적은 경우 클라우드 사용이 유리. 그러나 대용량 데이터 처리를 지속 수행한다면 비용이 큼 | demand - 머신러인 수요는 기하급수적으로 증가. 초기 각자 구축하던 부분에서 공통적인 부분을 플랫폼에서 제공 | . 2. 어떻게 만들어야 할지 생각해보기 &gt; AI Suite . 이미 구축된 플랫폼이 많았음 . 분산저장 플랫폼 | 분산 처리 플랫폼 | 피쳐 엔지니어링 플랫폼 &gt; 없음 | 모델 학습 플랫폼 | 모델 서빙 플랫폼 | . 하면서 알게 된 머신러닝 모델 제품화의 세 단계 . 데이터 처리 데이터 수배, 검토, 처리 | . | 모델 학습 모델 학습, 평가, 튜닝 | . | 서빙 성능 평가(응답, 처리량)과 용량 산정 후 서비스로 제공 | . | 실제로 모델을 제품화할 때 필요한 것들 . 모델성능을 최적화하는 것보다 인프라를 만들고 처리하는 시간이 오래걸림 . 데이터 처리가 오래 걸립니다. | 자동화를 고려해야합니다. | 3. 머신러닝 데이터 준비 자동화하기: AI Features . (네이버는 분산 저장 플랫폼에 데이터가 다 저장되어있고 카탈로그를 웹에서 볼 수 있음) . DUMP: 데이터를 어딘가로 가져와서 (hive QL) | ANALYZE: 잘못됐거나 biased 데이터는 없나 보고, 간단한 가시화를 통해 데이터에 대한 인사이트를 얻은 후 (facet, jupyter) | BATCH: 잘못된 데이터는 버리고, 나머지는 가공해서 피쳐벡터를 만듦 (pyspark) | . 데이터 스냅샷 떠놓기 &gt; DUMP . 데이터 분석하기(검증) &amp; 가공하기 . Problem . facet은 데이터를 클라이언트에서 처리 | 10만건 이상의 데이터는 엄청 느려짐 | . Solution . 데이터를 가시화는 인사이트를 얻기 위함 | 모든 데이터를 다 볼 필요는 없고, 샘플링된 데이터로만 보면 됨. | . Tip1 . jupyter notebook에서 pyspark를 실행하면 cluster mode로 돌지 않음. (client mode임) | 노트북에서는 샘플데이터에 대해 수행하고, batch 기능을 통해 클러스터 모드로 수행하도록 지원함. | . Tip2 . 텍스트데이터는 대부분 NLP가 필요함 | NLP는 사전 데이터와 함께 사용함. &gt; 분산 처리에 적합하지 않음. (수행될 노드에 사전이 설치되어 있어야함) | NLP API + Throttling proxy(nginx) + mapParition(for bulk request) + UDF로 해결 | . 4. 머신러닝 모델 학습 . 모델 연구 때와는 다른 제품화 때의 관점 . 연구시 . 가능한 자원을 모두 사용하여 빨리 학습하고 평가하는 과정을 반복 | 최대한 동시에 실행해서 제일 성능 좋은 모델과 파라미터를 얻는 것이 목적 | 데이터는 고정 되므로 캐싱하면 이득이 있음 | . 제품화시 . 학습에 걸리는 시간을 이미 알고 있음. | 다음 번 갱신 전까지만 학습이 완료되면 됨 | 일정 시간 내 최소의 자원을 사용하면 좋음 | 배포 전/후 모델의 품질을 검증할 필요가 있음 (validation 필요 - ex. hamming distance 등) | 매번 새로운 데이터로 학습하는 경우가 많아 데이터 개싱 이득이 없음 | . 학습 자동화 . ML code =&gt; docker image =&gt; resource 입력 후 학습 . 5. 머신러닝 모델 서빙하기 . 서빙 아키텍처 소개 . 서빙 = APS 서빙 + 모델 서빙 . TFS(TFserving), CFS(Caffe servinf) 등 사용 | PyTorch model &gt; onnx converting &gt; .onnx | TF -&gt; SavedModelBuilder &gt; .pb | . APS 서버: 전처리 후처리 전담 서버 . APS 서버 에서 전처리 및 후처리 작업을 한 후, serving 서버에 inference 요청을 날림. | APS 서버가 inference 서버의 frontend 역할을 함 (비지니스 로직, 메타 데이터 정보를 가짐) | . 인퍼런스 서버 띄우기 . 사용자는 모델과 resource 만 입력 | . 데이터 전후 처리 서버 띄우기 . git 저장소의 코드 -&gt; CI/CD | json -&gt; gRPC로 매핑해서 날려줌 | . 6. 모든 단계를 자동화하기 . 각 단계를 API로 만들기 . 드래그 앤 드롭으로 파이프라인 작업 코드 자동 생성하기 . MNIST 데모 시연함 | . 알아서 자동으로 배포하게 만들기 . 최종 데모 . 세션2: 신호처리 이론으로 실용적인 스타일 변환 모델 만들기 (Better Faster Stronger Transfer) - 유재준님 . 1. 스타일 트랜스퍼란? . 이미지에서 스타일은 뭘까? 컨텐츠는 뭘까? 그리고 어떻게 옮기지? . Artistic style transfer vs. Photo-realistic style transfer . Artistic style transfer: 이미지를 예술적으로 변환하는 문제 (ex. Gatys et al. CVPR 16) . | Photo-realistic style transfer: 이미지를 사실적으로 변환하는 문제 (ex. 낮에서 저녁으로 등) . | . Style transfer 왜 중요한데? . Style transfer: Generative model . Unsupervised Learning | Representation Learning | Feature extraction | . 질감 생성: 옛날에는 어떻게 했을까? . 어떤 선형 필터들의 조합으로 표현 | 질감마다 필터 정의가 필요했음 | . 최초로 CNNs을 사용한 연구 - Neural Style Algorithm . Gatys et al., NIPS 2015 | Gatys et al., CVPR 2016 | . 2. 스타일 트랜스퍼, 왜 되는걸까? . How it works? . Minimizing Maximum Mean Discrepancy (MMD) (Li et al., IJCAI 2017) | 두게의 분포 사이의 거리를 최소화하는 것과 같음을 밝혀냄 | . Why VGG만 잘되나? . 아직 밝혀지지 않음. | CNNs 가 texture에 편향되어 있음을 밝혀낸 논문이 있음 (Gerihos et al., ICLR 2019) | CNN이 사실 구조를 보는 것이 아니라 texture를 봄. 사람은 구조를 보고 판단. | VGG가 특히 texture에 민감 | . 3. 스타일 트랜스퍼: 최신 연구 흐름 한 눈에 살펴보기 . Artistic Style Transfer: recent trends, why &amp; how? . 하나의 스타일마다 네트워크 하나씩 학습하는 것이 필요. 실용적이지 않음 | Instance Normalization: 스타일 변환 결과가 컨텐츠 이미지의 Contrast에 따라 바뀌지 않게 하자 | Conditional Instance Normalization (CIN): 하나의 모델이 하나의 스타일만 변환하던 문제해결을 시도 | Adaptive Instance Normalization (AdaIN): Scaling과 Shifting 값을 구하는 과정을 학습과 분리함. (고양이, 사자를 따로 encoding 후 사자 스타일을 고양이에 입힘) | Whitening and Coloring Transforms (WCT): AdaIn에서 평균과 분산만 맞추던 것을 공분산까지 맞춤 | . Photorealistic Style Transfer: recent trends, why &amp; how? . Deep Photo Style Transfer: 스타일 변환 전후 픽셀 간의 관계를 맞춰줌. gram matrix를 영역별로 나눠서 최적화. | PhotoWCT: decoding 할때 어디서 max pooling이 일어낫는지 알려줌 &gt; 후처리에서 다 보정됨. | . WCT via Wavelet Corrected Transforms (WCT2): 후처리 없이 기존 모델보다 성능이 좋은 모델을 개발. . Pooling과 비슷한 역할을 하면서 Encode-decode 과정에서 정보를 잃지 않아야하고, 입력이미지의 특징을 잘 표현할수 있는 모듈 -&gt; Wavelet 변환 활용함. (신호 처리 이론) | Multi-level 대신 한번의 feed-forward만 수행 | 비디오 스타일 변환도 잘 됨. | . 세션3: 문자인식(OCR), 얼마나 정확하지? (문자인식 성능을 정확하게 측정하는 방법) - 최찬규님 . 1. 성능평가의 중요성 . 성능 평가의 중요성 . 성능을 개선하기 위해 (문제점을 정확하게 파악) | 모델 선택 (많은 모델 중 어떤 모델을 서비스 할 것인가?) | 다른 연구 그룹과의 성능 비교 (ImageNet, Kaggle) | . 어떻게 하나? . 눈으로 (사람이 직접 평가): 시간과 비용이 많이 들고, 평가에 오류가 있을 수 있음 | 컴퓨터 (자동 평가 시스템): 시간과 비용이 거의 들지 않으며, 평가에 오류가 없음 | . 2. 문자인식 개론 . 문자인식: 오프라인의 글자를 기계가 읽을 수 있도록 디지털화 한것 . 응용분야 . 자동차 번호판, 명함, 신용카드, 신분증 인식, 이미지 번역 등 | . 문자인식 과정 . 문자검출: text detection | 문자 인식: text recognition | . 3. 기존의 성능 측정 방법 . Precision &amp; Recall . ex) 암진단 - TP, FN, FP, TN . Precision: 예측한 True 중에서 True를 올바르게 예측한 비율 Recall: 실제 True 중에서 True를 올바르게 예측한 비율 . 검출기/인식기/End-to-End 성능 측정방법 . 문자검출 . IoU (Intersection over Union): 정답(Ground Truth)와 예측 박스의 영역이 얼마나 겹치는지 확인 (50%이상이면 hit) | . 문자인식 . WEM(Word based Exactly Matching): 정답과 예측 단어가 정확히 일치하는지 체크 (단어기반) | 1-NED(Normalized Edit Distance): 두 단어간 편집거리(삽입, 교환, 삭제)를 측정한 뒤, 긴 단어의 길이로 정규화 (글자기반) | . End-to-End (검출+인식) 평가 . . 순차(Cascade) 평가 처리: 검출 평가(IoU) -&gt; 인식 평가(WEM, 1-NED) | . 4. 기존 방법의 문제점 (사례와 그 빈도) . 정교한 성능 측정 불가 (IoU에서 겹치는 영역이 50%가 넘지만 필요한 글자를 인식하지 못한 경우) | One-to-Many, Many-to-One 문제 One-to-Many: 하나의 정답 박스가 여러개의 박스로 나뉘어 예측되는 경우 (split) e.g) Riverside -&gt; River, side | Many-to-One: 여러개의 정답 박스가 한개의 박스로 합쳐져 예측되는 경우 (Merge) | . | end-to-end에서 잘못된 오류가 전파됨. | . 5. 신규 제안 방법 : PopEval . 설계시 고려한 점 . End-to-End 평가 | One-to-Many, Many-to-One 문제 해결 | 정교하게 세부적으로 성능 측정 가능 | 기존 평가셋과 호환되어야 함 | . 신규 방법 . . 겹치는 영역의 글자 중에서, 같은 글자(=맞춘 글자)를 하나씩 지움 | Recall: 맞춘 글자수 / 정답 글자수 | Precision: 맞축 글자수 / 예측한 글자수 | . 엣지 케이스 . 제거해야할 글자가 중복될 경우 어떤 글자부터 제거할 것인가? e.g. N”A”VER, P”A”PAGO | 중복이 없는 박스 우선 제거, 교집합이 클수록 우선 | . 6. 신규 성능 평가 방법의 검증 실험 . One-to-Many, Many-to-One 문제는 얼마나 발생하나? 전체 2~9%. 리더보드 TOP10 순위에 영향을 줌 | 기존 평가셋(=단어 단위)과 호환 가능한가? 호환 가능 | 신규 평가 방법은 믿을만 한가? (평가자를 모집해서 기존/신규 평가 방법에 대한 평가) | . 실험 환경 . OCR 평가셋 - IC13, IC15 평가셋 사용 | 예측 모델: 검출기: EAST, PixelLink | 인식기: GRCNN, ASTER | . | . 세션4: 레이블링 조금 잘못돼도 괜찮아요: Clova가 레이블 노이즈 잡는 법 - 강재욱님 . 데이터 전략에 대한 노하우가 실제 경쟁력이라고 생각함. . 1. 레이블 노이즈가 무엇인가? . 레이블 노이즈: 같은 범주의 데이터를 잘못 설명하는 의도되지 않은 Mislabel . . 왜 문제인가? . 레이블 노이즈는 모델의 feature extraction 을 어렵게함 | 모델의 성능을 떨어뜨리게 함 | . 어떻게 해결할 것인가? . 훈련모델 = 훈련방법 (데이터, 모델구조) | . Approach1: 모델구조: . 복잡한 패턴도 잘 인식하는 모델 구조를 쓴다 | 서빙 및 훈련 계산량이 증가함 | . Approach2: 훈련방법 . 커리큘럼을 만들어서 학습시킨다. (쉬운 데이터 부터 학습) | 훈련 계산량 증가 + 추가 데이터 필요 | . Approach3: Data Cleaning Method . Human Data Cleaning | Active Learning (모델이 1차로 labeling -&gt; 사람이 re-labeling) | . 사람의 도움 없이 레이블 노이즈를 제거 할 수 없을까? . Model inference | Relabeling by model | . 2. 레이블링 바로잡는 AutoML (in project Khan) . 문제: 흰 오리 한마리가 mislabeled 되어있을 때 어떻게 고칠 수 있을까? | . Split -Train - Check 알고리즘 . Split: 전체 dataset을 train /valid set으로 분할 | Train: correction을 위한 “기준 데이터” | Checker : 레이블 Correction용도로 훈련한 모델. valid set를 훈련된 checker에 입력하여 labeling 검사! | . MultiSplit – Train – Check - Vote . . 여러버전의 Split branch를 구성 | Vote: Label update를 위해서 각 branch의 “Split-Train-Check” 결과를 결합 | . 어떻게 Vote하면 좋을까? . Majority Vote: 가장 단순한 방법. | checker 의 soft-value 값을 활용! -&gt; PICO | . 3. PICO: Probabilistic Iterative COrrection . checker의 결과 베이지안 확률 결합 | labeling의 Iterative Probabilistic correction | 레이블링 히스토리의 hidden markov modeling을 통한 반영 | 반복적 확률적 Vote를 통해서 점진적으로 레이블 노이즈를 제거함. | . 설계와 구현사이 삽질기 . 많은 checker를 학습해야함. (GPU 리소스 이슈) | 확률값 저장 시 메모리 이슈 (Spark, sparse matrix 활용) | inference 서버 부하 이슈 (local serving project) | . PICO Architecture . . 4. FAQ 데이터 셋에 적용해 보기 . LINE 사용에 관한 FAQ 톡 서비스 데이터 set에 적용 | 일반적인 답변 인텐트 -&gt; 구체적인 답변 인텐트로 변화함. | . 5. 개선 방향 . 효율개선 - 데이터 셋 품질 사전 검증 모듈(PICO-trigger) | 품질개선: 생성모델를 통한 Imbalance Dataset 문제 해결 | 품질개선: 다양한 Metric voting 방식 적용 | . 6. 요약 . 데이터 품질 전략이 없는 AI 프로젝트는 성공 하기 어려움 | AI 데이터 자동정제 파이프라인은 매우 큰 경쟁력 | Naver Clova Chatbot Builder는 PICO를 통해서 데이터 자동 정제하여 서비스 품질 개선 | PICO 아키텍쳐는 다른 종류 데이터 셋에도 적용 가능 | . 세션5: 자율주행 시뮬레이터를 개발하면서 경험한 한계점 및 활용 방안 - 홍준님 . 0. 전달하고자 하는 내용 . 시뮬레이터의 한계점과 효과적으로 활용할 수 있는 방법 . 1. 자율주행 시뮬레이터란? . Real-World를 대체하는 것이 시뮬레이터의 역할임. | 구현하기 어려운 Edge case(차량에 비친 차량, 도로 위에 날아다니는 비닐 등) 들이 존재함 | . 시뮬레이터가 필요한 이유? . 실차를 이용하는 필드 테스트의 한계점이 있기 때문에 | . 시뮬레이터에서의 이슈 . 실제와 얼마나 비슷한가? | GT를 얻을 수 있는가? | . 2. “인지” 분야에서의 이슈 . 질문1: 가상 센서의 완벽한 정답 데이터를 얻을 수 있는가? . 3D map . 실제 존재하는 맵을 만들 경우 시간, 비용이 매우 많이 듦 | 해당 지역 촬영 및 3D 복원 -&gt; 3D 리터치, 오브젝트 분리작업 -&gt; 텍스쳐, 머티리얼, 라이팅 작업 | 표지판, 차량 등 오브젝트 생성 | . GPS . 정밀한 HD Map이 있어야 구현 가능함 | 신호가 약/강한 지역 구현 | Sensor Noise modeling (Multipath error) | . Camera . 아직은 현실 데이터와 차이가 많이 남 | GT를 얻기 쉽기 때문에 virtual data를 많이 활용하려고 함. | Dataset으로써 역할은 충분히 가능함 | Future work: 사용자의 카메라와 같은 “스타일”의 이미지로 데이터를 Translation해야한다 | . Lidar . Distance + Intensity | Noise의 모델링이 실제와 너무 달라 어려움 | . 질문2: 센서는 몇개나 사용할 수 있나요? . 환경 모사를 위한 CPU가 따로 필요함 (사용자의 PC성능에 달려있음) | . 3. “판단” 분야에서의 이슈 . 질문3: 시뮬레이터의 주변 차량들이 사람처럼 움직였으면 좋겠어요. . 주변 차량이 사람처럼 주행하려면 결국 완전 자율 주행이 가능해야 함. | 실제 도로에는 다양한 차량, 성향의 사람이 존재함. | . 방법 . 주변 차량의 파라미터 자유도를 높이자 | 멀티플레이로 “진짜”사람이 운전하는 상대 차량 | 딥러닝 적용 | . 질문4: 시나리오는 어떤것이 있나요? . 시나리오를 튜닝할 수 있도록 함. | . 4. “제어” 분야에서의 이슈 . 질문5: 실제 차량과 가상의 차량의 Dynamics가 얼마나 동일한가요? . 꽤나 비슷하지만 필드 테스트는 꼭 필요함. | 검증된 시뮬레이터들이 많음. | . 5. 알고리즘 개발 (넘김) . 좋았던 점/새로 알게됨 점/시도해볼 것 . facet으로 샘플 데이터 시각화 | 대용량 분산처리가 어려운 사전 NLP 작업의 경우(라이브러리 설치 등) API화 하여 호출하여 사용 | 모델 서빙 시 inference 서버 외 전/후 처리(비즈니스 로직 등)를 담당하는 서버 따로 사용 | Labeling 오류 수정 자동화 &gt; PICO 알고리즘 테스트 해보기 | .",
            "url": "https://inahjeon.github.io/devlog/conference/2019/10/28/deview2019.html",
            "relUrl": "/conference/2019/10/28/deview2019.html",
            "date": " • Oct 28, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Devfest2019",
            "content": "쉽게 따라할 수 있는 한국어 임베딩 구축 - 이기창 . 단어 임베딩으로 문서 분류하기 . 가장 많이 사용: fastText | . 핵심 컨셉 . 문서에 속한 단어가 유사하면 문서 의미도 비슷하다. | . 문서 벡터를 어떻게 만드나? . 단어 임베딩의 합(sum)으로 문서 백터를 표현한다. (단어 벡터의 합 = 단어 벡터의 평균 = 문서 벡터의 중심) | . 실험으로 검증 . 네이버 영화 리뷰 말뭉치 활용 | 학습데이터(문장, label) 댓글을 모두 문서 벡터(단어 임베딩의 합)로 변환한다. | cosine similarity 가 가장 높은 학습 데이터의 label로 분류 | . 시사점 . 복잡한 딥러닝 모델을 써도 80%대 성능 기록 | 단어 임베딩 품질이 좋으면 자연어처리 성능을 높일 수 있음 | . 임베딩이 어떻게 의미를 가지는가 . 말뭉치의 통계적 패턴이 들어있음 | . Back-of-Words . 단어 빈도를 센다. | 문서를 쓴이의 의도는 단어 사용 패턴에 드러난다. | . ELMo, Bert . 단어가 어떤 순서로 나타나는지 살핀다. (bi-direction) | 시퀀스 정보에 ㅡ이미가 녹아있다. | . Word2Vec, FastText, GloVe . 단어가 어떤 단어와 주로 같이 나타나는지 살핀다. | 문맥에 의미가 녹아있다. | . 문장 수준 임베딩 구축 및 활용 (요즘 대세): ELMo, BERT . 2014년은 워드 임베딩, 그 이후로 elmo (sentence embedding) 쪽으로 많이 연구되고 있음. . | ELMo: bidirectional LSTM, + CNN (다음 단어를 예측) . | BERT: (SOTA) Transformer network (Attention is all you need) . | . 문장 수준 임베딩 . 문장 수준 임베딩의 장점은 동음이의어 분간 가능 | 문장의 문맥적 의미를 벡터화할 수 있음. . | 토큰화 &gt; vocabulary &gt; 학습 | . BERT . https://github.com/yeontaek/BERT-Korean-Model | SKT: https://github.com/SKTBrain/KoBERT | . 임베딩 활용 . 임베딩이 가장 크게 쓰일 수있는 분야는 전이학습. | 다른 네트워크의 입력값으로 사용돼 자연어 처리 성능을 높일 수 있음 . | Bengio &gt; Data Augmentation &gt; Mixup | . Clean Code for ML/AI - 한성민 . What is Clean Code . 깨진 유리창 이론 ~ 코드 품질 . 보이 스카웃 규칙 - 떠날 때는 찾을 때 보다 캠프장을 더욱 께끗이 할 것. . Usecase . 코드 악취 - 코드에 문제가 발생할 때 생기는 나쁜냄새 . 코드 악취 유형 1 . branch는 코드 복잡도를 높이므로 너무 많이 사용하지 않는 것 이 좋음. | guard clause 패턴으로 코드를 아예 실행시키지 않도록 함. | . 코드 악취2 - 주석을 남용 . 자세하고 친절한 주석을 함수 이름으로 바꿔보세요. | . 코드 악취 3 - 중복코드 . 중복되는 코드 합치기 | . 그밖의 코드 악취 케이스 . 너무 긴 메소드 | 너무 거대한 클래스 | 상속 거부: 자식 클래스에서 부모 클래스의 규칙을 무시하고 오버라이드 | 과도한 복잡성 | 게으른 클래스: 클래스에 부여된 기능이 너무 적은 경우 | . 실제 모델 코드 최적화: Attention-based bidirectional LSTM . 안쓰는 키워드 제거 | 직접 기입되어 있는 설정 인수들을 전역 변수로 추출 (수정이 빈번하게 일어남) | 복잡성이 높은 로직은 함수로 추출 | 조건식 단순화, 중복 코드 함수 추출 (모델 코드의 경우 조건을 사용하는 경우가 많음) | 모델 혹은 옵티마이저의 인수정보들을 전역 변수에서 설정 프레임워크를 이용한 파라미터로 전환 (google/gin-config) | 코드 컨벤션 공통화 single, double quotation | 함수 사이 2줄 | 1줄을 넘어서는 함수 인수의 intent | . | . configuration 관리툴 . https://github.com/IDSIA/sacred | . Lint, Quality gate . python lint 도구: pylint, flake8, pycodestyle | Quality gate 도구: code grade를 알려줌. (sonarqube, codeclimate, codebeat) | . The anti-patterns . if/switch 절의 남용 -&gt; key access를 통한 분기대체 | 데이터 초기화와 데이터 삽입 로직의 분리: 고차함수를 이용한 대체 ex) 빈 배열 &gt; append로 배열 채움. . | 타입(type)의 부재 | . Q&amp;A . 모델 테스트: 기존 테스트의 결과와 유사도로 검증 (일정 유사도 이하면 error) | . BERT in kaggle - 이유한, 이영수, 송원호 . Transformer . Encoder - Decoder 구조임 (seq-to-seq NMT에서 많이 사용) | . Encoder . self-attension, feed forword network가 있음 | . self-attension . sealed dot product -&gt; (Q,K 내적)유사도를 구해서 scalaing | . BERT . Pretraining (masked LM, Next sentence prediction) | Fine-tuning . | Transformer의 Encoder만 사용함. | . image domain과 비교 . CNN -&gt; self-attention | Residual Block -&gt; Encoder Block | RESNET -&gt; transformer | RESNET(pretrained) -&gt; BERT (pretrained) | . Toxic Jigsaw unintended bias in tocity classification . 정상 코멘트(문장, 문단, 단어,..)인지 악성 코멘트인지 분류하는 문제 | 특정 키워드가 포함되면 악성으로 구분할 확률이 상당히 높아짐. (정상인데 악성으로 분류되는 경우) | 대회에서 키워드들을 제시함. | Metric: AUC, bias-AUC (특정 키워드가 포함되었지만 정상 코멘트를 잘 찾아내는 척도) | . 진행방법 . Baseline: Fasttext -&gt; RNN | evaluation EDA를 진행 | 커스텀 loss function을 정의함. | 문맥에 따라 임베딩을 다르게 함. -&gt; BERT를 활용 | Ensemble 사용 | . Conclusion . 좋은 머신은 꼭 필요하다. | 작은 모델 뿐 아니라 큰 모델 역실 중요하다 | Evaluation에 맞는 loss를 잘 정의해야한다. | 일찍 파이프라인을 구성하고 많은 실험을 시도한다. | 제한된 시간 하에 높ㅇ느 점수를 내야해서 속도를 줄이기 위한 여러 기술이 필요하다. | . Molucle . 분자 학습을 어떻게 시킬까? . 왜 다양한 딥러닝 아키텍쳐가 필요할까? . 일반 머신러닝은 데이터를 표현하는데 한계가 있음 (LeCun) | 주어진 데이터 내에서 피쳐를 잘 추출해내기 위함 | 데이터가 가진 정보는 다양한 형태, 관계, 성질을 가지기 때문 | . 분자 -&gt; 그래프로 표현 . 문장으로 표현해본다면? | 분자가 가진 여러 정보들을 문장으로 표현 (e.g. 탄소 4개가 있음 등) | 각각의 정보들을 임베딩해 vector로 만듦. | vector를 이어붙여 float sentence 생성 | . 캐글에서 높은 점수를 얻으려면? . (1) with diversity, do ensemble! . difreret model -&gt; different layer, different hidden dim | different seed -&gt; different training subset | . (2) Pseudo labeling . 우리가 예측한 값을 target으로 한 test set으로 새로 학습하는것 | . (3) 함께 하라! . 모델의 성능을 높이기 위해 생각해야할 것! . 데이터의 핵심 피쳐를 잘 표한할 수 있는 학습 방식을 선택하라. (CNN, RNN, GNN, Transformer, Embedding) | 데이터를 잘 학습할 수 있는 Loss function을 찾아라 (다 해봐야할). | 학습이 잘 될 수 있는 조건을 최적화시켜라 (학습 스케쥴, 구조 최적화 등) | .",
            "url": "https://inahjeon.github.io/devlog/conference/2019/10/20/devfest2019.html",
            "relUrl": "/conference/2019/10/20/devfest2019.html",
            "date": " • Oct 20, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Data augmentation in text classification",
            "content": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks 이라는 논문을 읽고 요약한 내용입니다. . 1. Summary: . NLP에서 범용적으로 적용가능한 Data augmentation technique (EDA: Easy Data Augmentation)을 제안함. | 제안한 방법 중 Synonym replacement에 대하서만 기존 연구결과가 있음. RI, RS, RD는 논문에서 새롭게 제안한 방법. | 실험 했던 5개의 benchmark task (classifier - RNN, CNN)에서 성능 개선이 있었고 특히 적은 샘플 데이터셋에서 효과가 있었음. | 전체 학습셋 중 50%의 학습셋만 가지고 100% 학습셋을 사용했을 때의 최고 성능과 동일한 정확도를 낼 수 있었음. | . 2. 제안한 방법: . Text data augmentation operations . Synonym Replacement (SR): 문장에서 stop word가 아닌 랜덤 n개의 단어를 선택한 후, 랜덤하게 각 단어를 동의어로 변경. | Random Insertion (RI): 문장 내 특정 랜덤 단어의 랜덤 동의어를 찾아서, 문장 내 랜덤 위치에 동의어를 삽입하는 방식. n회 반복함. | Random Swap (RS): 문장 내 두개의 단어를 랜덤으로 선택해서 두 단어의 위치를 변경함. n회 반복함. | Random Deletion (RD): 문장 내 단어를 랜덤하게 p의 확률로 제거함. | . Parameter n, α 설정 . 문장의 길이에 따라 n이 변경되도록 문장의 길이 (l)을 수식에 반영함. | n = αl (α: 문장 내 변경되는 단어의 비율, l: 문장의 길이) | 각 문장마다 n개의 augmented 문장을 생성하도록 함. | RD에서 p=α | . 3. 실험방법 . 5개의 benchmark text classification task를 사용 SST-2: Stanford Sentiment Treebank (Socher et al., 2013) | CR: customer reviews (Hu and Liu, 2004; Liu et al., 2015) | SUBJ: subjectivity/objectivity dataset (Pang and Lee, 2004 | TREC: question type dataset (Li and Roth, 2002) | PC: Pro-Con dataset (Ganapathibhotla and Liu, 2008) | . | . . 사용한 Classification method LSTM-RNN (Liu et al., 2016) | CNN (Kim, 2014) | . | . 4. Results . (1) Performance . 5개 task에 대해 각 method의 평균 Accuracy를 측정함. | . . (2) Traning set size에 따른 실험 결과 . 사용하는 traning set size를 원본 데이터 크기의 {1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100} 비율로 늘려가면서 테스트 | 100 사용 시 최고 정확도인 88.3% 정확도를 50%의 데이터를 활용했을 때 얻을 수 있었음 (88.6%) | . . (3) Conserving true labels . augmenation으로 만들어진 데이터에 대해 원본 문장의 label이 보존되는지 차원 축소 및 데이터 시각화를 통해 확인함. . | Pro-con data에 대해 t-SNE (Van Der Maaten, 2014) 차원축소를 통해 2D plot으로 시각화함. . | . . 생성된 문장의 latent vector가 동일 label을 가진 원본 문장들과 가까운 위치에 있음을 확인할 수 있었음. | . (4) 파라미터 α 에 대한 성능 분석 . . SR은 작은 α에 대해 잘 동작했음. 너무 많은 단어를 교체하면 원래 문장과 의미가 많이 달라져서 그럴것으로 해석함. | RI는 α에 대해 성능이 stable했음. 원래의 문장과 문장순서가 유지되기 때문에 그럴것으로 해석함. | RS는 α&lt;0.2일때 높은 성능을 보이고 그 이후로는 줄어듦. 문장 내 단어의 순서를 너무 많이 바꾸면 성능이 떨어짐. | RD는 작은 α에 대해 잘 동작함. | 대체로 모든 operation들에 대해 α=0.1이 좋은 성능을 보였음. | . (5) How much augmentation? n에 대한 성능 분석 . n={1, 2, 4, 8, 16, 32} 로 실험하여 데이터셋의 크기별 최적의 α, n 값에 대해 제시함. . | 데이터 크기별 추천하는 파라미터값 . | . . 5. Reference . Paper: EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks | Code: https://github.com/jasonwei20/eda_nlp | .",
            "url": "https://inahjeon.github.io/devlog/nlp/2019/09/16/data-augmentation-in-nlp.html",
            "relUrl": "/nlp/2019/09/16/data-augmentation-in-nlp.html",
            "date": " • Sep 16, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "Data imbalance summary",
            "content": "Data imbalace 문제에 대한 설명과 해결방법에 대한 요약입니다. 기록용으로 작성되었습니다. . Dataset imbalance problem . 한 클래스가 다른 클래스들 보다 매우 많은 샘플 데이터를 포함하고 있는 데이터 셋 | 적어도 한 클래스가 매우 적의 수의 샘플 데이터만 포함한 경우 (minority class) | 일반적으로 imbalance 데이터에 대해 분류기가 majority class에서는 좋은 정확도를 보이지만, minority class들에서는 낮은 정확도를 보임 | . Ex) 신용카드 fraud detection . 신용카드 이상거래의 경우 정상 거래 케이스에 비해 케이스가 매우 적음 | 정상거래가 99%, 이상 거래가 1% 인 경우, 모든 케이스를 정상거래로 분류하기만 해도 전체 99%의 정확도를 얻을 수 있음. | 그러나 이상거래에 대한 정확도만 보면 매우 낮음. | . Solution . 1. Sampling . class imbalance를 낮추기 위해 training 데이터를 resampling하는 방법 | test data에는 적용하지 않음! | . 1) Under/Down sampling . majority class의 subset을 사용하는 방법. | 장점: 데이터가 balance되고 학습속도도 더 빨라짐 | ex) RUS (random majority under-sampling) | 단점: 중요한 학습 데이터를 없앨 수 있는 위험이 있음 | . 2) Oversampling . minority class를 중복해서 사용함으로써 class imbalance를 해소하는 방법 | 장점: 정보의 손실이 없음, under-sampling보다 성능이 좋음 | 단점: 데이터를 복제하기 때문에 overfitting의 위험이 있음 | . 3) SMOTE(Synthetic Minority Over-sampling Technique) . minor class의 분포와 일관성있는 새로운 샘플을 만들어내는 방식 | 고차원 데이터에는 효과적이지 않음 | . . 2. Ensemble . 데이터를 sampling 하는 방법 외 알고리즘 적으로는 주로 ensemble method를 적용하여 성능을 개선할 수 있음. (내용 생략) . Reference . https://towardsdatascience.com/comparing-different-classification-machine-learning-models-for-an-imbalanced-dataset-fdae1af3677f | https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/ | .",
            "url": "https://inahjeon.github.io/devlog/ml/2019/09/16/data-imbalance.html",
            "relUrl": "/ml/2019/09/16/data-imbalance.html",
            "date": " • Sep 16, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "Ensemble 요약",
            "content": "Hands-On Machine Learning with Scikit-Learn and TensorFlow 책에서 설명하고 있는 앙상블 기법에 대한 요약입니다. 기록용으로 작성되었습니다. . Ensemble learning . 여러 예측기를 연결해서 더 나은 예측기를 만드는 방법. | 성능이 낮은 분류기라도 여러개를 모았을 때 좋은 성능을 발휘하는 경우가 많음. | 각 예측기가 가능한 한 서로 독립적일 때 최고의 성능을 발휘함. | 다양한 예측기를 만드는 방법으로는 각기 다른 알고리즘으로 학습시키거나, 다른 데이터셋으로 훈련하는 방법이 있음. | . Ensemble methods . 앙상블 학습을 하는 알고리즘 . 앙상블을 적용한 투표 기반 분류기 . hard voting classifier: 각 분류기의 예측을 모아서 가장 많이 선택된 클래스를 예측하는 방법 | soft voting classifier: 각 분류기에서 클래스 별 확률 값을 구할 수 있는 경우, 클래스 별 확률의 평균값으로 확률이 가장 높은 클래스를 예측하는 방법 | . bagging &amp; pasting . 원본 데이터 셋을 bagging이나 pasting을 활용하여 여러 개의 샘플 데이터 셋으로 나누고 각각의 모델에 학습 시킨 후, 분류기일 경우 최빈 클래스로 분류, 회귀일 경우 평균값으로 예측함 . bagging(boostrap aggregating) . training set 에서 중복을 허용하여 샘플링하는 방식 (중복을 허용하는 리샘플링: boostraping) | 일반적으로 bagging이 pasting 보다 더 나은 성능을 보임 | . pasting . 중복을 허용하지 않고 샘플링 하는 방식 | . feature sampling . random subspaces method: feature sampling 만 사용 | random patches method: training set sampling과 feature sampling을 모두 사용 | . boosting . 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법. 앞의 모델을 보완해나가면서 일련의 예측기를 학습시키는 방법. | . ada boost . 이전 모델이 under fitting 했던 훈련 샘플의 가중치를 더 높임. | 새로운 예측기는 학습하기 어려운 샘플에 점점 더 맞춰지게 됨. | 분류기를 연속적으로 학습해야 하기 때문에 병렬로 수행하지 못해 확장성이 떨어짐 | . 샘플 → 분류기1 → 가중치샘플 → 분류기2 → .. → 최종 분류기 . gradient boost . ada boost처럼 예측기를 순차적으로 추가하는 방식으로, 이전 예측기가 만든 잔여 오차(residual error)에 새로운 예측기를 추가하는 방식 | . classifier_n = y - classifier_n-1.predict(x) . y_pred = sum(classifier.predict(x) for classifier in classifiers) . stocastic gradient boosting: 훈련 샘플의 비율을 지정해서 각 분류기가 무작위로 샘플링한 훈련셋을 학습하게 함. | . stacking &amp; blending . 앙상블에 속한 예측기들의 결과를 취합하는 모델을 학습시키는 방법 . stacking: 훈련셋을 out-of-fold (cross-validation) 방식으로 나누어서 학습시킴 | blending: 훈련셋을 hold-out 방식으로 나누어서 학습시킴 | . 참고자료 . Hands-On Machine Learning with Scikit-Learn and TensorFlow | https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/ | .",
            "url": "https://inahjeon.github.io/devlog/ml/2019/09/09/ensemble-summary.html",
            "relUrl": "/ml/2019/09/09/ensemble-summary.html",
            "date": " • Sep 9, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "DVC - Data version control",
            "content": "데이터 모델링 과정 중 데이터셋이나 모델을 관리하기 위한 도구인 DVC를 사용해보았습니다. . 1. DVC 란? . DVC는 머신러닝 프로젝트를 위한 open-source version control 시스템이라고 소개하고 있습니다. git과 유사한 형태로 ML 모델이나 사용된 데이터 셋을 관리할 수 있는 도구입니다. . (Reference: https://dvc.org) . 2. Installation . 설치하는 방법은 os에 따라 여러가지로 제공되고 있습니다. 저는 mac os 유저라 처음에는 homebrew로 설치해보았는데, 에러가 발생해서 pip을 통해 다시 설치했습니다. . brew install dvc . pip install dvc . 3. Initialize . DVC는 Git과 같은 버전 관리 시스템과 같이 사용합니다. . mkdir dvc-test cd dvc-test git init . 먼저 원하는 디렉토리에서 git init을 해주고, . dvc init . 위 명령어로 initialze해주면 . Adding &#39;.dvc/lock&#39; to &#39;.dvc/.gitignore&#39;. Adding &#39;.dvc/config.local&#39; to &#39;.dvc/.gitignore&#39;. Adding &#39;.dvc/updater&#39; to &#39;.dvc/.gitignore&#39;. Adding &#39;.dvc/updater.lock&#39; to &#39;.dvc/.gitignore&#39;. Adding &#39;.dvc/state-journal&#39; to &#39;.dvc/.gitignore&#39;. Adding &#39;.dvc/state-wal&#39; to &#39;.dvc/.gitignore&#39;. Adding &#39;.dvc/state&#39; to &#39;.dvc/.gitignore&#39;. Adding &#39;.dvc/cache&#39; to &#39;.dvc/.gitignore&#39;. You can now commit the changes to git. . 다음과 같이 뜨면서 .dvc 디렉토리가 생성됩니다. . 4. Configure . dvc 도 git 처럼 remote 저장소를 만들어서 다른 사람들과 쉽게 dvc 프로젝트를 공유할 수 있습니다. . remote 저장소는 local directory를 활용하거나, s3, gs, azure등 다양한 클라우드 스토리지도 활용가능합니다. . 지원하는 remote 시스템 종류 . local: local directory | s3: Amazon Simple Storage Service | gs: Google Cloud Storage | azure: Azure Blob Storage | ssh: Secure Shell | hdfs: Hadoop Distributed File System | http: HTTP and HTTPS protocols | . local remote . remote 저장소는 dvc remote add 명령어로 설정 가능합니다. 프로젝트 디렉토리에 내 dataset을 관리할 디렉토리를 생성하고 local remote 저장소를 설정했습니다. . mkdir dataset dvc remote add -d dataset /Users/jeoninah/dev/dvc-test-remote . remote 저장소 설정 후 .dvc/config 파일을 열어보면 다음과 같이 remote 저장소에 관한 설정 내용이 추가된 것을 확인할 수 있습니다. . [&#39;remote &quot;dataset&quot;&#39;] url = /Users/jeoninah/dev/dvc-test-remote [core] remote = dataset . S3 remote . aws s3를 remote 저장소로 활용하는 경우 다음과 같이 설정할 수 있습니다. . dvc remote add -d dataset s3://mybucket/myproject . s3를 저장소로 이용하는 경우에는 boto3를 같이 설치해야합니다. . pip install boto3 . 또, 해당 s3 저장소를 접근하기 위한 AWS_ACCESS_KEY_ID와 AWS_SECRET_ACCESS_KEY를 환경변수에 설정해주어야 합니다. . 5. Usecase . dvc 기능 테스트를 위해 dataset에 text.txt 라는 파일을 만들었습니다. . cat &gt; dataset/text.txt abcd . (1) dvc add . dvc도 git 처럼 add를 사용하여 변경사항을 시스템에 추가할 수 있습니다. . dvc add dataset . dvc로 add한 디렉토리나 파일들은 .gitignore에 추가되고 dvc에서 관리됩니다. git status로 dvc add 명령 전후의 상태를 비교해보면, . add 하기 전: | . On branch master Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: .dvc/config Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) dataset/ no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) . add 한 이후: | . On branch master Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: .dvc/config Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) .gitignore dataset.dvc no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) . dataset이라는 디렉토리 대신, dataset.dvc라는 파일이 생성되고 .gitignore가 변경된 것을 확인할 수 있습니다. . cat .gitignore /dataset . .gitignore 파일에 /dataset이 추가된 것을 확인할 수 있습니다. . (2) dvc push . remote 저장소에 push 하는 것도 git과 동일하게 아래 명령어로 push할 수 있습니다. . dvc push . Preparing to upload data to &#39;../dvc-test-remote&#39; Preparing to collect status from ../dvc-test-remote Collecting information from local cache... 100%|██████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00, 3584.88md5/s] Collecting information from remote cache... 100%|██████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00, 7121.06md5/s] Analysing status: 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00, 19021.79file/s] . 파일이 업로드 된 것을 확인할 수 있고, remote 저장소로 설정해두었던 dvc-test-remote 디렉토리를 보면 무언가 알 수없는(?) 디렉토리들이 생성된 것을 확인할 수 있었습니다. . ls dvc-test-remote 6e f5 . (3) dvc pull . remote 저장소에 저장된 내용을 받아오는 것도 pull 명령어로 가능합니다. dataset 디렉토리를 제거했다가 pull 로 다시 받아올 수 있었습니다. . rm -rf dataset dvc pull . 아예 새로운 곳에 데이터를 받아오고 싶을 경우, git 저장소를 clone해서 내용을 받아온 후 dvc pull로 데이터를 가져올 수 있습니다. . 특정 file 또는 directory 만 pull 하고 싶은 경우, 해당 데이터를 관리하는 some-file.dvc (dvc add some-file 시 생성되는 dvc 파일)파일만 지정해서 pull 할 수 있습니다. . dvc pull some-dir.dvc . 기타 . 위와 같은 버전 관리 기능 외에도 코드와 연결해서 데이터 파이프라인을 구성하거나 모델을 만들고 결과를 비교할 수 있는 experiment기능들도 있는데, 사용해보진 않았습니다. . 느낀점 . training 코드를 git에서 관리할 때 사용했던 dataset과 모델들은 따로 README에서 링크를 걸어두었는데, 데이터셋과 모델을 따로 관리할 수 있는 유용한 툴인 것 같습니다. | 익숙한 git과 동일한 인터페이스를 제공하고 있어서 사용하기도 매우 편리해서 앞으로 잘 활용해볼 것 같습니다. | .",
            "url": "https://inahjeon.github.io/devlog/ml%20/%20data%20engineering/2019/09/08/dvc.html",
            "relUrl": "/ml%20/%20data%20engineering/2019/09/08/dvc.html",
            "date": " • Sep 8, 2019"
        }
        
    
  
    
        ,"post13": {
            "title": "2019 if kakao 컨퍼런스 후기",
            "content": "세션: 밑바닥부터 시작하는 쇼핑 데이터 엔지니어링 고군 분투기 . 레거시 시스템 개선 과정 카카오 커머스 - 선물하기, 쇼핑하기, 다음 쇼핑 등 . 레거시 시스템의 스케일업 이슈 . 초장기 서비스 형태: 단일 DB구조 -&gt; 애플리케이션 -&gt; 서비스 | 서비스 개발 후 여러가지 운영 이슈 중 누적되는 데이터로 인해 확장의 이슈가 있었음 | . 현실적인 해결방안 . DB의 Join 쿼리 제거 &gt; 검색 기능은 elstic search 사용 / DB는 DB로만 | 새로운 데이터에 대해 색인을 만드는 작업이 필요함 | . 데이터 파이프라인 구조 . DB ~~&gt; Elastic search . 배치처리 | 실시간 처리 | 전체 복구 | . ES 색인을 어떻게 만들까? . 검색 대상이 되는 필드와 id만 색인함 | 검색에 최적화된 DB와 데이터 저장용 DB의 역할을 분리함! | . 데이터 변경 감지 솔루션 . CDC 기술 -&gt; 기술스택과 운영노하우가 필요함 | DB의 증분 필드를 pull하는 방식 &gt; 선택 | . Pull 방식 데이터 변경 감지 &gt; DB에 의해 자동 갱신됨. . modified_at, update_ts 을 사용하여 자동 갱신 | . 실시간 색인 . 실시간 데이터 변경 처리 . 변경된 데이터 -&gt; Kafka -&gt; ETL -&gt; ES | . 문제점 . 데이터의 순서 보장 필요 | 순서가 보장되더라도 적재된 이벤트를 replay하면서 순간적으로 과거 상태로 회귀하는 이슈 발생 -&gt; 해결 불가능 | 색인에 필요한 모든 데이터를 채워야 하는 부담 발생 | . (Eventual Consistency 문제) . 해결 . 변경된 데이터를 id만 kafka에서 받고, id로 DB의 데이터를 조회해서 색인 데이터를 가져옴 | . 실시간, 배치, 데이터 복구 파이프라인 . 실시간: spark streaming | 배치: spark batch (hopping 방식으로 안정적으로 처리) | 전체 색인 복구: spark recovery (예전데이터보다 최신 데이터부터 복구함) | data skew문제로 id 단위로 복구함 | . | . 데이터 품질을 유지하기 위한 노력 . 로직 검증을 위한 테스트 환경 구조 . DB fixture 데이터 생성 | Join service 호출 | ES join 쿼리 생성 | 쿼리 결과 검증 | . 검색 요구사항 별 테스트 케이스를 모두 만듦 . 운영환경의 복구 . zookeeper로 모니터링 및 자동 복구 | . 세션: 카카오와 다음의 컨텐츠들은 어떻게 분류되고 있을까? . 포털에서 콘텐츠 분류를 왜 하는가? . 대/소 분류체계로 콘텐츠가 분류되고 있음 | 이전엔 콘텐츠를 찾기위해 많은 단계를 거쳐야했음 -&gt; 찾는데에 많은 힘을 써야했음 | 콘텐츠 찾는 시간을 줄이기 위해 다 리스트업 했더니 너무 많고 복잡해짐 | 자동 분류가 필요해짐 | . 자동 분류 후 . 분류된 후 대량의 콘텐츠를 파악하기 좋아짐 | 기간 별 콘텐츠의 경향도 쉽게 파악할 수 있음 | 유저의 취향에 맞는 콘텐츠를 제공해주기 쉬워짐 | . 학습 셋 구축 과정 . 사례참고 . 카카오 내/외부 여러 서비스들에서 분류 체계를 참고함 | 카테고리 정보를 수집함 | . 현황 파악 뼈대 만들기 . 분류할 카테고리 체계를 구축함 | 분류 가능할만큼 데이터가 있는지 판단 | 너무 유사한 카테고리가 있지 않은지, 세부 카테고리가 존대하는게 나을지 | 감정형 카테고리를 넣을 것인지 판단 (다른 카테고리들과 잘 섞임) | . 카테고리 구체화 . 카테고리간 경계 명확화 | . 학습 셋 구축 . 카테고리별 학습셋 수백개 이상 | 제목, 본문을 발췌하여 카테고리 페이지별로 올림 | csv형태로 다운로드 | 하나의 학습셋으로 만듦 | . ML Model 타당성 검증 . confusion matrix로 검증 | . 카테고리 통합, 제외 . 정확도가 낮거나 효용성이 떨어지는 카테고리는 통합, 제외함 | . 현황 -» 통합/제외 * n회 반복 . 인사이트 . 카테고리의 개수가 적절할 떼 학습 성능이 좋았음 (개수 자체보다 도메인에 맞는 개수가 중요!) | 학습 셋의 숫자는 중요하다 (테스트 셋의 개수가 많은 편이 실 예측의 성능이 더 잘 나옴) | 학습셋 구축은 고품질 노동 집약적 분야임 한두 개가 아닌 최소 몇 만개 | 도메인 전문가가 필요 (겹치는 성격의 카테고리, 모호한 카테고리가 많음) | . | 학습 셋 구축 과정 효율화, 자동화 필요성 (데이터 양, 버전 관리, 카테고리 별 콘텐츠 수량 파악) | . 똑똑하게 학습 셋 관리하는 법 . 두가지 메인 포인트: 수정 (잘못 매핑된 데이터), 추가 (신규 데이터) . 학습 셋 어드민 . 카테고리 리스팅, 수량, 콘텐츠 추가/수정/삭제 | . 히스토리 관리 . 롤백 지원 | 더티체크 | . 색, 시각화를 통해 라벨링 검증 효율화 . 부정확한 데이터에 대해 시각화 -&gt; 가장 성능이 좋았던 챔피언 모델로 검증 | 카테고리 별 학습셋에 대해 오분류 개수, 정확도, 실 예측 정확도, 오분류 개수를 표시함 | . 얻게된 것 . 학습셋을 구성하는 운영 인력 감소 (7명 -&gt; 1~2명) | 지속적인 실제 예측 성능 향상 | . 기본모델, fastText알고리즘 . 좋은 모델의 요소: 정확성, 중요한 카테고리 잘 잡아줄 것, 학습 비용 저렴, 빠른 학습 시간, ..등 | 모델에 대한 요구사항을 세세하게 정의하고, 선정된 기준으로 여러가지 모델을 비교함 | . fastText 모델 . 특성 . 기본 성능이 높음 | 학습 시간이 빠름 (가성비 좋음) | API 응답 시간이 빠름 | GPU가 필수가 아님 | . 카카오에서 콘텐츠 분류(대/소카테고리)하는 세션 듣고 있는데 여기도 fastText가 프로덕션에서 쓸 때 다른 알고리즘들과 비교해서 기본 성능이 좋고, 학습 시간, API 응답 시간 등 여러 기준에서 퍼포먼스가 좋아서 잘 쓰고 있다고 하네용 ㅎㅎ . 학습 . 정답과 오답(정답을 제외한 나머지)을 함께 판별 | . 실제 분류가 이루어지는 단계 . 여러개의 모델과 룰 매핑 사용 | 룰매핑에 가중치를 두지만, 여러개의 모델의 만장일치면 룰 매핑을 이기도록 설정 | . 세션: 상품 카탈로그 자동 생성 ML 모델 소개 . 검색과 상품 카탈로그 . 검색 유입을 통한 유저는 60%가 추가 탐색을 하는 목적성 유저 . 검색결과 &lt;-&gt; 상품상세 (반복) -&gt; 최종 구매 | 탐색이 용이할 수록 구매전환율이 높아짐 | . 검색 품질에 있어 카탈로그 수와 매핑된 상품의 coverage는 매우 중요한 요소 . 정제된 상품명으로 검색 반응성이 높여줌 | 최저가 정보와 밀접. 상품 매핑수가 많아질수록 가격경쟁력이 올라감 | . 카탈로그 자동 생성? . 왜 머신러닝 모델로 만들었나? 유연성 필요 . 시즌, 트렌드에 민감 -&gt; 수명이 매우 짧음 | 특정 상품의 상품 수가 매우 적어질 수 있음 | 매일 신규 등록되는 상품 | . 각 쇼핑몰의 카탈로그 정보가 상이하고 부족한 정보가 많음 - 정제 x . 매핑 정확도가 낮아짐 | 상품 옵션 표기도 달라짐 (e.g. 18입, 12개 등) | . 자동화 적용 그리고 효과 . 자동화: 동일 상품 묶기, 타이틀 추출 | 품질: edge proportion(동일 상품일 가능성), 비지니스 로직 (서비스 담당자가 컨트롤 가능하도록) | 적용 범위: 옵션/스펙 유무, 카테고리 난이도 선정 | . 자동 카탈로그 (95% 정확도 / 수동: 92% 정확도) . 패션/뷰티위주 먼저 적용 | 수동vs자동 &gt; (2달치 생산량 - 1년치의 3배) | 기존 수동 카탈로그에서 누락했던 매핑도 추가됨 | . 상품 카탈로그 자동 생성 ML 모델 소개 . 어떤 데이터를 사용했는지? . 무엇을 보고 같은 상품이라고 생각할까요? . 상품 이미지 | 유사한 상품 이름 | 동일한 카테고리 | . 전체 과정 . 카테고리 별 상품 분류 (카카오 아레나에 있음) . Vector Model (상품 이미지, 상품 이름) - 고정된 크기의 벡터모델로 변환 . 상품 이미지: VGG19 모델 사용 | 상품 이름: 단어의 순서들이 다름, 명사가 적은 경우, 띄어쓰기가 없는 경우 | 자연어처리: 띄어쓰기, 불용어, 명사위주의 단어구성, 순서바뀌는 상품명, 비교 가능한 임베딩 벡터 | . 상품이름 벡터화 . 키워드 추출 (사내 라이브러리) | TF-IDF score 적용 (중요한 키워드에 가중치를 높게 줌) | 임베딩: word2vec (쇼핑상품명 만으로 따로 학습함) | TF-IDF * word2vec 사용 | . 왜 CNN, RNN을 안쓰는지? - 대화체등과 달리 순서가 중요하지 않음 . 유사상품 그래프: 서로 비슷한 상품을 연결지어 그래프로 표현 . 이미지 벡터가 유사한 (코사인 유사도 0.9 이상) 상위 100개의 상품을 뽑음 (approximate knn 사용) | 비슷하게 이름 벡터가 유사한 상품 추출 | . 엣지 케이스 . 실제로 서로 다른 상품인데 임베딩이 비슷함 (구글 홈 미니, 카카오 홈 미니) | 서로 같은 상품인데 임베딩이 많이 다름 (이미지가 많이 다른 경우) | approximate knn 으로 부정확한 경우 | . Community detection . Label Propagation (빠르게 수행하는 그래프 마이닝 알고리즘 사용) | . 카탈로그 품질 평가 . 유사상품 그래프의 연결된 정도로 평가점수를 측정함 | 특정 임계값 이상의 카탈로그만 적용함 | 생성시간: 상품 개수에 따라 거의 선형 증가함 | . 상품 카탈로그 생성 더 빠르게 하기 . 그래프 요약 | 분산처리 | 휴리스틱 등 | . . 적용해 볼 것들 . 학습 셋 운영/관리 전략 | 모델의 능력치에 대한 요구사항을 세부적으로 정의 (높은 정확도 만이 아닌) | 룰 매핑과 ML모델 혼합해서 활용 | .",
            "url": "https://inahjeon.github.io/devlog/conference/2019/08/30/ifkakao.html",
            "relUrl": "/conference/2019/08/30/ifkakao.html",
            "date": " • Aug 30, 2019"
        }
        
    
  
    
        ,"post14": {
            "title": "Kubeflow on GCP (1)",
            "content": "최근에 회사에서 머신러닝 워크플로 관리를 위해 툴들을 리서치 해보고 있습니다. 그 중에서 가장 잘 알려지기도 한 kubeflow를 소개하고 간단하게 GCP에 deploy하는 것을 해보려고 합니다. . 사실 간단한 서비스라면 AWS의 Sage Maker나 GCP의 AI Platform의 기능들을 활용해도 충분한 것 같습니다. . 1. Kubeflow 란? . kubeflow 는 kubernetes 에서 ML 워크플로(데이터 전처리, 모델 학습, 예측 서비스 및 서비스 관리)를 구축하기 위한 툴킷입니다. . Kubeflow에서 제공하는 메인 기능들은 다음과 같습니다. . Jupyter Notebooks: Using Jupyter notebooks in Kubeflow . | Hyperparameter Tuning: Hyperparameter tuning of ML models in Kubeflow . | Pipelines: ML Pipelines in Kubeflow . | Serving: Serving of ML models in Kubeflow . | Training: Training of ML models in Kubeflow . | . 2. Deploy Kubeflow on GCP . Kubeflow document에서 Kubeflow on GCP - Deploying Kubeflow 라는 메뉴에 보면 GCP에서 kubeflow를 설치하는 방법을 step별로 잘 설명해 둔 문서가 있습니다. . 설치 방법은 CLI를 이용하는 방법과 kubeflow 에서 제공하는 웹 UI를 이용하는 방법이 있는데, 저는 빠르게 kubeflow를 세팅해서 써보고 싶은게 목적이라 훨씬 간편하게 할 수 있는 후자의 방법을 선택했습니다. . 웹 UI를 이용해서 Kubeflow 세팅하기 . 웹 UI는 https://deploy.kubeflow.cloud/#/deploy 링크로 들어가면 보실 수 있습니다. . . 위와 같이 Project id, deployment name, login 방식, GKE zone 등의 설정만 선택하면 간단히 Kubernetes 위에서 돌아가는 kubeflow를 구성하실 수 있습니다. (세상 편함…) . 로그인 방식은 차후 kubeflow endpoint에 접근하기 위한 로그인 방식입니다. . Login with GCP Iap: Oauth client를 생성하고, 구글 계정으로 로그인 | Login with Username and Password: 로그인할 사용자 계정과 password를 입력해서 해당 계정으로 로그인 | . 저는 GCP IAP 방식으로 설정했습니다. 이것도 https://www.kubeflow.org/docs/gke/deploy/oauth-setup/ 를 보시면 쉽게 따라서 설정하실 수 있습니다. . . 위와 같이 적절히 원하는 설정값을 입력하고 Create Deployment 버튼을 누르면, 아래에 진행상황 창이 뜨면서 클러스터를 구성하기 시작합니다. . 클러스터 세팅까지는 10분정도, kubeflow UI에 접속가능한 endpoint가 구성되기까지는 20분정도 소요됩니다. . 아래 진행상황 창에서 다음과 같이 완료 시점도 알려줍니다. . your kubeflow service url should be ready within 20 minutes (by 오후 7:47:29): https://kubeflow-test-1.endpoints.your-project-id.cloud.goog . GCP - Kubernetes Engine에서 클러스터 확인 . 구성된 클러스터는 GCP 콘솔의 kubernetes Engine 메뉴에서 확인할 수 있습니다. . . 클러스터의 노드 풀을 확인해보면 다음과 같은 스펙으로 클러스터가 구성되어 있습니다. . . n1-standard-8 머신 2개를 사용하고 있고 디스크는 노드 당 100GB로 설정되어 있습니다. . (가격표는 아래 참고) . . 클러스터 설정할 때 가까운 곳이 asia-east1 밖에 없어서 여기로 설정했는데, 타이완이었네요. . 서비스 및 수신 메뉴에서는 띄워진 서비스들을 확인할 수 있습니다. . . envoy-ingress 옆에 나와있는 엔드포인트로 kubeflow UI에 접속가능합니다. . 참고로 endpoint url 형태는 https://&lt;cluster-name&gt;.endpoints.&lt;gcp-product-id&gt;.cloud.goog/ 입니다. . 3. Kubeflow UI . . 위 엔드포인트 주소로 접속해서 클러스터를 구성할 때 설정한 로그인 방식으로 로그인하면 다음과 같은 화면을 보실 수 있습니다. . 왼쪽 메뉴는 kubeflow의 document와 제공하고 있는 기능들에 대한 대시보드들입니다. . Notebooks | TFJob Dashboard | Katib Dashboard | Pipeline Dashboard | . 맺음말 . 이렇게 30분만에 간단히 kubeflow를 띄워봤습니다. (만세 &gt; &lt;!) . 여담이지만, 회사에서는 AWS를 주로 활용하고 있긴한데 공부할 때나 혼자 toy project를 할 때는 주로 GCP를 활용하게 되는 것 같습니다. . 인프라 쪽 지식이 별로 없는 제가 볼 때 커스텀한 설정없이 기본 설정으로 특정 서비스를 써보는 것은 AWS에 비해 GCP가 문서화가 잘 되어있고 단계 별로 스크린샷도 잘 제공하고 있어서 더 접근성이 높다고 느껴지는 것 같습니다. :) . 다음 글에서는 kubeflow의 각 기능들에 대해서 알아보고 간단한 예제 데이터를 활용해서 전체 워크플로를 한번 돌아보는 걸 해보려고 합니다. .",
            "url": "https://inahjeon.github.io/devlog/ml%20/%20data%20engineering/2019/08/11/kubflow-1.html",
            "relUrl": "/ml%20/%20data%20engineering/2019/08/11/kubflow-1.html",
            "date": " • Aug 11, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "Little Big Data",
            "content": "(컨퍼런스에 참석하면 세션에서 들었던 내용을 잘 정리해두려고 하는 편인데, 오늘은 어쩐지 집중이 잘 되지 않아서 대충 정리했다.) . 세션 1: TF에서 팀 빌딩까지 9개월의 기록 : 성장하는 조직 만드는 여정 - 변성윤님 . 늦게 도착해서 앞부분 놓침 ㅠ 나중에 발표자료 링크해놔야지. | . 세션 2: Data는 Sports를 어떻게 바꿨는가? - 김인범님 . 1. 머니볼로 시작된 변화의 바람 . 미국 프로 스포츠 시장 . MLB야구 관련 수입 11조 | 매년 성장하는 시장 | 구단들의 선수영입 경쟁이 치열함 | . 2. 머니볼: 최소한의 비용으로 극대화된 가치를 얻기 위한 시도 . 누가 더 훌륭한 선수인가? - 새로운 관점 선수평가 지표로 classic stat 이 사용되었지만, 더 면밀한 평가지표를 도입하게됨. . ex. WAR . 선수가 팀 승리에 얼마나 크게 기여했는가를 보여주는 지표 | WAR 1점 당 500만불의 가치 | . 3. 데이터에 기반한 전술의 변화 . 경기 수집 방식이 달라짐 . 수기 기록 -&gt; 영상 데이터 수집 -&gt; 스탯캐스트 | 기본적인 기록을 넘어 순간순간 발생하는 다양한 상황의 데이터까지 수집이 가능함. | . 전술변화 . 수비 시프트 (당겨치기할 확률 80%인 타자라면 수비수들이 시프트함) | 투구 회전 수를 통한 투수 평가 (정상급 투수들의 투구 회전수 2500. 타 선수들의 평균 회전수는 2000) | 발사각도 이론 (홈런을 칠 확률이 높은 발사각도 수정 - 후보선수였던 저스틴 터너 선수가 발사각도를 수정하여 올스타 플레이어가됨) | . 4. GM(단장)과 Agent의 역할 강화 . GM . 빌리 빈, 테오 단장의 사례 | . Agent . 선수가 최선의 계약을 맺을 수 있도록 도와주는 역할 . 스캇 보라스 사례 | . 새롭게 시도하고 있는 분야 . 선수들의 부상과 회복 | 선수들의 심리 상태 | Team Chemistry | . 야구데이터 &gt; 공격과 수비가 철저하게 나뉘어 있어, 데이터 분석이 다른 미식축구나, 농구에 비해 용이함. . 세션 4: ML엔지니어가 기획자, 개발자와 협업하는 법 - 백영민님 . 대화 . 일상 대화 . 일상에서 하는 모든 대화 | 정답이 여러개 | 팟 캐스트 들어봤어? | . 기능 대화 . 팟캐스트 틀어줘 | . 대화 시스템의 구성 . Query -&gt; Model(Dialogue Manager) -&gt; Reply . 모델 Retrieval Method . 사전에 정해진 답변 중 가장 좋은 것을 찾자 | . Generation Model . 새로운 답변을 생성 | . 대화 데이터의 특징 . 일상 대화 데이터의 수집 &gt; 연애의 과학 . 특징: . 대화의 도메인이 일상 생활 전 범위로 매우 넒음 | 사람마다 조금씩 다른 패턴 (말투, 자주사용하는 단어 등) | 줄임말 및 잦은 오타 (그냥 -&gt; 걍) | 신조어의 사용 (갑분싸) | 독자적인 자음의 사용 (ㅋㅋㅋ) | . 데이터의 선택 &amp; 전처리가 특히 중요함 . 유저필터링 세션 필터링: 공통된 주제가 유지되는 시간 Normalize: 의미 없는 토큰의 치환 및 삭제 . Multi-turn Reaction Model . 개발 과정에서의 협업 문제정의 - 목표설정 - 데이터 구축 - 모델 테스트 - 피드백 - 실제 검증 문제정의 . 비전을 추구하면서 모두가 공감할 수 있는 주제 -&gt; 살아있는 대화를 할 수 있는 인공지능을 만들자! . 기존모델: Single-turn Reaction Model 유저의 이전 발화를 보고 사전에 정의된 class st에서 리턴 . 유저의 이전턴 하나의 발화 만으로 대답 구성 | 이전에 했던 말을 번복하는 경우 발생 | . 목표설정 . 목표: Multi-turn Reaction 모델을 만들자 . 데이터 구축 . 관점 . 기획자) . 어떤 답변 셋을 이요하면 더 사람같은 답변을 제공할 수 있을까? | 어떤 답변 모델의 목적에 맞는 좋은 답변일까? | 모델의 목표를 평가하기 위한 테스트 셋을 어떻게 구성할까? | . 엔지니어) . 어떻게 하면 쉽게 학습 데이터를 구축할 수 있을까? | 레이블링을 쉽게 할 수 있는 방법은 무엇이 있을까? | 어떤 데이터로 학습을 하면 수렴을 잘 할 수 있을까? | . 정성적 평가 셋 . 사람이 볼 수 있는 정도 쿼리 쌍을 레이블링함 | . 모델 테스트 . Baseline과 BERT 사용 | . 피드백 . 정량적 평가 셋으로 정량적 평가를 먼저 함 | 정성적 평가 with 기획자 | . 모든 진행상황은 기록으로 남기자. ML을 모르는 사람이 이해할 수 있도록 . 데모 페이지 작성 및 테스트 . 성능을 테스트 해볼 수 있도록 . 4. 최적화 및 제품화 (제품에 들어가기 위한 최적화) - 개발자와 협업 . 학습 코드를 정리/최적화 + 서비스 코드 작성 . 코드의 최적화 | 프로파일링 | . 결과 모델의 제품화 . 보기 좋은 UI 및 사용자를 고려한 UX | .",
            "url": "https://inahjeon.github.io/devlog/conference/2019/08/03/litte-big-data.html",
            "relUrl": "/conference/2019/08/03/litte-big-data.html",
            "date": " • Aug 3, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "FastText Pre-trained 한국어 모델 사용기",
            "content": "1. FastText 소개 . FastText는 Facebook에서 만든 word representation 과 sentence classification의 효율적인 학습을 위한 라이브러리입니다. . 일반적으로 자연어처리에서 말뭉치 사전 데이터 수집하고 전처리하는 데 많은 시간이 소요됩니다. . 그런데 facebook에서 fasttext를 소개하면서 무려 157개국의 언어에 대해 common crawler와 wikipedia의 데이터를 학습한 Pre-trained model을 제공하고 있습니다. . 한국어도 제공하고 있습니다. 만세! . 글에서는 미리 훈련된 데이터가 어느정도 성능이 나올 지 테스트 해보았습니다. . 2. Fast Text 설치 및 pre-trained 모델 다운받기 . FastText는 공개된 github 저장소 https://github.com/facebookresearch/fastText.git를 clone해서 하거나, gensim 이라는 파이썬 패키지에 포함되어 있어 gensim을 설치해서 사용할 수 있습니다. . 여기서는 gensim을 설치해서 사용해보았습니다. . pip install gensim . 미리 훈련된 한국어 모델은 데이터는 https://fasttext.cc/docs/en/crawl-vectors.html에서 제공하고 있습니다. . 모델은 .vec .bin 의 두 가지 형태의 파일로 제공 하고 있는데, .vec 파일은 라인마다 단어에 대한 vector가 있는 형태이고 .bin 파일은 벡터 뿐만 아니라 dictionary와 모델의 하이퍼 파라미터와 같은 정보들이 모두 들어있는 형태입니다. . binary 파일을 사용하면 모델을 추가로 학습해서 개선할 수 있다고 합니다. . 둘다 다운 받고 압축을 풀었보았습니다. (모델을 읽어서 쓸 때는 굳이 압축을 풀지 않고 사용해도 됩니다.) . gunzip cc.ko.300.vec.gz gunzip cc.ko.300.bin.gz . (압축을 푸니 gunzip cc.ko.300.vec 은 4.5GB, gunzip cc.ko.300.bin 은 7.2GB로 용량이 매우 컸습니다. ㄷㄷ) . 3. Pre-trained 한국어 모델 적용 . 미리 훈련된 모델로 비슷한 단어 추출해보기 . 훈련된 모델을 이용해서 파이썬 이라는 단어와 유사한 단어를 출력해보았습니다. . 모델 파일의 크기가 매우 커서 그런지 모델을 로딩하는 시간이 거의 3분~4분 정도 걸립니다. (테스트 할때마다 시간이 오래걸려, 코드를 짤 때는 다른 테스트용 모델로 작업하고 모델만 바꿔서 돌렸습니다.) . from gensim import models ko_model = models.fasttext.load_facebook_model(&#39;cc.ko.300.bin&#39;) for w, sim in m_fasttext.similar_by_word(&#39;파이썬&#39;, 10): print(f&#39;{w}: {sim}&#39;) . 파이썬과 유사한 단어 결과: . Python: 0.565061628818512 자이썬: 0.5624369382858276 레일스: 0.5598082542419434 파이썬을: 0.5595801472663879 언어용: 0.5288202166557312 파이썬의: 0.5250024795532227 프로그래밍: 0.5225088000297546 wxPython: 0.5222088098526001 파이썬이나: 0.5201171636581421 함수형: 0.5187377333641052 . 아이언맨과 유사한 단어 결과: . 아이언맨2: 0.6507514119148254 아이언맨과: 0.6068165302276611 아이언맨에: 0.5806230306625366 아이언맨의: 0.5622424483299255 스파이더맨: 0.5526455044746399 아이언맨3: 0.5465470552444458 아이언맨은: 0.5207207798957825 가디언즈오브갤럭시: 0.5029110908508301 헐크버스터: 0.5008623600006104 어벤져스: 0.4941202402114868 . 단어 간 유사도 비교 . 다음과 같이 단어의 유사도도 쉽게 비교해 볼 수 있습니다. . print(f&quot;&#39;아이언맨&#39;과 &#39;헐크&#39;의 유사도: {m_fasttext.similarity(&#39;아이언맨&#39;, &#39;헐크&#39;)}&quot;) print(f&quot;&#39;아이언맨&#39;과 &#39;스파이더맨&#39;의 유사도: {m_fasttext.similarity(&#39;아이언맨&#39;, &#39;스파이더맨&#39;)}&quot;) . &#39;아이언맨&#39;과 &#39;헐크&#39;의 유사도: 0.4033605754375458 &#39;아이언맨&#39;과 &#39;스파이더맨&#39;의 유사도: 0.5526454448699951 . 아이언맨은 스파이더맨과 더 친한 것으로.. . 단어 벡터 시각화 . 이번에는 차원축소기법인 PCA를 사용해서 단어 벡터들을 2차원으로 축소시키고, 다음과 같이 2차원 그래프에 표현해보았습니다. . from sklearn.decomposition import PCA words = [ &#39;사과&#39;, &#39;바나나&#39;, &#39;오렌지&#39;, &#39;키위&#39;, &#39;스파이더맨&#39;, &#39;아이언맨&#39;, &#39;헐크&#39;, &#39;타노스&#39;, &#39;캡틴아메리카&#39;, &#39;어벤져스&#39; ] # matplotplib 폰트 설정을 안해서 그래프에서 한국어 라벨이 깨져서 아래 단어로 임시처리했습니다. ㅠ word_labels = [ &#39;apple&#39;, &#39;banana&#39;, &#39;orange&#39;, &#39;kiwi&#39;, &#39;spider man&#39;, &#39;iron man&#39;, &#39;hulk&#39;, &#39;thanos&#39;, &#39;captin america&#39;, &#39;avengers&#39; ] pca = PCA(n_components=2) xys = pca.fit_transform([m_fasttext.wv.word_vec(w) for w in words]) xs = xys[:,0] ys = xys[:,1] plt.figure(figsize=(14, 10)) plt.scatter(xs, ys, marker=&#39;o&#39;) for i, v in enumerate(word_labels): plt.annotate(v, xy=(xs[i], ys[i])) . 결과: . . 그래프에서 사과, 바나나, 키위 같은 과일 단어들은 그리 가깝게 뭉쳐지진 않았지만, 아이언앤, 스파이더맨, 어벤져스 등의 특정 고유명사들은 연관된 단어들끼리 잘 뭉쳐진 걸 확인할 수 있었습니다. (왠일인지 헐크는 저멀리 떨어져 있지만…) . 느낀점 . pre-trained 모델이 생각보다 괜찮은 것 같은데, 모델을 로딩하는 시간이 매우 느립니다. ㅠ | 훈련된 모델에 데이터를 추가해서 더 학습이 가능한 걸로 보여서 특정 도메인에 맞는 학습 데이터를 더 수집해서 성능을 비교해보면 좋을 것 같습니다. | fasttext를 테스트해보면서 다른 공개된 한국어 모델이나 데이터셋을 찾아봤는데, 잘 정리된 곳을 찾진 못해서 따로 정리해보면 좋을 것 같습니다. | .",
            "url": "https://inahjeon.github.io/devlog/nlp/2019/07/21/fasttext.html",
            "relUrl": "/nlp/2019/07/21/fasttext.html",
            "date": " • Jul 21, 2019"
        }
        
    
  
    
        ,"post17": {
            "title": "글또 3기를 시작하며",
            "content": "글또 3기 다짐 및 상반기 회고 . 글또 3기를 시작하며 &amp; 다짐 . 지난 글또 2기에 11월 부터 참여하면서 블로그 글을 습관적으로 많이 쓰게 된 것 같다. 어느정도 글쓰기 습관이 만들어졌다고 생각했지만, 2기가 끝나고 나서 두달간 2편 정도 밖에 추가로 쓰지 못했던 걸 보면 아직은 좀 더 노력이 필요할 것 같다. (물론 회사 프로젝트가 빡셌던 탓도 있지만…ㅠ) . 다짐 . 2기때는 다짐 &amp; 회고글을 제외하고 6편 정도였지만, 3기에는 10편이상 쓰는 것이 목표 | 패스권 안쓰기! (이건 매번 다짐 하지만….) | 데이터 분석 쪽 프로젝트 진행 &amp; 글쓰기 (지난번에도 관련 주제로 글쓰고 싶었지만 실패해서 이번에도 다시 시도) | . 블로그 글쓰기 &amp; 발표 . 나름 일하면서 새롭게 시도했던 것 배운 점들을 차곡차곡 쌓아서 글로도 정리하고 외부 발표를 많이 해야지라고 생각했지만, 사내에서 발표했던 것 말고는 딱히 없었던 것 같다. 역시 발표는 일단 한다고 질러놓아야 어떻게든 연구를 하고 시작하게 되는 것 같다. 하반기 때는 무언가 1개라도 다른 사람들 앞에서 자신있게 어떤 주제에 대해서 말할 수 있다면 좋겠다. . 최근의 고민: 어떤 문제를 풀 것 인가 . 외부 컨퍼런스에 참가했던 걸로는 최근에 참가했던 2019 DevGround 가 상당히 괜찮았던 것 같다. 사실 이전 까지는 어떤 문제를 풀 지 보다도 어떤 데이터를 다루어보고, 어떤 알고리즘이나 다른 라이브러리등을 사용해서 문제를 풀고 새로운 기술을 공부하는 데만 신경썼던 것 같다. 최근에는 회사에서도 그렇고 데이터로 어떤 문제를 풀어야 임팩트가 있을까 하는 고민이 많았는데 DevGround에서 다른 회사 사례들을 보면서 어느정도 실마리를 찾게 되었던 것 같다. 이전까지는 다른 곳들도 봐도 데이터만 정작 쌓아놓고 비지니스 밸류로 만들지 못하고 있던 단계였는데, 서서히 성공 사례들이 늘어가고 있는 것 같다. .",
            "url": "https://inahjeon.github.io/devlog/diary/2019/07/07/start-geultto3.html",
            "relUrl": "/diary/2019/07/07/start-geultto3.html",
            "date": " • Jul 7, 2019"
        }
        
    
  
    
        ,"post18": {
            "title": "DevGround 2019 세션 요약",
            "content": "Summary . 세션1: 데이터와 머신러닝이 비즈니스와 만날 때 발생할 수 있는 비극들 - 하용호님 (Kakao) 비즈니스에서 데이터 분석과 머신러닝 도입에서 장애와 성공적인 밸류를 만드는 법 . | 세션2: AI 프로젝트 간지나게 잘 진행하는 법 - 백정상님 (Google) AI 프로젝트 셋업을 위해 필요한 것들 . | 세션3: 온라인 게임 데이터 분석 사례와 향후 과제 - 이은조님 (NCSoft) 현업에서 데이터 분석을 적용하면서 맞닥뜨리는 현실적인 문제들과 해결 아이디어 . | 세션 5: 맛있는 데이터를 물어다주는 멍멍이 - 노상래님 (마켓컬리) 엑셀 시대에서 실시간 대시보드 &amp; 예측 시스템 시대까지 데이터 분석을 비즈니즈에 도입한 과정 . | 세션 6: MOBILITY X DATA : 모빌리티 산업의 도전 과제 - 변성윤님 (쏘카) 모빌리티 산업의 데이터와 다양한 문제들에 대해 소개 . | 세션 7: 데이터가 흐르는 조직 만들기 - 양승화님 (마이리얼트립) 데이터가 흐르는 조직을 위해 시도한 실질적인 노력들 . | . 세션1: 데이터와 머신러닝이 비즈니스와 만날 때 발생할 수 있는 비극들 - 하용호님 . 데이터에서 패턴을 찾아내어 비지니스 기회로 . 많은 회사들이 업무에 데이터와 머신러닝을 도입하고 싶어하지만 잘 안됨. . 왜 -&gt; 많은 사람들이 데이터로 일을 해본 적이 없기 때문 . 대표적인 잘못된 회사의 데이터 사업 계획 데이터를 모아서 추천도하고 프로파일링도하고.. 유저의 성향을 파악하고 인사이트를 도출 후, 마법의 뿅 . 구슬이 서말이어도 꿰어야 보배 . 일단 서말이 안된다 (데이터가 없다) | 꿰는 기술이 없다 (관련 전문가가 없다) | 보배 (뭐가 보배인지 모른다) | . 구슬이 서말이어도 꿰어야 보배 -&gt; 가능하려면 . -&gt; 엄청난 데이터 필요 -&gt; 엄청난 서버 -&gt; 엄청난 엔지니어 —-&gt; 다없다 . 1. 일단 서말이 없다. . 데이터로 뭐하지 &gt; 추천이나 광고에 쓸거에요 -&gt; 사실 엄청난 데이터가 필요하다 . 기업의 데이터는 2가지 폼으로 존재 -&gt; 없거나, 쓸 수 없거나 ex) 사장님: 우리 회사 데이터 진짜 많다 오면 뭐든지 할 수 있다. ~다 거짓말이다…~ . | 추천으로 의미가 있기 위해서는 MAU 20만은 필요 (다운로드 기준 100만) | 광고기준으로는 적어도 MAU 200민 (다운로드 기준 1000만) | 유저마다 수십 수백건 서비스 사용기록이 필요 | . 추천, 광고 등으로 재미보는 회사는 정해져있다 . 재미보는 회사 -&gt; 커머스: 아마존, 쿠팡 | 광고로 재미보는 회사 -&gt; 구글, 페이스북, 네이버, 카카오 | 본업이 추천, 광고를 하는 회사임 | . 회사는 노력을 본질에 집중해야 한다. . 초기라면 그냥 인기 순위로도 충분하다. | 더 쉽고 빠르고 편한 유저 플로우를 만드는 것이 좋다. | . 2. 꿰는 기술이 없다. . 인력 -&gt; 일단 비싸고 구하기 힘듦 . 머신러닝 엔지니어 6천 | 좋은 엔지니어 1억 | 훌륭한 엔지니어 싯가 -&gt; 일반 대기업은 매력적인 직장이 아님 | . 데이터도 없고 회사의 핵심이 다른 곳에 있기때문에 가지 않음. . 선호도: 카카오 &gt; 네이버 &gt; 스타트업 &gt; 통신사들 &gt; 전자회사들 &gt; 그외… . 3. 보배: 비지니스와 만남 기술은 이익을 만들어내야함 . 실패하는 환상적인 만남 . 뭔가 새로운 것을 도전하고 싶은 상위권자 | 빅데이터와 머신러닝도 쓰는 간지나는 서비스를 꿈구는 기획자 | 데이터와 머신러닝을 공부했지만 현업적용은 못해본 열정적인 엔지니어 | . 실패하는 이유 . 머신러닝을 쓰고 싶다 -&gt; 우리 문제에 어떻게 구겨넣지 (x) . 잘못된 곳에 적용하거나 필요하지 않은 상황에 적용 . 핵심: 메인 비즈니스의 밸류 체인에서 비효율 구간을 찾아야 한다 . 비지니스 밸류 체인 . 회사의 메인비지니스 ———————–&gt; | 뭔가 멋진것 (새롭게 데이터로 하는 비지니스) —&gt; | 새로운 비지니스는 시간이 걸리고 파급이 적다 | . 회사에서 데이터로 밸류를 만들 수 있는 부분: 메인 비지니스 밸류 체인에서 비효율 적인 부분을 효율적으로 하는 것 . 사람이 감으로 하거나 | 사람이 하기에 느려지는 부분 (병목인 부분) -&gt; 머신러닝과 데이터를 이용해, 대체하여 자동화 하거나, 판단을 보조하여 빠르게 한다. | . 복잡해 보인다고 답은 아니다. . 예제: 매출을 올리기 위해, 메인 상품 진열 순서는 어떻게 하는게 좋을까? 간단한 데이터 활용 멋있고 간지나는 머신러닝: 유저마다 프로파일하여, 최적을 추천 . 세상 모든 것은 ROI . 1) 룰 베이스 접근으로 60짜리를 빠르게 10개 만들 수 있음 &gt; 600개 2) 머신러닝 접근으로 80짜리를 1개 만들 수 있음 &gt; 80개 . 기회비용 일단 가장 심플한 방법을 방치하고 말고 빨리 하는 것 심플에서 충분히 뽑아내고 있을 때 머신러닝을 시도하는 것이 좋음 . 그럼 언제,왜 머신러닝 하는가? 규모가 커지면서, 심플한 방법을 도저히 매니지할 수 없을 때 . 카카오에서 한 것: 플러스 친구 메시지 최적화 . 카카오는 뭐로 돈 벌지? 광고(이미 잘하고 있음), 메시지(돈버는 것 -&gt; 플러스 친구 메시지) . 플러스 친구 메시지 &gt; 본질과 밸류 체인이 뭐지? . 무엇을 보낼 것인가: 컨텐츠 셀렉터 | 누구에게 보낼 것인가: 반을 잘 할 유저를 선택 | 효과는 어땠나: 사람이 일일히 분석하지 않도록 로봇 분석가를 개발 | . 세션2: AI 프로젝트 간지나게 잘 진행하는 법 - 백정상님 . 1. 성공 &amp; 실패하는 머신러닝 프로젝트 . 멋지고 분위기 좋은 팀(like Brain팀)의 성공조건 . 세계 최고 수준의 팀을 기반으로 | 해결하고자 하는 비즈니스 문제가 굉장ㅎ ㅣ크고 아름답고 | 그 문제를 해결하면 생기는 비즈니스 임팩트가 커야 함 | 무조건 성공한다는 확신이 있어야함 | 실패 가능성을 최소화 해야함 | . 실패하는 머신러닝 프로젝트의 이유들 . 비즈니스에 대한 이해 부족 | 낮은 데이터 품질 (로그를 쌓는 시간은 전체 개발 시간에 비해 턱없이 부족) | 잘못된 머신러닝 사용 | 편견 또는 확증편향 | 부족한 인프라 지원 | 부실한 계획과 거버넌스 부재 | . 머신러닝 프로젝트를 실패하지 않으려면 . 풀어야 하는 비즈니스의 임팩트가 충분히 크고 | 비즈니스 도메인 지식이 충분해야 하고 | 높은 품질의 데이터를 쉽게 획득할 수 있어야 하며 | 머신러닝이 실제 프로젝트에 도움이 되어야하고 | 편견이 생기지 않도록 중심을 잡아줄 데이터 사이언티스트가 필요하며 | 비용 효율적이며 충분한 인프라를 확보하고 | 충분한 프로젝트 여정에 대한 계획을 기반으로 | 최고 의사 결정자의 서포트를 충분히 받아 진행해야하며, 그로 인해 충분히 일정이 쪼야여 됨 (기본 유지비가 꽤 큼) | . 2. AI 프로젝트 셋업 . 비즈니스 케이스 탐색 . 크게 생각해야함. Think x 10 | 팀 유지비 배비 최소 10배를 더 벌어주는 프로젝트여야 함 | . 최초 머신러닝 팀 빌딩 . 프러덕트 -&gt; 프로덕트 매니저 (1) | 비즈니스 -&gt; 비즈니스 분석가 (1) | 데이터 사이언스 -&gt; 데이터 사이언티스트 (1) | 머신러닝 -&gt; 머신러닝 엔지니어 (1) | . 프로젝트 예산 =&gt; 8억 (투자비용) . 팀 인건비 -&gt; 4명 월급 4000만원 | 인프라 비용 -&gt; 3억 (하둡 클러스터 온프레미스) | 소프트웨어 구입 및 구독 (1000만원) | 예상 개발 기간 (1년) =&gt; 8억 ==&gt; 80억을 버는 프로젝트를 찾아야함 | . 프로덕트 디자인 및 마일스톤 플래닝 . 쉽게 말하면 제품 기획 | 풀어야 할 비즈니스 문제를 명확하게 정의 | 프로적트를 통해 얻는 비즈니스 임팩트를 계측 가능하도록 정의 | . 데이터 디자인 . 데이터 = 돈 | 프로턱트에서 필요로 하는 모든 데이터는 수집할 수 있어야 함 | 비교적 유연하게 변경 가능한 JSON으로 디자인 하는 경우가 많음 | . 밸류 임팩트가 큰 데이터 . 구조화된 데이터 | 시계열 | 이미지 | 비디오 | 텍스트 | 오디오 | . 데이터 파이프라인 구축 . 데이터의 유실이 없어야 함 | 중복된 데이터 허용 및 dedup | 가급적이면 매니지드 서비스 혹은 ETL플랫폼을 활용 | . 데이터 분석 . EDA | 데이터 상관관계 분석 | 통계적 검증: 빈도검증, 타당도 검증, z 스코어 검증, t 스코어 검증 | 고전적 머신러닝 회귀모형 클러스터링 | 데이터의 품질과 특징을 분석해야 한다 | 데이터 분석만으로도 문제를 해결하는 경우가 많음. 이 경우에는 바로 비즈니스 임팩트를 만들고 다음 프로젝트를 간지나게 시작한다. | 통계나 머신러닝으로 threshold를 구할 수 있다면 룰 베이스 모델 구현 | . 머신러닝 시작 . 분석 결과에 따른 최적의 모델 선택 | AI툴셋 - AI hub | 모델 개발: 텐서플로, pytorch | . 모델 학습 및 평가 . 피쳐 셀렉션의 두 가지 전략: 다 넣자 vs 상관관계에 따라 선택하자 | 절충안: 다 넣고 상관관계가 높은 피쳐에 웨잇을 더 가하자 | 피쳐 엔지니어링 &amp; 셀렉션 작업 시작 (data prep등 활용) | 모델 배포 &gt; 아무거나 써도 됨 . | . 비즈니스 임팩트 실현 . 데이터 QA의 경우 예전에는 QA 엔지니어가 하다가 최근에는 데이터 엔지니어가 데이터 검증 레이어를 만들어서 ETL툴에서 Validation를 자동화 하는 경우가 많음. . 세션 3: 온라인 게임 데이터 분석 사례와 향후 과제 - 이은조님 . 1. 온라인 게임 데이터의 특징 . 현실세계와 매우 유사한 환경과 경험 제공 . 성장 활동: 퀘스트, 레벨얼, … | 경제 활동: 사냥/채집, 거래, 경매, … | 사회 관계: 친구, 파티, 길드/혈맹, … | . 거의 모든 종류의 데이터 분석 가능 . 소셜 네트워크 분석 | 텍스트 분석 | 이미지 및 동영상 분석 | . 데이터 활용 사례 . 게임 현황 지표 및 심화 분석 주요 업데이트 전/후 효과 및 동향파악 | 매출, 게임 활동 관련 지표 | . | 기계 학습 및 통계 모델링 재화 이상 탐지 | 작업장 탐지 | 모바일 광고 어뷰징 탐지 | . | . 2. 불쾌한 골짜기 (Uncanny valley) - Robotics . Uncanny valley: 로봇 외형을 점점 인간과 비슷하게 만들다 보면 오히려 이질감이 커지는 지점이 발생함. | 데이터 분석 기법을 고도화 하다 보면 오히려 활용성이 떨어지는 순간이 발생함 (처음에는 현황 지표만 볼 수 있게 되어도 성과가 있지만, 고도화된 기법을 도입하다 보니 성과가 떨어지게되었음) | . 무엇이 불쾌한 골짜기를 만드는가? . 데이터 부정확한 레이블 | Concep drift | . | 모델링 비용을 고려하지 않은 예측 분석 | 잘못된 테스트 셋 선정 | 모델의 복잡함 | . | 서비스 구현 테스팅 및 디버깅의 어려움 | . | . 3. 어떻게 불쾌한 골짜기를 해결할 것인가? . 데이터 . 오류의 원인 . 주관적 편향 | 불일치 | 사소한 실수 | . 레이블 오류는 모델의 신뢰도에 직접적인 영향을 끼침 . 학습할 레이블 양이 많지 않다면? | 오탐이 있으면 안되는 민감한 분야라면? (ex. 리니지 일부 영구정지자들 소송 사례) | . 엄밀한 레이블링 프로세스 구축하기 . 2인 이상의 운영자가 같은 데이터에 대해 독립적인 판단 후 레이블 결과가 같은 데이터만 학습에 활용 | 판정 사유 기입 후 누적된 판정 사유를 정형화 및 목록화 하여 활용 | Leave One Out Cross Validation 사용 -&gt; 99개의 데이터로 모델을 만들고 1개를 판정 -&gt; 데이터가 이상하거나 잘못 레이블링 된 데이터 | . Weak supervision . 낮은 신뢰도를 갖는 레이블로 어떻게 하면 높은 신뢰도의 모델을 만들 수 있을까? . Snorkel: labeling runction과 generative model로 이루어진 기계학습 시스템 . 레이블에 신뢰도를 부여하여 신뢰도가 높은 데이터는 높은 학습 가중치를 부여하고, 신뢰도가 낮은 데이터는 낮은 학습 가중치를 부여함 | . 불명확한 레이블 문제 -&gt; 확률로 표현 . 애초에 레이블 기준이 모호한 경우도 있음 . 이탈 예측: 가입/탈퇴가 불명확한 상황 | 이탈을 확률로 표현 -&gt; Pareto/NBD model | . Concep drift: 시간이 지남에 따라 대상 데이터의 통계적 특성이 변하는 상황 . 왜 Concept drift 문제가 많이 논의되지 않을까? . 학계의 경우 지속성에 대해 고민할 필요가 없음 | 분야에 따라 데이터의 특성이 변하지 않을 경우: 개와 사람 이미지를 구분하는 모델 | . 온라인 게임의 경우 콘텐츠의 소비속도가 어마어마하게 빠름 . 빈번한 게임 업데이트 및 이벤트 게임 밸런스의 변화 | 주요 컨텐츠 삭제 및 추가 | 비즈니스 모델 변경 | . | . 어떻게 대처해야 하나? . Robust modeling 시간에 영향을 받지 않는 피처로만 모델 구축 (정교함이 떨어짐) | . | Change detection 예측 성능을 지속적으로 모니터링하다가 성능이 떨어지는 시점에 재학습 | . | Online learning 학습 / 적용 과정을 분리하지 않고 라이브 환경에서 지속적으로 모델 개선 (추천 분야) | . | Citizen data scientist &gt; 도메인 전문가들이 직접 데이터 분석에 참여 (분석도구, 인프라 제공) | . 2. 모델링 . 비용을 고려하지 않은 예측 분석 . 구매예측 ex. A 상품을 구매할 고객 &gt; 마케팅과 상관없이 구매 (불필요한 마케팅 비용 발생) | 이탈예측: 악성 고객이나 잔존 가치가 낮은 고객을 예측 대상에 포함해야 할까? 잔존 가치가 높은 고객에 대한 이탈을 잘 맞추는 것이 중요 | . 사례: 전체 고객을 예측 대상에 포함 vs 충성 고객만 예측 대상에 포함 . 예측 성능: 1 &gt; 2 | 기대 이익: 1 « 2 | . 아이디어: 애초에 목적에 맞는 비용함수를 사용할 수는 없을까? . 잘못된 테스트 셋 선정 . 모델 성능 측정에 사용해야 하는 테스트 데이터는 가장 최근 시점의 데이터 | . 모델이 복잡할수록 유관 부서에서 사용할 가능성은 떨어짐 . 고객 세그멘테이션할때 k-means clustergin을 많이씀 (설명하기 쉬움) | . 3. 서비스 구현: 테스팅과 디버깅의 어려움 . 문명 6 AI오류 사건: 산출량(Yield) 관련 설정치 이름 오타 | . 심지어 오류가 있어도 결과가 나온다..(심지어 잘..) . word2vec 윈도우 사이즈 사례 | . 세션 5: 맛있는 데이터를 물어다주는 멍멍이 - 노상래님 . 1. 소개: 마켓컬리와 데이터 농장 . 마켓 컬리(식료품 전문 유통업체) . 최적의 서비스 제공을 위한 상품 소싱/제조, 주문처리, 재고관리, 배송, 데이터 분석, 큐레이션 . 데이터 농장 . 하는 업무 . Ad-hoc | IR제작 | 데이터 프로덕트 | 알고리즘 프로덕트 | 대시보드 | 분석용 데이터베이스 구축 | . (달리는 차 위에서 바퀴를 교체하는 사진) -&gt; 빠르게 변화하는 회사에서 데이터 시스템 구축 . 2. 마켓컬리 데이터 시스템의 과거와 현재 . 지난 4년동안 시행착오 . 수기로 운영하던 엑셀의 시대 . 데이터 분석 &amp; 운영 업무에 관련된 대부분의 데이터가 엑셀 자료로 이루어진 시기 | 데이터 분석에 너무 많은 시간이 소요 | 통합이 어려움 | 데이터 수집을 위한 발품팔이 | . AWS 시대 . 회사의 급성장으로 인한 예측 시스템의 필요성 대두 | 인프라를 도입하며 분석용 데이터 인프라 설계 | 슬랙에서 주요 지표와 전사 공유 시스템 도입: 데멍이 | . 문제점: . 데이터 인프라 설계 경험과 지식 부족 | 데이터 추출이 가속화되면서 추출 업무만 하루에 20개씩 진행 -&gt; 대시보드 개발의 필요성 대두 | . 자체 봇 &amp; 실시간 대시보드 시대 . 데이터 플랫폼 인프라 확대 | 주요 지표 대시보드 운영 (고객 현황, 상품 현황, 배송현황 등) | 각 기능별 팀별 실시간 대시보드를 통한 업무 효율화 30분 단위 현황 공유 | D-1 전일 주요 현황 전사공유 | 운영 데이터 수집 관리 | 예상 매출액 | . | . 피드백을 기다리는 야옹이 -&gt; 데이터 운영 시스템에 대한 피드백 . 데이터와 비즈니스 이해 집중 -&gt; 데이터 인프라 관리 집중 -&gt; 데이터의 가치 활용 집중 . 3. 데이터를 물어다주는 멍멍이 ‘데멍이’: 데멍이의 역할과 예측 퍼포먼스 . 데이터 과학으로서의 가치: 예측 시스템 (매출 예측, 물류 예측) . 주 예측(과소 예측 경향), 일 예측(과대 예측 경향) &gt; 결합해서 사용 (페이스북 prophet 활용) | . 퍼포먼스 성과: 월간 예측 성과 오차율 3% (실제값-예측값/실제값) 달성 . 조직 문화로서의 가치: 공유 시스템: 전사 지표 공유 (담당 팀에게 실시간 지표 공유) . 4. 급성장하는 회사에서 데이터는 우리 조직문화에 어떤 기여를 하였는가 . 데이터 업무의 효율화 &gt; 인사이트 도출 &gt; 같은 눈높이의 공유 문화 &gt; 조직 문화 발전 . 초기에 인사이트에 집중하지 않고 운영업무 자동화를 먼저 진행했음 . 세션 6: MOBILITY X DATA : 모빌리티 산업의 도전 과제 - 변성윤님 (쏘카) . 주제: 모빌리티에서 어떤 데이터가 있고, 어떤 문제를 풀고 있을까요? . 모빌리티 업계 (Car sharing / Ride hailing)의 데이터 | 모빌리티 업계에서 풀고 있는 문제가 어떤 것이 있을까? | . 1. Mobility? . 사람들의 이동을 편리하게 만드는 각종 서비스 (전통적인 교통 수단 + IT를 결합해 효율과 편의성을 높임) . CES 2019 -&gt; 주요화두로 Concepted Car, Self Driving Car 등 . Concepted Car: 컨셉을 가지는 차량 컨텐츠를 즐기는 차량, 회의를 위한 차량 등 . | Map: 고정밀 지도 데이터 디테일한 정보를 가진 지도데이터가 필요함. 1차선인지 2차선인지, 정보 등 . | Driver Status Monitoring 운전자가 흡연을 하는지, 졸고 있는지 등 상태를 모니터링해서 알람 및 사고 예방 (주로 컴퓨터 비전 활용) . | Car Maintenance with AI 차량 유지보수에 소요되는 다양한 것들을 자동으로 탐지하고 Report작성 (오일 누수, 부품 미스매치, 차량 스크래치 등 등) | . 모빌리티 회사들 . 서비스 . Car Sharing: 자동차 공유 비즈니스를 하는 회사 Station Base: 지정된 곳에 차량을 반납하는 역 기반의 카셰어링 | Free Floating: 자동차 반납처가 지정되지 않은 유동식 카셰어링 | . | Ride Hailing: 이동을 원하는 소비자와 이동 서비스를 제공하는 사업자를 실시간으로 연결해주는 회사 우버, 타다, 그랩 등등 | . | . 차량 . 주차장 | 자율주행 | 안전 &amp; 보안 | 센서 | . 2. 모빌리티의 데이터 . 데이터의 종류 . 차량 데이터 | 좌표, 지리데이터 (GPS, 지리 데이터 등) | 센서 데이터 (엔진 상태, 배터리 전압상태, 주유 데이터..) | 고객 데이터 (면허 취득 날짜, 사용 이력 패턴, 앱 로그, 결제 데이터) | 날씨 데이터 (기상청 날씨 데이터) | . 왜 재미있을까? . 삶과 밀접한 데이터 | 생활 패턴 반영 | 큰 의미에서 도시계획 &amp; 사회발전에 밀접 | 어려워서 매우 재미있음 | 다양한 데이터의 혼합 | . 모빌리티의 데이터를 보려면 . NYC Open DATA (Taxi) | awesome-public-datasets - transportation 데이터 | . 3. 모빌리티의 다양한 문제들 . Car Sharing . ex. 쏘카 경험 여정 . 쏘카존에서 차량 대기 | 차량 예약 | 쏘카존 방문 | 차량 탑승 | 차량 이동 | 차량 반납 | . 데이터 기반 존 및 차량 운영 전략 수립 . 특정 존 개발 (어디에) / 차량의 가격 설정 | 어떤 존에 어떤 차량을 넣어야 할까 | 수요 예측 및 운영 전략 수립 | 차량 구매 전략 (성수기) | . 주로 활용하는 방법: Operation Research (수학적 모델링, 통계적 모형, 최적화 기법 등을 활용해 효율적인 의사결정을 돕는 기법) . 차량 예약 . 개인화된 가격 (쿠폰 및 혜택) | . 차량 퀄리티 관리 . 소모품 교환 및 세차 주기 최적화 | 차량 배터리 수명 관리 | . 차량 이용 과정에서 사고 관련 . 차량별/ 개인별 / 상황별 보험료 산정 | . 운영 정책 효과 분석 . 신규 상품 기획 (쏘카 구독제 등) | 운영 | . Ride Hailing . 타다 경험 여정 . 차량 호출 | 차량 배차 | 차량 도착 | 고객 탑승 | 목적지로 출발 | 도착 | . 차량이 언제 도착할까? (ETA) . ETA: 도착 예정 시간 (늦는 경우 고객경험에 악 영향) | 머신러닝을 통해 정확한 ETA값 예측 | . 차량 수요 예측 시 탄력 요금제 적용 . 갑자기 비가 내리는 경우 | 새벽 2시에 월드컵 결승 | 불금, 연휴 전날 수요증가 | . 우버의 Surge Pricing: 급증하는 시간대, 지역에 탄력 요금제 설정, 차량 구매 전략에 활용 . 알고리즘을 오프라인에 바로 적용하는데 큰 리스크 존재: 리스크를 줄이고 실험을 다양하게 하기 위해 시뮬레이션 환경을 구현 . 머신러닝 모델을 테스트하기 위해 과거 데이터를 기반으로 확률 분포를 통해 시뮬레이션 환경 생성 . | 실제 환경과 비슷하게 구축하는 것이 매우 중요 . | . (SimPY: 간단히 체험할 수 있는 라이브러리) . 지도, 네비게이션 문제 . Route Planning 출발지에서 목적지까지 어떤 경로로 갈 것인가 (최소 시간, 최단 시간) | 교통량 예측 | Map Matching: GPS 데이터와 도로 데이터를 매칭 | . | . 산업의 성숙도에 따라서 풀어야 하는 문제 단계가 다름 . 세션 7: 데이터가 흐르는 조직 만들기 - 양승화님 (마이리얼트립) . 마이리얼트립 매출이 급성장 중이었는데 데이터에 대한 고민을 하고 있었음. . 데이터를 기반으로 일하는 회사를 만들자 | 데이터를 바탕으로 000 문제를 해결하자 | . Growth팀에 기대하는 역할 . 핵심지표 선정 및 관리 | 데이터 파이프라인 설계 및 구축 | 주제별 데이터 분석 (차근차근 하면 되는 것) | 데이터 추출 및 분석 요청 대응 (당장 시간을 제일 많이 쓰는 것) | 데이터 기반으로 일하는 문화 (어떻게 해야 할 지 막막한 것) | . 혼자 였음.. 분석하려면 야근을 해야되… . 1. 데이터 분석 팀과 실무자와의 갈등 . 실무자 입장 . 어떤 데이터가 있는지 모르겠음 | 간단한 요청인데 오래걸려.. | 요청하고 받았더니 단순 합계, 평균인데,.. | 업무에 쓸만한 건 없네 | . 분석가 입장 . 여기저기서 쏟아지는 데이터 추출 요청에 정신이 없다 | 목적이 000인 것 같은데, 이 데이터를 달라고? | 대시보드 만들면 고쳐달라고 하고 잘못했다고 하고 | 데이터 분석 좀 해보고 싶다 ㅜㅠ | . 2. 뭐가 문제인가? . 문제가 아님 . 대시보드가 잘 되어 있는데도 계속 요청한다 &gt; 보다보면 궁금한게 생김 | 조금씩 조건을 바꿔서 자꾸 요청한다 &gt; 쓸만한 인사이트나 아이디어는 데이터를 다양한 각도에서 살펴봐야 답이 나옴 | . 이건 문제 . 데이터 분석가들이 추출만 하고 있다. &gt; 다른일 할 시간이 없다. | 데이터 추출 요청이 명확하지 않아 추출에 시간이 오래걸린다. | 데이터 추출 요청하는게 번거롭고 데이터팀의 눈치를 본다. | 데이터 분석을 데이터팀에서한 한다. | 데이터팀에서 분석한 결과가 서비스에 반영되지 않는다. | . 3. 지향하는 조직 . 프로세스와 역량을 갖춘 회사 . 복잡한 절차 없이, 필요한 데이터를 누구든 찾아볼 수 있고 가공해서 인사이트를 찾을 수 있다. | 데이터 분석가들이 본업에 집중할 수 있다. | 분석 결과물들이 체계적으로 쌓이고 실제 서비스에 반영된다. | . 요청자와 분석가의 역할이 명확하게 구분되지 않는 조직 . 4. 데이터가 흐르는 조직을 만들기 위한 노력 . 1) 사내교육: 데이터 추출과 분석을 위한 기본 지식 쌓기 . 데이터 분석을 위한 마인드셋 . 왜 데이터 분석이 필요하고 내 업무에 어떻게 적용할 것인가 데이터 분석의 목표: 서비스를 운영하면서 쌓이는 유저 데이터를 바탕으로 서비스를 지속적으로 개선해 나가는 것 . | . SQL . 동영상 강의를 지정해서 수강하게 함 (자기주도 학습) | 서비스 DB에 대해 설명 | 써먹을 수 있는 과제를 출제 | . Excel . 실제 업무에서 필요한 문제들을 풀기 위한 스킬들 | . 사내교육이 의미 있으려면 . 주기적으로 해야함 | 리더의 의지와 지원이 필요함 | 배운 걸 즉시 써먹을 수 있는 환경이 지원되어야 함 | 배운 걸 실제 업무에 써먹고 있는지 체크해야 함 (업무에서 잘 활용하고 있어야 함) | . 2) 시스템 . 데이터 파이프라인 만들기 . 구성원들이 자유롭게 쿼리할 수 있는 환경을 만드는 게 시작 | . 간단한 BI툴에서부터 시작 . (추천: redash) | (추천: Stitch &gt; 데이터 엔지니어 없이 ETL하기) | (추천: 빅쿼리) | . 3) 조직문화 . 업무환경 . 리더의 의지 (매우 중요함) | 데이터에 대한 폭넓은 접근성 | . 조직구조 . 낮은 부서간 업무 장벽 | 고립되지 않은 분석 조직 (R&amp;R이 모호한 구조) | . 일하는 방식 . 지표를 명확하게 정의하고 사용해야 함 (사람마다 정의가 다름) | 반복되는 실패, 지속적인 실험 | . 좋은 질문을 찾는 노력 . (조직구성) . 그로스팀 &gt; 데이터 기반 회사를 만드는 조직 | 크로스셀TF &gt; 데이터에 기반해서 핵심지표를 개선하는 팀 | . (꼭 데이터가 있어야 시작할 수 있는 건 아님 &gt; 데이터가 없어도 연역적으로 예측했던 사례도 있었음) .",
            "url": "https://inahjeon.github.io/devlog/conference/2019/06/27/dev-ground.html",
            "relUrl": "/conference/2019/06/27/dev-ground.html",
            "date": " • Jun 27, 2019"
        }
        
    
  
    
        ,"post19": {
            "title": "Time Series Analysis & Forecasting (1)",
            "content": "Time Series? . Time series (시계열) 데이터는 연도별(annual), 분기별(quarterly), 월별(monthly), 일별(daily) 또는 시간별(hourly) 등 시간의 경과(흐름)에 따라 순서대로(ordered in time) 관측되는 데이터입니다. . Time series 데이터의 예시: . 국민 총 생산액, 물가지수, 주가지수 등과 같이 경제활동과 관련된 시계열 (economic time series) | 일일 강수량, 기온, 연간 지진의 발생 수 등과 같이 물리적 현상과 관련된 시계열 (physical time series) | 상품판매량, 상품재고량, 상품매출액 등 회사의 경영활동과 관련된 시계열 (marketing time series) | 총인구, 농가 수, 인구증가율, 평균결혼연령 등과 같이 인구와 관련된 시계열 (demographic time series) | 품질관리 등과 같은 생산관리와 관련된 시계열(time series in process control) | 확률과정, 음성파와 같이 통신 공학 또는 공학과 관련된 시계열 | 월별 교통사고 건수, 월별 범죄 발생 수와 같이 사회생활과 관련된 시계열 | . 연속적으로 생성되는 연속시계열(continuous time series)과 이산적 시점에서 생성되는 이산시계열(discrete time series)로 구분할 수 있습니다. 많은 시계열들이 연속적으로 생성되고 있지만 일정한 시차를 두고 관측되므로 이산시계열의 형태를 지니는 경우가 많습니다. . 시계열자료(time series data)들은 시간의 경과에 따라 관측된 자료이므로 시간에 영향을 받습니다. 시계열자료를 분석할 때 관측 시점들 간의 시차(time lag)가 중요한 역할을 합니다. 예를 들어, 오늘의 주가가 한달 전, 일주일 전의 주가보다는 어제의 주가에 더 많은 영향을 받는 것과 마찬가지로 가까운 관측 시점일수록 관측 자료들 간에 상관관계가 커집니다. . Time Series Analysis의 목적 . Modeling . 시계열자료가 생성된 시스템 또는 확률과정을 모형화하여 과정을 이해하고 제어(control)합니다. | 예) 여름기간 동안의 시간 당 전기 수요(ED, electricity demand)에 대해 모형화하고 변동 요소(현재 기온, 인구, 시간, 요일 등)들을 제어하여 전기 수요량에 대해 시뮬레이션 | . Forecasting . 과거 시계열자료의 패턴(pattern)이 미래에도 지속적으로 유지된다는 가정하에, 시계열 데이터 모델링을 통해 미래 시점을 예측(Forecasting)합니다. | 예) 어떤 상품의 매출액 자료를 분석하여 미래의 매출액을 예측 | . Components of Time Series . . Trend (추세) | Cycle (주기성) | Seasonality (계절성) | Random Variation (불규칙 변동) | . Trend . 장기간에 걸쳐 지속적으로 증가 또는 감소하거나 일정한 상태를 유지하려는 패턴입니다. | Trend를 이용한 데이터 추정: 시간 t에 대한 추세 함수 f(t)와 정상 확률 과정 x(t)로 표현될 수 있습니다. | . . Cycle . 시간의 경과(흐름)에 따라 고정된 빈도가 아닌 형태로 상하로 반복되는 변동하는 패턴입니다. | 추세변동은 장기적으로(일반적으로 1년 초과) 나타나는 추세경향이지만, 순환변동은 대체로 2년 이상의 기간을 주기로 순환적으로 나타납니다. (ex. 경기순환) | . . Seasonality . 해마다 어떤 특정한 때나 1주일마다 특정 요일 등의 계절성 요인이 시계열에 영향을 줄 때 나타나는 패턴입니다. | 계절성은 항상 알려진 상수 빈도 형태입니다. | . . Random Variation . 시간에 따른 규칙적인 움직임과는 무관하게 랜덤한 원인에 의한 변동성분입니다. | . . Stationary vs Non-Stationary . Stationary . 시계열 데이터의 통계적 특성(mean, variable,..)이 시간의 흐름에 따라 변하지 않을 때 stationary 라고 말합니다. | . Non-Stationary . 시계열 데이터의 통계적 특성(mean, variable,..)이 시간의 흐름에 따라 변할 때를 말합니다. | 많은 시계열 모델링 기법들에서 데이터가 stationary라고 가정하기 때문에, 모델링에 앞서 Non-Stationary 한 데이터를 Stationary 한 데이터로 변형합니다. | . Non-Stationary to Stationary . Differencing (차분) . 주로 trend, seasonality를 가지는 시계열 데이터의 경우 활용합니다. | Differencing: y′t=yt−yt−1. | Seasonality differencing: y′t=yt−yt−m (m: seasonality 의 주기) | 언제 적용해야하는가? Dicker-Fuller Test로 검증 | . . Transformation . 주로 분산이 커지는 경우 Log 변환을 취하여 안정화 시킬 수 있습니다. | 분산이 줄어드는 경우는 특정 점으로 수렴함을 의미하므로 모델링이 불필요합니다. | . . 전체 변형 적용 예시 . .",
            "url": "https://inahjeon.github.io/devlog/time%20series/2019/05/15/time-series1.html",
            "relUrl": "/time%20series/2019/05/15/time-series1.html",
            "date": " • May 15, 2019"
        }
        
    
  
    
        ,"post20": {
            "title": "Apache Superset 세팅",
            "content": "Apache Superset . Apache Superset은 business intelligence를 위한 대시보드 툴입니다. 지원하는 data source도 많고 대시보드 구성에 필요한 일반적인 기능들도 다 제공하고 있어서, Superset을 이용해서 쉽게 대시보드를 구성해 볼 수 있습니다. . 설치 및 실행화면 . Reference: Superset 설치 참고 . Docker 이미지를 사용하지 않고 pip으로 직접 설치하는 경우, 공식 문서에 나와있는 아래 스크립트를 따라하면 쉽게 설치할 수 있습니다. . # Install superset pip install superset # Create an admin user (you will be prompted to set a username, first and last name before setting a password) fabmanager create-admin --app superset # Initialize the database superset db upgrade # Load some data to play with superset load_examples # Create default roles and permissions superset init # To start a development web server on port 8088, use -p to bind to another port superset runserver -d . 이때, 사용하는 라이브러리(pandas, sqlalchemy)들의 버전 문제로 중간에 보통 아래와 같은 에러가 발생하기 때문에 superset 설치 후 다음과 같이 라이브러리 버전을 낮춰서 설치해주어야 중간에 에러가 발생하지 않습니다. . 1. Admin 설정 시 에러 . fabmanager create-admin --app superset . 에서 admin 계정 생성 후 다음과 같이 에러가 발생합니다. . Was unable to import superset Error: cannot import name &#39;_maybe_box_datetimelike&#39; from &#39;pandas.core.common&#39; . pandas version 문제로 발생하는 에러로 pandas 버전을 낮춰서 설치하면 잘 됩니다. . pip install pandas==0.23.4 . 2. DB initialization 시 발생하는 에러 . superset db upgrade . 마찬가지로 sqlalchemy 버전 문제로 아래와 같은 에러가 발생합니다. . &quot;Can&#39;t determine which FROM clause to join &quot; sqlalchemy.exc.InvalidRequestError: Can&#39;t determine which FROM clause to join from, there are multiple FROMS which can join to this entity. Try adding an explicit ON clause to help resolve the ambiguity. . sqlalchemy 버전도 맞춰줍니다. . pip install sqlalchemy==1.2.18 . 실행 화면 . 설치 완료 후, 로컬 서버에서 테스트 대시보드를 불러온 화면 . . 위 화면은 superset을 설치할 때 superset load_examples 이란 명령어로 예시 대시보드를 불러왔을 때 볼 수 있는 화면입니다. . Superset 공식 document에서 chart type에 따른 시각화 방법 (time column, metric 설정 등) 대한 내용이 부실하기 때문에 처음 사용 하시는 분이라면 예시 대시보드를 보면서 데이터를 시각화하는 방법을 익히는 것을 추천드립니다. . DB 연결 . DB에 연결할 때는 사용할 DB 종류에 따라 superset이 설치된 환경에 설치해주어야하는 라이브러리들이 있습니다. . . 참고 . 해당하는 DB의 라이브러리를 표의 명령어를 사용하여 설치해주면 됩니다. .",
            "url": "https://inahjeon.github.io/devlog/ml%20/%20data%20engineering/2019/04/25/superset.html",
            "relUrl": "/ml%20/%20data%20engineering/2019/04/25/superset.html",
            "date": " • Apr 25, 2019"
        }
        
    
  
    
        ,"post21": {
            "title": "글또를 끝내며",
            "content": "작년 11월부터 시작한 글또모임이 이번 달로 끝난다. 6개월간 2주마다 1편씩 해서 총 12편을 글을 작성하게 된다. 돌아보니 이번 회고 글을 포함해서 총 9편의 글을 작성했다. . 처음 모임을 시작하면서 각자 글또를 시작하며 다짐글을 썼었는데, 시작 글에서 이루고 싶은 목표 세 가지를 적어 놓은 게 있었다. . 미루고만 있었던 공부, 개인 프로젝트를 이번 기회에 하나씩 진행해서 이에 대한 주제로 글쓰기. -&gt; 절반정도(?) 성공 | 내용 정리를 지금보다 더 깔끔하게 잘하고, Markdown 손에 익히기. -&gt; 성공 | Pass권 쓰지않고 예치금 꼭 다 받아가기. -&gt; 실패 | . 자체평가를 해보니 절반 정도는 달성한 것 같다. 처음 의욕 만땅으로 시작했을 때 보다는 적지만, 그래도 꾸준히 글을 쓰고 끝마쳤다는 것에 의의를 두자. 그래도 블로그를 시작하고 나서 혼자 끄적끄적 작성하던 것에 비해서는 확실히 발전했던 것 같다. . 아쉬운 점은 글또를 시작할 때 진행해보려고 했던 주제들이 많았는데, 생각보다 잘 진행되지 않았던 점이다. 따로 시간을 내서 공부하고 글로 정리하지 않았고, 글또 마감이 있는 주의 주말에 시간을 짬짬이 내서 단 시간에 글을 쓰다보니 그때 그때 생각나는 주제들로 글을 썻던 것 같다. . 글또 모임이 끝나더라도 계속 공부하고 글을 꾸준히 쓸 생각이라 다시 글쓰기 계획을 잡아봐야겠다. 계획대로 어떻게 강제로 쓰게 할 수 있을 지는 고민이다. .",
            "url": "https://inahjeon.github.io/devlog/diary/2019/04/21/end-geultto.html",
            "relUrl": "/diary/2019/04/21/end-geultto.html",
            "date": " • Apr 21, 2019"
        }
        
    
  
    
        ,"post22": {
            "title": "Meet Michelangelo: Uber’s Machine Learning Platform (번역) (2)",
            "content": "참고자료 . Meet Michelangelo: Uber’s Machine Learning Platform - Uber Machine Learning Platform team의 Jeremy Hermann (Engineering Manager), Mike Del Balso (Product Manager) . Evaluate models . 모델들은 주로 문제를 해결에 가장 좋은 모델을 생성하는 feature 집합, 알고리즘, hyper-parameter들을 식별하기 위한 체계적인 탐색 프로세스의 일부로 학습됩니다. 주어진 usecase에 이상적인 모델에 도달하기 전에 목표를 달성하지 못하는 수 백개의 모델을 학습하는 일은 드문 일이 아닙니다. 비록 최종적으로 제품에 사용되지 않지만, 이러한 모델들의 성능은 엔지니어들이 가장 좋은 모델에 대해 configuration을 할 수 있도록 가이드 해줍니다. 이러한 학습된 모델들을 지속적으로 트래킹(예: 누가 그 모델들은 언제 어떤 데이터셋과 hyper-parameter로 학습했는지 등) 하고, 평가하고, 다른 모델들과 비교하는 것은 수많은 모델들을 다루는 환경에서 일반적으로 큰 어려움이 있고, 현재 플랫폼에는 큰 가치를 더할 수 있는 기회입니다. . Michelangelo에서 학습된 모든 모델들에 대한 버전 객체를 우리의 Cassandra의 모델 저장소에 저장합니다. 버전 객체는 다음의 내용을 포함하고 있습니다. . 누가 모델을 훈련 시켰는가? | 학습 작업의 시작 및 종료 시간 | 전체 모델 구성 (사용된 feature들, hyper-parameter 값 등) | 학습 및 테스트 데이터셋에 대한 참조 | 각 feature들의 분포와 중요도 | 모델 정확도 지표 | 각 모델 타입에 대한 표준 차트와 그래프 (예: 이진 분류기에 대한 ROC 곡선, PR 곡선, confusion matrix) | 모델에서 학습된 전체 paramter들 | 모델 시각화를 위한 요약 통계 | . 이러한 정보는 웹 UI를 통해 사용자들이 쉽게 사용할 수 있고, API를 통해 개별 모델들의 세부 사항에 대한 검사와 하나 이상의 서로 다른 모델들을 비교할 수 있습니다. . Model accuracy report . 회귀 모델에 대한 모델 정확성 보고서는 표준 정확도 지표와 차트들을 보여줍니다. 분류 모델들은 Figure 4나 5와 같이 다른 집합을 표시합니다: . Figure 4: Regression 모델 리포트는 regression과 연관된 성능 지표를 보여줍니다. . Figure 5: 이진 분류의 성능 리포트는 분류와 연관된 성능 지표를 보여줍니다. . Decision tree visualization . 중요한 모델 타입의 경우, 우리는 모델러들이 왜 모델이 정상적으로 작동하는지 이해하고 필요한 경우 디버깅할 수 있도록 돕기 위해 정교한 시각화 도구들을 제공합니다. Decision tree 모델들의 경우, 우리는 사용자들이 각각의 개별 tree들을 탐색하여 전체적인 모델에 대한 상대적인 중요도, 분할 지점, 특정 tree에 대한 개별 feature들의 중요도, 각 분할 지점에서의 데이터 분포를 확인할 수 있게 합니다. 아래 Figure 6에서 보듯이, 사용자가 feature 값을 명시할 수 있고, 그에 따라 decision tree에서 활성화되는 경로, tree 별 예측, 전체적인 모델의 예측에 대해 시각화합니다: . Figure 6: Tree 모델들은 강력한 tree 시각화로 탐색할 수 있습니다. . Feature report . Michelangelo는 개별 feature들에 대해 모델에서의 중요도 순으로 부분적 의존 관계 그래프와 분포 histogram을 보여주는 feature 보고서를 제공합니다. 아래와 같이, 두 가지 feature를 선택하면 사용자는 양방향 부분적 의존 관계 다이어그램을 통해 feature들의 상호작용에 대해 이해할 수 있습니다: . Figure 7: Feature들, 모델에서의 영향력, feature 간 상호작용을 탐색해 볼 수 있습니다. . Deploy models . Michelangelo는 UI 또는 API를 통해 모델 배포 관리를 위한 end-to-end 지원을 제공하고, 모델을 배포할 수 있는 세 가지 모드를 지원합니다: . Offline deployment. 이 모델은 offline 컨테이너에 배포되고, on demand 또는 반복되는 스케줄링에 따른 배치 예측 작업을 위해 Spark job에서 실행됩니다. . Online deployment. 이 모델은 online 예측 서비스 클러스터(일반적으로 load balancer 뒤에 수 백개의 머신을 포함하고 있는)에 배포됩니다. Client는 RPC 호출과 같은 개별 또는 배치 예측 요청을 보낼 수 있습니다. . Library deployment. 우리는 다른 서비스에서 라이브러리 형태로 내재화되어 Java API로 호출될 수 있는 모델을 serving 컨테이너에 배포하려고 합니다. (아래 Figure 8에는 표시되어 있지 않지만 online deployment와 유사하게 동작합니다) . Figure 8: 모델 서빙을 위해 모델 저장소로부터 모델이 online과 offline 컨테이너에 배포됩니다. . 모든 케이스에서, 배포에 필요한 모델 결과물(metadata 파일, 모델 parameter 파일, 컴파일된 DSL 표현식들)은 ZIP 파일로 압축되고, 우리의 표준 코드 배포 인프라를 사용하여 Uber의 데이터 센터를 통해 연관있는 host에게로 복사됩니다. 예측 컨테이너들은 자동적으로 디스크로부터 새로운 모델을 불러오고 예측 요청을 처리하기 시작합니다. . 많은 팀들은 Michelangelo의 API를 활용하여 주기적인 모델 재학습과 배포를 위한 자동 스크립트를 보유하고 있습니다. UberEATS의 배달 시간 모델의 사례에서는, 데이터사이언티스트들과 엔지니어들이 웹 UI를 통해 학습과 배포를 수동으로 트리거 합니다. . Make predictions . 한번 모델이 배포되고 서빙 컨테이너에 로드되면, 모델들은 데이터 파이프라인이나 client 서비스로 부터 직접 불러온 feature 데이터를 기반으로 예측 작업을하는 데 사용됩니다. 원본 feature들은 원본 feature를 수정하거나, Feature Store에서 부가적인 feature를 추가할 수 있는 컴파일된 DSL 표현식에 전달됩니다. 그리고 최종 feature vector가 계산되어 스코어링을 위해 모델에 전달됩니다. Online 모델의 경우, 예측 결과는 네트워크를 통해 Client 서비스에 반환됩니다. Offline 모델의 경우, 예측 결과는 Hive에 다시 쓰여지고, 배치 작업에 사용되거나 사용자가 SQL 기반 쿼리 툴을 사용해서 접근할 수 있습니다. . Figure 9: Online과 Offline 예측 서비스에서 feature vector 셋을 예측하는데 사용합니다. . Referencing models . 하나 이상의 모델을 주어진 서빙 컨테이너에 동시에 배포할 수 있습니다. 이것은 이전 모델을 새로운 모델로 안전하게 변경하는 작업과 모델 간 A/B테스팅을 가능하게 합니다. 서빙 시점에서, UUID와 배포중에 명시된 부가적인 태그(또는 별칭)를 통해 모델이 식별됩니다. Online 모델의 경우, 클라이언트 서비스가 사용하고자 하는 모델의 UUID나 모델 태그와 함께 feature vector를 전송합니다. 태그를 사용할 경우, 컨테이너는 해당 태그에 가장 최근에 배포된 모델을 사용하여 예측 결과를 생성합니다. 배치 모델의 경우 모든 배포된 모델들은 개별 배치 데이터셋에 대해 점수를 매기는데 사용되고, 예측 결과들이 모델 UUID와 부가적인 태그를 포함하고 있어 사용자가 적절하게 결과를 필터링할 수 있습니다. . 만약 이전 모델을 대체하기 위해 새로운 모델을 배포할 때 두 모델이 같은 signature를 가졌다면(예: 동일한 feature 셋을 가졌을 경우), 사용자들은 새로운 모델을 이전 모델과 같은 태그로 배포할 수 있고, 컨테이너들은 새로운 모델을 즉시 사용하기 시작합니다. 이것은 사용자들이 그들의 client 코드를 수정하지 않고도 모델을 갱신할 수 있게 해줍니다. 사용자들은 또한 이전 모델에서 새로운 모델로 점진적으로 트래픽을 전환하기 위해 client나 중간 서비스에서 UUID만으로 새로운 모델을 배포하고 configuration을 변경할 수 있습니다. . 모델 간의 A/B 테스팅을 위해, 사용자들은 간단하게 UUID나 태그를 통해 비교할 모델들을 배포할 수 있고, 클라이언트 서비스에서 Uber의 실험 프레임워크를 사용하여 트래픽의 일부를 각 모델에 보내고 성능 지표를 트래킹할 수 있습니다. . Scale and latency . 머신러닝 모델들이 stateless이고 무엇도 공유하지 않기 때문에, online이나 offline 서빙 모드 모두에서 스케일 아웃하기 쉽습니다. Online 모델의 경우, 우리는 간단하게 더 많은 호스트를 예측 서비스 클러스터에 추가하고, load balancer로 네트워크 부하를 퍼뜨릴 수 있습니다. Offline 예측의 경우, 우리는 더 많은 Spark 실행작업을 추가하고 Spark로 병렬처리를 관리할 수 있습니다. . Online 서빙 응답시간은 모델이 Cassandra feature store에 저장된 feature를 가져오는 것과는 관계없이 모델 타입과 복잡도에 달려있습니다. Cassandra에 저장된 feature를 필요로 하는 모델의 경우, 주로 5 밀리초 이내의 P95의 응답시간 정도 소요됩니다. Cassandra의 feature를 필요로 하는 모델의 경우, 주로 10 밀리초 이내의 P95의 응답시간을 가집니다. 가장 높은 트래픽의 모델은 현재 초당 250,000개의 예측을 수행하는 모델들입니다. . Monitor predictions . 모델이 학습되고 평가될 때, 기록 데이터가 항상 사용됩니다. 특정 모델이 미래에도 잘 동작할 지 확인하기 위해, 데이터 파이프라인이 계속 정확한 데이터를 전송하고 모델이 더 이상 정확하지 않게 production 환경이 변하지 않았는지 모델의 예측 결과를 모니터링 하는 것은 중요합니다. . 이 문제를 해결하기 위해 Michelangelo는 자동으로 로그를 기록하고, 선택적으로 예측 결과 비율을 저장하여 데이터 파이프라인에서 생성된 실제 관측 결과(또는 label)와 결합합니다. 이러한 정보를 통해 우리는 실시간으로 모델의 정확도에 대한 지표를 생성할 수 있습니다. 회귀 모델의 경우, 아래 그림과 같이 우리는 예측 결과에 대한 R-squared/coefficient, root mean square logarithmic error (RMSLE), root mean square error (RMSE), mean absolute error 지표를 Uber의 시계열 모니터링 시스템에 발송하여 사용자들이 시간에 따라 차트를 분석하고 특정 threshold 값으로 알람을 설정할 수 있도록 합니다. . Figure 10: 예측 결과가 샘플링되고 관측 결과와 비교하여 모델의 정확도 지표를 생성합니다. . Building on the Michelangelo platform . 앞으로 몇 달 동안, 우리는 고객 팀과 Uber 비즈니스 전반의 성장을 지원하기 위해 기존 시스템을 아래와 같은 기능들에 대해 계속 확장하고 강화할 계획입니다. 플랫폼 계층이 성숙 해짐에 따라 우리는 머신러닝의 민주화를 추진하고 비즈니스 요구를 보다 잘 지원하기 위해 더 높은 수준의 도구와 서비스에 투자 할 계획입니다. . AutoML. 이것은 주어진 모델링 문제에서 모델이 최고의 성능을 내도록 하는 configuration(알고리즘, feature 셋, hyper-parameter 값들 등)들을 자동적으로 검색하고 발견하는 시스템이 될 것 입니다. 이 시스템은 또한 모델에 필요한 feature들과 label들을 생성하기 위해 자동적으로 production 데이터 파이프라인들을 생성할 것 입니다. 우리는 우리의 Feature Store, 균일화된 오프라인과 온라인 데이터 파이프라인들, hyper-parameter 검색으로 이 문제에 대한 큰 부분들을 이미 해결했습니다. 우리는 초반의 데이터 사이언스 업무를 AutoML을 통해 가속화하려고 합니다. 이 시스템은 데이터 과학자들이 label들과 목적함수를 정의하고, Uber의 데이터를 보안적으로 안전하게 사용하여 문제에 대한 최적을 답을 찾을 수 있도록 합니다. 목표는 똑똑한 도구로 데이터 과학자들을 일을 손쉽게 만들어서 생산성을 높이는 것입니다. . Model visualization. 모델을 이해하고 디버깅하는 것은 특히 딥러닝에서 점점 중요해지고 있습니다. 우리가 트리 기반의 모델들을 시각화하는 도구로 몇가지 중요한 첫 스텝들을 밟고 있는 반면, 데이터 과학자들이 예측 결과를 믿고, 모델을 이해하고, 디버그하고 튜닝하기 위해 완료되어야 할 더 많은 니즈들이 남았습니다. . Online learning. 대부분의 Uber의 머신러닝 모델들은 직접적으로 Uber의 production 환경에 실시간으로 영향을 끼칩니다. 이것은 그들이 물리적 세계의 물체가 움직이는, 복잡하고 계속 변화하는 환경에서 동작을 실행한다는 의미입니다. 우리의 모델이 환경이 변화하더라도 정확성을 유지하기 위해, 우리는 변화에 맞게 모델을 변경해야합니다. 현재, 팀들은 규칙적으로 Michelangelo에서 그들의 모델을 재학습시킵니다. 이 use case에 대한 완벽한 플랫폼 솔루션은 손쉬운 모델 타입 갱신, 빠른 학습과 평가 구조 및 파이프라인, 자동화된 모델 검증 및 배포 그리고 섬세한 모니터링과 알림 시스템을 포함합니다. 큰 프로젝트일지라도, 초기 결과물은 online learning을 당장 시작하는 것이 뒤따라오는 잠재적인 이익을 얻을 수 있음을 나타냅니다. . Distributed deep learning. 증가하고 있는 Uber의 머신러닝 시스템들은 딥러닝 기술로 구현되어있습니다. 딥러닝 모델을 정의하고 반복시키는 사용자의 워크플로는 표준화된 워크플로와 충분히 달라 플랫폼의 고유한 지원이 필요합니다. 딥러닝 use case들은 일반적으로 많은 양의 데이터를 다루고, GPU같은 다른 하드웨어 사양이 필요하기때문에, 분산 학습에 대한 더 많은 투자와 유연한 자원관리 스택과 긴밀한 통합을 유도하고 있습니다. .",
            "url": "https://inahjeon.github.io/devlog/ml%20/%20data%20engineering/2019/04/07/meet-michelangelo_2.html",
            "relUrl": "/ml%20/%20data%20engineering/2019/04/07/meet-michelangelo_2.html",
            "date": " • Apr 7, 2019"
        }
        
    
  
    
        ,"post23": {
            "title": "Meet Michelangelo: Uber’s Machine Learning Platform (번역) (1)",
            "content": "참고자료 . Meet Michelangelo: Uber’s Machine Learning Platform - Uber Machine Learning Platform team의 Jeremy Hermann (Engineering Manager), Mike Del Balso (Product Manager) . . Uber Engineering은 고객에게 완벽하고 효과적인 경험을 제공하는 기술 개발에 전념하고 있습니다. 우리는 이러한 비전을 실현하기 위해 인공 지능 (AI) 및 머신러닝에 대한 투자를 늘리고 있습니다. Uber에서 우리의 기여는 머신러닝을 민주화하고 Uber에서 승차를 요청하는 것만큼 쉽게 비즈니스의 니즈를 충족 할 수 있도록 AI를 확장하는 내부 ML-as-a-service platform인 Michelangelo를 지원하는 것입니다. . Michelangelo는 내부 팀이 Uber의 규모로 머신러닝 솔루션을 완벽하게 제작, 배포 및 운영 할 수 있게합니다. 이는 end-to-end ML workflow (데이터 관리, 모델 학습, 평가 및 배포, 예측 및 예측 모니터링)를 포괄하도록 설계되었습니다. 이 시스템은 전통적인 ML 모델, 시계열 예측 및 딥러닝을 지원합니다. . Michelangelo는 약 1년 동안 Uber의 production usecase에 사용되고 있으며 수십 개의 팀이 모델을 구축하고 배포하면서 엔지니어 및 데이터 과학자들이 머신러닝을 쓸 수 있게 하기위한 실질적인 시스템이 되었습니다. 여러 Uber 데이터 센터에 배치되어, 특수한 하드웨어를 활용하고 회사에서 로드가 가장 많은 온라인 서비스에 대한 예측을 제공합니다. . 이 글에서는 Michelangelo를 소개하고, production에서의 usecase 대해 논의하며, 이 강력한 새 ML-as-a-service 시스템의 워크 플로우를 살펴 봅니다. . Motivation behind Michelangelo . Michelangelo 이전에는, Uber의 운영 규모와 스케일에 관련하여 머신러닝 모델을 만들고 배포하는데 수많은 챌린지를 마주했습니다. 데이터 사이언티스트들이 예측 모델을 만들기 위해 다양한 범위의 툴 (R, scikit-learn, custom algorithms 등) 을 사용하는 반면, 엔지니어링 팀들은 production환경에서 이러한 모델들을 사용하기 위해 일회용 시스템을 구축했습니다. 그 결과로, Uber에서 머신러닝의 영향력은 대부분 오픈소스 툴을 통해 소수의 데이터사이언티스트들과 엔지니어들이 단기간내에 만들 수 있는 것으로 제한 되었습니다. . 특히, 확장성 있게 예측 데이터를 만들고 학습을 관리할 수 있는 신뢰성있고, 균일하고, 재생산 가능한 파이프라인이 없었습니다. Michelangelo 이전에는 데이터사이언티스트들의 데스크탑 머신에서 돌아갈 수 있는 크기보다 더 큰 데이터에 대해서는 모델을 학습하는 것이 불가능했고, 학습 실험 결과를 저장하고 다른 실험들과 쉽게 비교할 수 있도록 하는 표준화된 저장소가 없었습니다. 가장 중요한 것은, 대부분의 경우에서 production환경에 모델을 배포하는 확실한 경로가 없었고, 관련있는 엔지니어링 팀에서 특정 프로젝트에 대해 커스텀한 서빙 컨테이너를 만들어서 관리했습니다. 동시에, 우리는 Scully에 의해 문서화된 많은 ML anti-pattern들의 징후를 볼 수 있었습니다. . Michelangelo는 회사내 누구든 쉽게 머신러닝 시스템을 확장성 있게 만들고 동작시킬 수 있는 end-to-end 시스템을 통해, 팀끼리 사용하는 워크플로와 도구들 표준함으로써 이러한 문제들을 해결하기위해 디자인되었습니다. 우리의 목표는 당장 처한 문제들을 해결하는 것 뿐만 아니라, 비지니스와 함께 성장할 수 있는 시스템을 만드는 것입니다. . 2015년 중반부터 우리가 Michelangelo를 만들기 시작했을 때, 우리는 확장성있게 모델을 학습하고 production 환경의 서빙 컨테이너에 배포하는데 따른 어려움들을 해결하는 것에서부터 시작했습니다. 그리고, 우리는 feature 파이프라인을 공유하고 관리하기에 더 용이한 시스템을 만드는 것에 집중했습니다. 최근에는 개발자의 생산성(아이디어에서 첫 production 모델로 만드는 데까지 걸리는 시간을 어떻게 빠르게 할 것이냐와 이를 빠르게 반복하는 것)을 향상시키는 데 집중하고 있습니다. . 다음 section에서 우리는 어떻게 Michelangelo가 Uber의 특정 문제를 해결하기 위해 모델을 만들고 배포하는데 어떻게 사용되는지 이해하기 위해 예시 애플리케이션 하나를 살펴보려고 합니다. 우리가 UberEATS의 특정 use case를 강조하긴 했지만, 이 플랫폼은 회사 전반에 걸쳐 다양한 예측 use case를 위한 이와 비슷한 많은 모델들을 관리하고 있습니다. . Use case: Uber EATS 예상 배송 시간 모델 (Uber EATS ETD) . UberEATS는 음식 배달 시간 예측, 검색 랭킹, 검색 자동완성, 음식점 랭킹등을 포함하여, Michelangelo 에서 실행되는 여러가지 모델이 있습니다. 배달 시간 모델은 주문이 들어가기 전과 배달의 각 단계에서 음식이 준비되고 배달되기까지 얼마의 시간이 걸리는지 예측합니다. . Figure 1: UberEATS 앱은 Michelangelo에서 만들어진 머신러닝 모델을 이용하여 배달 시간을 예측합니다. . 음식 배달 시간을 예측하는 것은 쉬운일이 아닙니다. UberEATS의 사용자가 주문할 때 주문 요청을 처리하기 위해 음식점에 주문이 보내집니다. 음식점은 주문에 대해 응답하고 음식을 준비해야하는데, 주문의 복잡성과 음식점이 얼마나 바쁜지에 따라 얼마의 시간이 걸릴지 결정됩니다. 음식이 거의 준비되었을 때, Uber의 배달원이 음식을 픽업하도록 배정됩니다. 그리고 배달원은 음식점에 방문하고, 주차할 장소를 찾고, 음식점 안에 들어가서 음식을 받은 후, 차에 돌아와서 주문한 사람의 위치로 이동하고(이동경로, 트래픽, 기타 등등 의 영향을 받습니다), 주차공간을 찾고, 주문자의 문 앞에 가서 배달을 완료합니다. 그래서 Uber에서는 이러한 복잡한 여러 단계에 대해 전체 소요시간을 예측하는 것은 물론 각 단계에서 배달에 걸리는 시간을 재계산하는 것까지를 목표로 하고 있습니다. . UberEATS의 데이터사이언티스트들은 Michelangelo 플랫폼에서 이 end-to-end 배달 시간을 예측하는데, gradient boosted decision tree regression 모델을 사용합니다. 모델의 feature들은 주문 요청 (요청시간, 배달 장소 등) 에 대한 정보, historical feature들 (예를 들어 최근 7일간의 평균 음식 준비 시간), 그리고 near-realtime에 계산되는 feature들 (예를 들어, 지난 한 시간 동안의 평균 음식 준비 시간)을 포함합니다. 모델들은 Uber의 데이터센터를 지나 Michelangelo 모델 서빙 컨테이너로 배포되고, UberEATS의 마이크로 서비스들의 네트워크 요청에 의해 호출됩니다. 이러한 예측들은 UberEATS의 고객에게 음식점에서 주문하기 전, 그리고 음식이 준비되고 배달되는 동안에 디스플레이됩니다. . System architecture . Michelangelo 는 오픈소스 시스템들과 built in-house 컴포넌트들의 조합으로 구성되어 있습니다. 주요한 오픈소스 컴포넌트들로는 HDFS, Spark, Samza, Cassandra, MLLib, XGBoost, Tensorflow 를 사용하고 있습니다. 우리는 일반적으로 가능하면 fork하고, 커스터마이즈하고, 필요하다면 contribute 할 수 있는 성숙한 오픈소스 옵션을 사용하는 것을 선호하고, 오픈소스 솔루션이 우리의 usecase에 맞지 않으면 종종 우리 스스로 시스템을 구축합니다. . Michelangelo는 Uber의 데이터와 계산 인프라의 최상단에 구축되어, Uber의 내역과 로깅된 데이터를 저장하고 있는 data lake, 모든 Uber의 서비스들에서 로깅된 메시지들을 모으는 Kafka 브로커, Samza streaming 계산 엔진, Cassandra 클러스터, 그리고 Uber의 프로비저닝된 in-house 서비스, 그리고 배포 툴을 제공하고 있습니다. . 다음 section에서는, Michelangelo의 기술 세부사항에 대해 설명하기 위해 UberEATS ETD 모델을 case study로 하여 시스템 레이어를 살펴보겠습니다. . Machine Learning Work Flow . 분류, 회귀, 시계열 예측을 포함하여 Uber의 대부분의 머신러닝 usecase들은 대부분 같은 workflow를 따릅니다. Workflow는 일반적으로 구현에 독립적이고, 새로운 딥러닝 framework들과 같이 새로운 알고리즘 타입과 framework를 지원하기 위해 쉽게 확장됩니다. 그리고 online, offline (차안 그리고 휴대폰)에서의 예측 사례들과 같이 서로 다른 배포 모드들에도 적용됩니다. . 우리는 다음 여섯 단계의 workflow를 따르기 위해 Michelangelo를 확장성있고, 안정성있고, 재생산가능하고, 사용하기 쉽고, 자동화된 기능을 제공하도록 설계했습니다. . Manage data | Train models | Evaluate models | Deploy models | Make predictions | Monitor predictions | 그리고, 우리는 Michelangelo의 아키텍쳐가 어떻게 이 workflow의 각 단계를 용이하게 하도록 하는지 세부적으로 보려고 합니다. . Manage data . 우리는 좋은 feature를 찾는 것은 종종 머신러닝에서 가장 어려운 부분이고, 데이터 파이프라인을 구축하고 관리하는 것은 완전한 머신러닝 솔루션에서 가장 비용이 많이 드는 부분 중의 하나라는 사실을 발견했습니다. . 학습(그리고 재학습)을 위한 feature와 정답 데이터셋과 예측을 위한 feature로만 구성된 데이터 셋 생성을 위한 데이터파이프라인을 구축하기 위해 머신러닝 플랫폼을 표준화된 도구들을 제공해야합니다. 이러한 도구들은 회사의 data lake 또는 warehouse, 회사의 실시간 데이터 제공 시스템들과 깊이 통합되어 있어야합니다. 파이프라인들은 확장성 있고 효율적이어야하며, 데이터 흐름과 질에 대한 통합된 모니터링을 포함해야하고, online과 offline 학습과 예측을 지원해야합니다. 이상적으로 파이프라인들은 중복된 작업을 줄이고 데이터의 질을 높이기 위해서, 여러 팀들이 공유가능한 형태로 feature들을 생성해야합니다. 또한 사용자들로 하여금 best pratices들(예를 들어, 학습과 예측 단계에서 같은 데이터 생성 및 전처리 과정을 거치도록 하는 것을 쉽게 보장 하는 것)을 적용하기 용이하도록 하기 위해 강력한 가드레일과 제어 기능을 제공해야합니다. . Michelangelo의 데이터 관리 구성요소들은 online과 offline 파이프라인으로 나뉘어집니다. 최근에, offline 파이프라인들은 배치 모델을 학습하고, 예측하는 작업들에 사용되고, online 파이프라인들은 online 학습과 low latency를 가지는 예측들에 사용됩니다. . 또한, 사내 팀들이 그들의 머신러닝 문제에 대해 서로 공유하고, 발견하고, 잘 선정된 feature셋을 사용 할 수 있도록 데이터 관리 계층에 feature저장소를 추가 했습니다. 우리는 Uber의 많은 모델링 문제들이 동일하거나 비슷한 feature들을 사용한다는 것과, 이것이 사내 서로 다른 조직의 팀들에게 서로의 feature를 공유하는 것이 가능하다는 점에 상당한 가치가 있다는 것을 발견했습니다. . Figure 2: 데이터 준비 파이프라인에서 데이터를 Featrue Store 테이블과 학습 데이터 저장소에 넣습니다. . Offline . Uber의 거래 내역과 로그 데이터는 HDFS data lake로 흘러들어가고, Spark와 Hive SQL 계산 job으로 쉽게 접근가능합니다. 우리는 프로젝트에 private 하거나 팀들간에서 공유할 수 있도록 Feature Store에 배포되는 feature들을 계산하기 위한 규칙적인 job을 돌리기 위해 컨테이너와 스케줄링을 제공합니다. 배치 job들은 스케줄러 또는 트리거에 의해 동작하고, 코드나 데이터 이슈들 에서 발생한 이상현상을 빠르게 탐지하기 위해 data quality 모니터링 툴로 통합됩니다. . Online . 온라인에 배포된 모델들은 HDFS에 저장된 데이터에 접근할 수 없고, 종종 성능 관점에서 직접 Uber의 production 서비스의 온라인 database로 부터 계산하기 어려운 feature들이 있습니다. (예를 들어, 특정 기간대의 한 음식점의 평균 음식 준비 시간을 계산하기 위해 UberEATS의 주문 서비스 database에 직접 query할 수 없습니다.) 그래서, 우리는 온라인 모델에 필요한 feature들을 예측이 필요한 시점에 low latency로 읽을 수 있는 Cassandra에 미리 계산해서 저장해두었습니다. . 우리는 온라인에서 서비스되는 feature들을 계산하기 위해 두가지 옵션(아래 batch 계산, near-real-time 계산 방식)을 제공합니다. . Batch precompute. 첫 번째 옵션은 bulk 선행 계산과 HDFS에서 Cassandra로 historical feature들을 불러오는 옵션입니다. 이 일은 간단하면서 효율적이고, 일반적으로 매 시간이나 하루마다 갱신되는 historical feature에 잘 동작합니다. 이 시스템은 같은 데이터와 batch 파이프라인을 학습과 serving에 사용한다는 것을 보장합니다. UberEATS는 이 시스템을 ‘특정 음식점의 최근 7일간의 평균 음식 준비 시간’과 같은 feature들에 사용합니다. . Near-real-time compute. 두 번째 옵션은 관련있는 metric들을 Kafka에 배포하고, low latency로 aggregation feature들을 만들기 위해 Samza 기반의 스트리밍 계산 job을 돌리는 것입니다. 이러한 feature들은 serving을 위해 Cassandra에 바로 쓰여지고, 미래의 학습 job을 위해 HDFS에 로그로 남습니다. 배치 시스템과 같이 near-real-time 계산 또한 같은 데이터와 batch 파이프라인을 학습과 serving에 사용한다는 것을 보장합니다. 우리는 데이터를 다시 채우기 위해 로그 데이터에 맞게 배치 잡을 돌려서 학습 데이터를 생성합니다. UberEATS는 near-realtime 파이프라인을 ‘특정 음식점의 지난 1시간 동안의 평균 음식 준비 시간’ 와 같은 feature들에 사용합니다. . Shared feature store . 우리는 Uber의 팀들이 자신들의 팀 또는 다른 팀에서 사용되는 공동의 feature들을 만들고 관리할 수 있는 중앙화된 Feature Store를 만드는 것이 큰 가치가 있다는 사실을 발견했습니다. 높은 관점에서 이것은 두 가지를 달성했습니다. . 이것은 프로젝트의 목적을 위해 사용자들이 만든 feature들을 공유된 feature store에 쉽게 추가할 수 있도록 합니다. 한번 feature들이 Feature Store에 저장되면, feature의 정규 이름을 참조하는 방식으로 online과 offline 모두에서 사용하기 매우 쉬워집니다. 이러한 정보를 장착하고, 이 시스템은 모델 학습이나 배치 예측에 필요한 정확한 HDFS 데이터셋들을 join하고, online 예측을 위해 Cassandra에서 정확한 값을 가져오는 것을 핸들링합니다. . 현재, 우리는 Feature Store에 머신러닝 프로젝트들을 가속화 하기 위해 사용되는 대략 10,000개의 feature를 저장하고 있습니다. 그리고 회사내 팀들은 동시에 새로운 feature들을 추가하고 있습니다. Feature Store에 저장된 feature들은 일별로 자동으로 계산되고 갱신됩니다. . 앞으로는, 우리는 주어진 예측 문제를 해결하기 위해 Feature Store를 검색하여 가장 유용하고 중요한 feature들을 식별하는 자동화된 시스템을 만들 수 있는 가능성을 찾을 계획입니다. . Domain specific language for feature selection and transformation . 데이터 파이프라인에서 생성되거나, 클라이언트 서비스에서 보내진 feature들은 종종 모델에 적합한 형식을 가지고 있지 않고 또는, 값을 채워야하는 결측값일 수 있습니다. 더욱이 모델이 주어진 feature들의 부분 집합만을 필요로 할수도 있습니다. 예를들어, timestamp를 하루 중의 시간 또는 요일로 변경하는 것이 주기적 패턴을 찾는 모델에 더 유용할 수 있습니다. 다른 케이스로는, feature 값들이 정규화될 필요가 있습니다. (평균과 표준편차를 구하는 등) . 이러한 문제들을 풀기위해, 우리는 모델러들이 모델을 학습하고 예측하는 시점에 모델에 보내지는 feature들을 선택하고, 변형하고, 결합할 수 있는 DSL (domain specific language)를 만들었습니다. DSL은 Scala의 부분 집합으로 구현되었습니다. 이것은 일반적으로 많이 사용되는 함수들의 완전한 집합을 가진 순수 함수형 언어입니다. DSL을 이용해서 우리는 또한 고객팀이 그들만이 고유하게 정의한 함수들을 추가할 수 있는 가용성을 제공합니다. 그리고 현재의 context (데이터 파이프라인)에서 feature값들을 가져오는 accessor하는 함수가 있습니다. . DSL 표현식들은 모델 구성의 한 부분이고, 같은 최종 feature 세트를 보장하기 위해 같은 표현식들이 학습 시점과 예측시점에 적용된다는 사실은 중요합니다. . Train models . 우리는 최근에 offline 상에서 큰 스케일로 분산 처리되는 Decision tree 학습, linear와 logistic 모델들, unsupervised 모델들 (k-means), 시계열 모델들, 딥러닝을 지원합니다. 우리는 고객의 필요에 대한 대응으로 규칙적으로 Uber의 AI lab이나 내부 연구자들이 개발한 새로운 알고리즘들을 추가합니다. 또한 우리는 커스텀 학습, evaluation과 serving 코드를 제공하여 사내 팀들이 그들만의 모델 타입을 추가할 수 있도록 합니다. 분산 모델 학습 시스템은 수 억개의 샘플들을 다루기 위해 확장되고, 빠른 반복을 위한 작은 데이터셋까지 스케일 다운 됩니다. . 모델 configuration은 모델 타입, hyper parameter, 데이터 소스, feature DSL 표현식들 뿐 아니라, 컴퓨팅 리소스 요구사항들(머신 수, 메모리량, GPU 사용여부 등)까지 명시합니다. 이 명세는 YARN이나 Mesos 클러스터에서 돌아가는 학습 job을 구성하는데 사용됩니다. . 모델이 학습되고 난 후, 성능 지표(예: ROC 곡선, PR 곡선)들이 계산되어 모델 평가 보고서에 결합됩니다. 학습이 끝나고 원래의 configuration과 학습된 parameter들, 평가 보고서들은 분석과 배포를 위해 우리의 모델 저장소에 저장됩니다. . 단일 모델을 학습하는 것 외에도, Michelangelo는 파티션된 모델을 포함해서 모든 타입의 모델에 대해 hyper-parameter 검색을 지원합니다. 파티션된 모델을 사용하여 우리는 자동으로 사용자의 configuration에 따라 학습 데이터를 분할하고, 파티션 별로 모델을 학습하고 필요할 때 상위 모델로 되돌립니다 (예: 도시 별로 모델을 학습시키고, 정확한 도시 수준의 모델을 얻을 수 없을 때 국가 수준의 모델로 되돌립니다.). . 학습 job들은 웹 UI나 API, 또는 종종 Jupyter notebook을 통해 관리되고 구성될 수 있습니다. 많은 팀들은 API와 workflow 도구를 사용하여 정기적으로 모델을 재학습합니다. . Figure 3: 모델 학습 작업들은 Feature Store와 학습 데이터 저장소의 데이터셋을 사용하여 모델을 학습하고, 모델 저장소에 저장합니다. . (To-be continued) .",
            "url": "https://inahjeon.github.io/devlog/ml%20/%20data%20engineering/2019/03/24/meet-michelangelo.html",
            "relUrl": "/ml%20/%20data%20engineering/2019/03/24/meet-michelangelo.html",
            "date": " • Mar 24, 2019"
        }
        
    
  
    
        ,"post24": {
            "title": "Selenium으로 Google Play Store 크롤링하기",
            "content": "Selenium(https://www.seleniumhq.org) 으로 Google Play Store를 크롤링하는 간단한 웹 크롤러를 만들어 보았다. . Selenum 이란? . 브라우저를 자동화하는 툴로, 주로 자동화된 웹 테스팅, 웹 자동화 작업에 사용되는 라이브러리다. 데이터 분석 분야에서는 데이터를 크롤링하기 위한 용도로 많이 활용되고 있다. . 지원하는 웹 브라우저들 . Firefox | Internet Explorer | Safari | Opera | Chrome | . 많이 사용되는 웬만한 브라우저는 다 지원하고 있고, 언어도 Python, Java, R, Ruby 등 널리 쓰이는 다양한 프로그래밍 언어를 지원하고 있다. . 필요한 라이브러리 준비, 실행 . 크롤링을 위해 selenium 라이브러리를 설치하고, 사용할 웹 드라이버를 받아야한다. . Selenium (Python) . pip install selenium . Web driver . 웹 드라이버는 https://www.seleniumhq.org/about/platforms.jsp#browsers 에서 원하는 드라이버를 찾아서 다운받을 수 있는 링크로 이동할 수 있다. 여기선 익숙한 Chrome 드라이버를 설치해보았다. . . 웹 브라우저 실행하기 . 일단 간단하게 웹 브라우저를 실행해보았다. . from selenium import webdriver driver = webdriver.Chrome(&#39;./chromedriver&#39;) driver.get(&#39;https://www.google.com&#39;) . . 코드를 실행하면 웹 브라우저가 뜨고 입력한 url로 이동한다. . Play Store 인기차트 크롤링 . Selenium으로 어떤 데이터를 크롤링 해볼까하다가 Google Play Store의 인기 앱 차트 목록을 간단히 크롤링 하는 작업을 해보기로 했다. . . (뱅크샐러드가 인기 앱 11위에요! 😎 깨알같이 자랑…) . 크롤링 하고 싶은 영역 찾기 . html 문서를 크롤링 하기 위해서는 원하는 html element를 선택해서 내용을 가져와야 한다. Chrome 브라우저의 개발자도구를 활용해서 원하는 내용을 포함하고 있는 element를 선택하기 위한 정보(id, class name, tag… 등을 확인할 수 있다. . . class name 이 details 인 div 안에 원하는 내용이 있어서 find_elements_by_class_name 함수를 활용해서 내용을 가져올 수 있었다. . from selenium import webdriver driver = webdriver.Chrome(&#39;./chromedriver&#39;) driver.get(&#39;https://play.google.com/store/apps/collection/topselling_free&#39;) top_app_details = driver.find_elements_by_class_name(&#39;details&#39;) for app in top_app_details: print(app.text) . 실행 결과: . 1. 브롤스타즈 Supercell 2. Color Bump 3D Good Job Games 3. YouTube Kids Google LLC 4. CGV CJ CGV 5. Netflix(넷플릭스) Netflix, Inc. 6. 카카오톡 KakaoTalk Kakao Corporation 7. Samsung Smart Switch Mobile Samsung Electronics Co., Ltd. 8. 신비아파트 고스트헌터 주식회사 3F FACTORY ... . 내용을 잘 긁어온 것을 확인할 수 있다. . 카테고리 별, 유료/무료 인기 top N개의 앱 리스트를 긁어오기 . from selenium import webdriver from datetime import datetime def crawling_top_apps( web_driver, category: str = &#39;&#39;, paid: str = &#39;&#39;, limit: int = 10 ): category_url = f&#39;/category/{category}&#39; free_app_rank_url = f&#39;https://play.google.com/store/apps{category_url}&#39; f&#39;/collection/topselling_{paid}&#39; web_driver.get(free_app_rank_url) top_app_details = web_driver.find_elements_by_class_name(&#39;details&#39;) for idx, app in enumerate(top_app_details[:limit]): content = app.text.split(&#39;. &#39;)[-1] app_title = content.split(&#39; n&#39;)[0] corp = content.split(&#39; n&#39;)[-1] print(f&#39;{idx + 1}위 {app_title} - {corp}&#39;) if __name__ == &#39;__main__&#39;: top_n = 10 categories = [ &#39;HEALTH_AND_FITNESS&#39;, &#39;FINANCE&#39;, &#39;SHOPPING&#39; ] paid = &#39;free&#39; driver = webdriver.Chrome(&#39;./chromedriver&#39;) today = datetime.now().strftime(&#39;%Y-%m-%d&#39;) for category in categories: print(&#39;-&#39; * 60) print(f&#39;{today} Top {top_n} {paid} Apps in {category}&#39;) print(&#39;-&#39; * 60) crawling_top_apps(driver, category, paid, top_n) . 실행 결과: . 2019-02-11 Top 10 free Apps in HEALTH_AND_FITNESS 1위 Samsung Health(삼성 헬스) - Samsung Electronics Co., Ltd. 2위 캐시슬라이드 스텝업 - No.1 적립형 만보기앱 - NBT Inc. 3위 M건강보험 - 국민건강보험공단 4위 AIA Vitality x T건강걷기 - AIA Vitality (바이탈리티) 5위 복근 운동 - 28일 이내 지방을 연소시켜 111자 복근을 만들어줍니다 - Super T Group 6위 마이아큐브 - (주)한국존슨앤드존슨 7위 ANT+ Plugins Service - ANT+ 8위 홈 트레이닝 - 기구가 필요 없습니다 - Leap Fitness Group 9위 만보기 - ITO Technologies, Inc. 10위 건강iN(건강인) - 국민건강보험공단 2019-02-11 Top 10 free Apps in FINANCE 1위 뱅크샐러드 - 신경 꺼도 내 돈 관리 (통합 자산관리, 자동 가계부) - Rainist Co., Ltd. 2위 NH스마트뱅킹 - 농협BANK 3위 카카오뱅크 - 같지만 다른 은행 - 카카오뱅크 4위 토스 - Viva Republica 5위 ISP/페이북 - VP Inc 6위 KB국민은행 스타뱅킹 - KB국민은행 7위 NH콕뱅크 - 농협BANK 8위 터치엔 엠백신 for Web(기업용) - 라온시큐어(주) 9위 KB국민카드 - KB KOOKMINCARD CO., LTD. 10위 바이오인증 공동앱 - 금융결제원(KFTC) 2019-02-11 Top 10 free Apps in SHOPPING 1위 쿠팡 (Coupang) - 쿠팡 2위 당근마켓 - 우리 동네 중고 직거래 벼룩장터 - 당근마켓 3위 롯데홈쇼핑 LOTTE Homeshopping - Lotte Homeshopping 4위 위메프 - 특가대표 (특가 / 쇼핑 / 쇼핑앱 / 쿠폰 / 배송) - 위메프 5위 SSG.COM - SSG.COM 6위 공구마켓 - 제이슨 컨텐츠 7위 현대Hmall - 홈쇼핑, 백화점 - (주) 현대홈쇼핑 8위 롯데백화점 - 롯데쇼핑 9위 심쿵할인 - 제이슨 컨텐츠 10위 GS SHOP - 당신의 가장 좋은 선택을 만듭니다 - GS SHOP . 완성! (금융 앱 1위는 역시 뱅크샐러드 💰) . 자, 그럼 본격적으로 앱 정보, 리뷰 데이터까지 다 긁어오는 기능까지 만들자! . 라고 생각했지만… 크롤링 코드를 짜면서 html 요소를 찾고 값을 가져오고 하는 작업이 생각보다 귀찮았고 (-.-) 찾아보니 당연하게도 Play store 데이터를 크롤링하는 오픈소스들이 많이 있어서 바로 생각을 접었다. 크롤링할 영역 클릭만 하면 알아서 크롤링해주는 오픈소스 누가 만들어 주면 좋겠다. . Google login . 뭔가 아쉬워서 Google 계정 로그인을 하는 스크립트도 만들어보았다. 로그인을 하기 위해서는 input form에 계정정보를 입력하고, 로그인 버튼을 클릭하는 동작을 지정해야한다. input form의 값은 send_keys라는 함수로 넣을 수 있고, click 함수를 사용해서 html element 요소를 click하는 액션을 지정할 수 있다. . . Google 계정 로그인 프로세스는 . 1) email 주소를 입력하고 . 2) 다음 버튼을 클릭 . 3) password 를 입력하고 . 4) 다음 버튼을 클릭 . 의 단계로 진행된다. . from selenium import webdriver from meta import MY_EMAIL, MY_PASSWORD if __name__ == &#39;__main__&#39;: driver = webdriver.Chrome(&#39;./chromedriver&#39;) driver.implicitly_wait(3) driver.get(&#39;https://accounts.google.com/ServiceLogin?hl=ko&amp;passive=true&amp;continue=https://www.google.com/&#39;) driver.find_element_by_name(&#39;identifier&#39;).send_keys(MY_EMAIL) driver.find_element_by_id(&#39;identifierNext&#39;).click() driver.find_element_by_name(&#39;password&#39;).send_keys(MY_PASSWORD) element = driver.find_element_by_id(&#39;passwordNext&#39;) driver.execute_script(&quot;arguments[0].click();&quot;, element) . 로그인 프로세스대로 동작을 넣고, 실행해보면 알아서 값을 넣고 로그인하는 걸 볼 수 있다. . 컴퓨터가 혼자서 내 계정에 로그인 하는걸 보고 있으면 뭔가 기분이 묘하다; .",
            "url": "https://inahjeon.github.io/devlog/ml%20/%20data%20engineering/2019/02/10/web-crawling.html",
            "relUrl": "/ml%20/%20data%20engineering/2019/02/10/web-crawling.html",
            "date": " • Feb 10, 2019"
        }
        
    
  
    
        ,"post25": {
            "title": "CloudML Engine으로 모델 학습부터 배포까지",
            "content": "0. CloudML engine 이란 . . 학습 데이터 셋과 모델만 작성하면 나머지 모델을 학습시키고, 학습된 모델 버전을 관리하고, production에 활용할 수 있도록 배포하는 등의 머신러닝 workflow에서 필요한 기능들을 제공해주는 서비스이다. . Cloud 머신러닝 모델 학습 환경을 찾고 있던 터라 한번 사용해 보았다. . 1. 학습 dataset 준비 . GCP 문서에서는 미리 제공된 census 데이터를 활용하는 예제가 있지만, 예제만 그대로 따라했을 때 잘 모르고 넘어가게 되는 부분이 있어서 직접 다른 데이터셋과 코드를 짜서 진행해보기로 했다. . 학습 데이터는 kaggle titanic dataset으로 간단히 테스트해보기로 했다. (아무 이유 없고 그냥 바로 생각나서) . kaggle에 올려져있는 dataset은 kaggle api 를 활용해서 다운 받을 수 있다. . 1) kaggle api 설치 . pip install kaggle . 2) kaggle api 활용하기 위한 token key 등록 . kaggle profile 기능 에서 kaggle.json 파일(username, kaggle_key 정보가 있는 파일)을 다운 받아서 ~/.kaggle/kaggle.json 에 위치하도록 옮긴 후, 다음과 같이 파일 권한을 조정한다. . chmod 600 ~/.kaggle/kaggle.json . 3) 데이터 셋 다운로드 . kaggle competitions download -c titanic . titanic dataset이 이 다음과 같이 다운로드 된 것을 확인할 수 있다. . . 2. 로컬에서 모델 만들고 학습 테스트 . ml engine에 사용할 코드는 아래 git repository 있는 예제 코드들을 참고해서 작성했다. . https://github.com/GoogleCloudPlatform/cloudml-samples . 아래 주소에서는 ml engine의 기본 코드 구조에 대한 설명을 확인할 수 있다. https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/cloudml-template . metadata.py: input data, target feature, unused features 등 주로 입력 데이터 셋에 대한 정의를 하는 파일이다. . | input.py: input 파일을 custom하게 읽고, 간단한 데이터 처리를 하는 곳이다. | featurizer.py: feature engineering을 위한 곳 | model.py: 학습 모델을 정의하는 곳 | task.py: cloud ml engine의 명령어를 처리하는 인터페이스 | . 예제 코드에서 나머지는 크게 수정하지 않았고, 데이터 정의와 전처리를 위해 metadata.py, input.py 만 titanic 데이터에 맞게 수정해서 사용했다. . cloud ml engine을 사용하는게 목적이라 feature는 5개 정도만 간단히 넣어서 학습을 진행했다. . 로컬에서 작성한 모델은 다음과 같이 테스트해 볼 수 있다. . gcloud ml-engine local train --module-name trainer.task --package-path trainer/ --job-dir output -- --train-files data/train.csv --eval-files data/eval.csv --train-steps 1000 --eval-steps 100 . 학습에 사용한 train.csv 과 eval.csv 는 원본 train.csv 의 데이터를 나눈 데이터를 사용했다. . 3. Cloud ML Engine으로 모델 학습하기 . GCP에서 ML engine을 돌리기 위해 로컬에서 테스트했던 코드와 dataset을 Google cloud storage에 업로드 한다. . . data: 학습할 데이터 | models: 학습된 모델을 저장할 장소 | trainer: 학습 모델 코드 | . Cloud ml engine은 다음과 같이 job을 만들어서 작업을 큐에 넣어 실행시킬 수 있다. . gcloud ml-engine jobs submit training $JOB_NAME --module-name trainer.task --package-path trainer/ --job-dir $OUTPUT_PATH --runtime-version 1.8 --region $REGION -- --train-files $TRAIN_DATA --eval-files $EVALUATION_DATA --train-steps 1000 --eval-steps 100 . 작업을 실행시키면, ML엔진에서 job이 실행되고 실시간으로 log를 조회할 수 있다. . (작업이 최종적으로 완료된 모습) . titanic_single_8 이라는 jobname을 보면 유추할 수 있듯이 예제를 따라해보며 single machine으로 돌린 titanic 모델 학습 job을 7번째 실패하고 8번째 시도만에 드디어 성공했다는 의미이다…! :sob: 흙.. . 성공한 모델 . . 드디어 첫번째 버전의 titanic 예측 모델이 배포되었다. . 4. 생성한 모델로 prediction 해보기 . Cloud ML engine에 올려진 모델을 이용해서 특정 input data 의 결과값을 prediction 해볼 수 있다. . 파일 형식은 csv, json 형태를 지원하고 있고, 다음과 같이 test.json 을 만들어서 prediction 해보았다. . {&quot;Pclass&quot;: 3,&quot;Sex&quot;: &quot;male&quot;,&quot;Age&quot;: 0,&quot;SibSp&quot;: 0,&quot;Embarked&quot;: &quot;Q&quot;} {&quot;Pclass&quot;: 1,&quot;Sex&quot;: &quot;female&quot;,&quot;Age&quot;: 20,&quot;SibSp&quot;: 0,&quot;Embarked&quot;: &quot;Q&quot;} . 여기서 처음에 한참 헤멨었는데, 입력 형식이 요상하다. -_- . json format이라고 되어있는데, line마다 json input 데이터 1개가 들어가는 형식 이다. input 파일 형태에 대해 문서에 명확하게 잘 나와있지 않아서 github 예제 코드에서 참고해서 만들었다. :cry: . 그리고 모델에서 학습에 사용하지 않는 feature column key를 넣었을 때, 인식이 안되어서 에러가 났다. :thinking_face: (단순 예제 파일을 고쳐서 돌려서 여러가지 input 처리들이 잘 안되어 있는 탓일까..) . 어찌됬든 test.json 과 만든 모델로 다음과 같이 prediction을 실행해 볼 수 있다. . prediction . gcloud ml-engine predict --model titanic101 --version titanic1 --json-instances test.json . 예측결과 . . 위와 같이 예측된 class 와 확률값에 대해서 확인해 볼 수 있다. . 5. 간단한 서버로 ML서비스 만들어보기 . 이제 최종적으로 Google cloud ML API를 활용해서 간단한 머신러닝 서비스를 만들어보자. . 서버 라이브러리는 python flask를 활용했다. . google-api-python-client==1.6.2 google-auth google-auth-httplib2 flask ujson . 필요한 라이브러리를 설치하고, 다음과 같이 간단한 서버를 작성했다. . app.py . import ujson from google.oauth2 import service_account from flask import Flask, jsonify, request from googleapiclient import discovery from .meta import GOOGLE_APPLICATION_CREDENTIALS, PROJECT_ID, MODEL_NAME, MODEL_VERSION def get_google_api_client(): raw_credential = ujson.loads(GOOGLE_APPLICATION_CREDENTIALS) credentials = service_account.Credentials.from_service_account_info(raw_credential) return discovery.build( &#39;ml&#39;, &#39;v1&#39;, credentials=credentials ) GOOGLE_API_CLIENT = get_google_api_client() app = Flask(__name__) @app.route(&#39;/&#39;) def index(): return &#39;Hello Cloud ML Engine!&#39; @app.route(&#39;/predict&#39;, methods=[&#39;POST&#39;]) def predict(): data = request.json name = &#39;projects/{}/models/{}/versions/{}&#39;.format( PROJECT_ID, MODEL_NAME, MODEL_VERSION ) response = GOOGLE_API_CLIENT.projects().predict( name=name, body={&#39;instances&#39;: data[&#39;data&#39;]} ).execute() if &#39;error&#39; in response: raise RuntimeError(response[&#39;error&#39;]) return jsonify(response[&#39;predictions&#39;]) . (GOOGLE_APPLICATION_CREDENTIALS 값은 API 사용이 허가된 서비스 계정에서 키를 생성해서 받는 json을 이용하여 만들 수 있다.) . PROJECT_ID, MODEL_NAME, MODEL_VERSION 는 각각 사용했던 GCP 프로젝트 명, 학습된 모델명, 버전명을 넣어주면 된다. . 그리고 다음과 같이 실행하면 기본적으로 localhost:5000 에 서버가 띄워진다. . FLASK_APP=app.py flask run . . 사실 여기서도 별거 아닐 줄 알았는데, 한참 해멨다. . 중간에 서버를 띄우고 API 요청을 날렸는데 403에러(API가 비활성화 되어있습니다. 어쩌구저쩌구,,)가 나와 IAM 권한만 생성했다 지웠다가, API 활성화 되어있는데 뭐지 하면서 헤맸다. 나중에 알고보니 client를 생성할 때 GCP project id 값을 넣어야 하는데, project name을 넣어서 작동하지 않는 에러였다. -_-… . API 호출 결과 . . 잘 돌아간다…! (속도는 매우 느리다…) . API monitoring . 모델 대시보드에서 다음과 같이 API 호출에 대한 모니터링도 제공하고 있다. . . 느낀점 . 학습 시간이 오래 걸리는 모델을 학습 시킬 때나, batch prediction이 필요한 작업에서 파이프라인을 구성해서 유용하게 써볼 수 있을 것 같다. 제공하고 있는 monitoring, parameter tuning 등에 대해서도 좀 더 알아봐야 할 것 같다. | 문서에서 명확하게 설명하지 않고 대충 스펙을 생략한 경우가 있어 생각보다 많이 헤맷다… | 아직 shell script로만 제공하고 있는데, GUI로 된 인터페이스가 있다면 매우 유용해질 것 같다. | .",
            "url": "https://inahjeon.github.io/devlog/ml%20/%20data%20engineering/2019/01/13/cloud-ml-engine.html",
            "relUrl": "/ml%20/%20data%20engineering/2019/01/13/cloud-ml-engine.html",
            "date": " • Jan 13, 2019"
        }
        
    
  
    
        ,"post26": {
            "title": "Word Embeddings 2",
            "content": "An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec - 번역 (2) . 개인 공부 용 번역글 입니다. . Reference: . An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec . Prediction based Vector . Word Embeddings (1) 에서 단어를 vector화 하는 것에 관한 deterministic한 방법들을 살펴보았는데, 이 방법들은 word2vec에 비해 단어의 특성을 표현하는데 있어서 제한적이었다. . Mitolov가 제안한 word2vec의 경우 prediction 기반의 embedding 방법으로, 단어 유추 및 단어 간 유사성에 관련된 작업들에 대해서 최고의 성능을 보이고 있다. (예를 들어 word2vec을 이용해서 King - man + woman = Queen 과 같은 결과물도 얻을 수 있었다.) . Word2vec은 문맥(Context)으로부터 한 단어를 추측해내는 CBOW(Continuous bag of words) 와 한 단어로 부터 문맥을 Skip-gram 모델 조합으로 이루어져 있다. (Context란 한 단어 또는 여러 개의 단어 그룹을 의미한다.) 두 모델 모두 neural network 를 이용하여 데이터를 학습시킨다. . CBOW (Continuous bag of words) . CBOW 모델은 주어진 문맥(context)에서 어떤 단어의 확률을 예측하는 모델이다. . 예를 들어 다음과 같은 코퍼스가 주어졌다고 가정하자. . C = “Hey, this is sample corpus using only one context word.” . Context window = 1일 때, 다음과 같이 CBOW모델의 학습데이터를 구성할 수 있다. . Input, Output 1 = (Hey, this) | Input, Output 2 = (this, Hey) | Input, Output 3 = (this, is) | Input, Output 4 = (is, this) | . … . Input, Output 16 = (context, word) | Input, Output 17 = (word, context) | . 이때 각 Input, Output 은 one-hot 인코딩된 단어 vector로 표현된다. . Input, Ouput1의 예시: . Hey = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] | this = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] | . CBOW 모델의 도식적 표현은 다음과 같다. . (Reference: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec) . Input Layer: [1 X V] (one-hot encoding) | Hidden Layer: [V X N] (N: hidden layer의 뉴런 개수) | Oputput Layer: [1 X V] (one-hot encoding) | . 위 예시는 context winow = 1인 경우로, 여러 단어를 가지는 문맥일 경우는 다음과 같이 표현할 수 있다. . 예: context window = 3인 경우 . . (Reference: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/) . Skip-Gram Model . Skip-gram 모델은 CBOW 모델과 정반대로, 하나의 단어에서 context(하나 이상의 단어 그룹)를 예측하는 모델이다. . 참고: CBOW와 Skip-gram모델의 차이를 직관적으로 잘 표현해주는 그림 . . (Reference: https://textprocessing.org/wp-content/uploads/2017/03/CBOW-SKIPNGRAM.png) . 앞선 CBOW 의 예제 corpus에서, context-window = 1일 때, 다음과 같이 Skip-gram model의 학습 데이터셋을 구성해 볼 수 있다. . Input, Output 1 = (Hey, [this, &lt;padding&gt;]) | Input, Output 2 = (this, [Hey, is]) | Input, Output 3 = (is, [this, sample]) | . … . Input, Output 10 = (context, &lt;padding&gt;) | . Skip-gram 모델의 도식적 표현은 다음과 같다. . . (Reference: https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/05000515/Capture2-276x300.png) . 번외 . word2vec에 관한 요약글을 번역하고 관련 자료들을 찾아보다가 개인적으로 느낀점은 겉핥기 식으로 핵심 아이디어는 알 수 있었지만, 짧은 설명으로 명확하게 이해되지 않는 부분이 많아서 원 논문을 보거나 자세히 설명된 강의 자료를 보는 것이 더 낫겠다는 생각이 들었다. 그냥 찬찬히 공부해봐야지. (; ㅁ ;) . Stanford cs224n NLP 강의: http://web.stanford.edu/class/cs224n/syllabus.html . | Word2vec paper: Efficient Estimation of Word Representations in Vector Space . | .",
            "url": "https://inahjeon.github.io/devlog/nlp/2018/12/09/word-embedding2.html",
            "relUrl": "/nlp/2018/12/09/word-embedding2.html",
            "date": " • Dec 9, 2018"
        }
        
    
  
    
        ,"post27": {
            "title": "Word Embeddings 1",
            "content": "An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec - 번역 (1) . 개인 공부 용 번역글 입니다. . Reference: . An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec . Word Embedding 이란 . . Word embedding은 대부분의 기계 학습 알고리즘에서 원시 문자열이나 텍스트 정보를 처리할 수 없기 때문에, 정보를 처리하고 학습 알고리즘에 이용하기 위해 숫자 Vector형태로 단어를 변환하는 것이다. . 가장 간단한 방법으로는 ‘고유 단어 목록’ 사전을 이용해서 one-hot encoding을 통해 vector로 변환 할 수 있다. . 예를 들어 다음과 같은 고유 단어 사전이 있을 때, . [‘사과’, ‘언어’, ‘커피’, ‘영화’] . ‘사과’ 라는 단어를 [1, 0, 0, 0] 이라는 숫자 vector로 변환할 수 있다. . Word Embedding의 유형 . . Frequency based Embedding | Prediction based Embedding | Frequency based: Count Vector . . D개의 문서 {d1, d2, …, dD} 와 N개의 고유한 단어 토큰을 가진 corpus C가 있다고 생각해보자. . Count vecor 행렬 M은 D X N의 크기를 가지고, 각 행은 문서 D(i)에서 각 단어 토큰의 frequency를 나타낸다. . 예를 들어, . D1: He is a lazy boy. She is also lazy. . D2: Neeraj is a lazy person. . 이라는 두 문장이 주어졌을 때, . 고유 단어 토큰은 [He, She, lazy, boy, Neeraj, person] 로 구성된다. . 그러면 count matrix M은 다음과 같은 2 x 6 행렬로 표현할 수 있다. . He She lazy boy Neeraj person   . D1 | 1 | 1 | 2 | 1 | 0 | 0 | . D2 | 0 | 0 | 1 | 0 | 1 | 1 | . matrix M 의 행은 문서들, 열은 단어 토큰에 대응된다. . Count vector는 여러 변형이 있는데, 일반적으로 다음과 같이 나누어진다. . 1) dictionary를 구성하는 방법 . 실세계 언어는 수많은 단어로 구성되어 있기 때문에 모든 단어를 다 사용하면, 매우 sparse한 matrix가 되고, 쓸모없이 크기 때문에 계산하는 데에도 매우 비효율적이다. 그렇기 때문에 대안으로 10,000개의 상위 빈도를 가지는 단어들만으로 사전을 구성해서 사용하기도 한다. . 2) 각 단어를 세는 방법 . 각 단어가 문서에 얼마나 나타났는지 빈도(frequency)를 세는 방법과 단순히 나타났는지 아닌지(presence) 여부로 표현 할 수 있다. 일반적으로 빈도를 세는 방법이 더 많이 사용된다. . Frequency based: TF-IDF Vector . . Count vector와 동일하게 빈도 기반의 벡터화 방법이지만, 단일 문서 내에서의 단어 출현 빈도 뿐 아니라 전체 코퍼스에서의 출현 빈도를 고려한 방법. . ‘is’, ‘the’, ‘a’ 등과 같은 일반적인 단어는 거의 모든 문서에서 높은 빈도로 나타나기 때문에, 거의 모든 문서에서 발생하는 일반적인 단어의 비중을 줄이고, 각 문서에서 특별하게 많이 나타나는 단어의 가중치를 높게 주도록 동작하는 방법. . TF-IDF 계산방법 . 예) . Document1: (This, 1), (is, 1), (abount , 2), (Messi, 4) . Document2: (This, 1), (is, 2), (abount , 1), (Tf-idf, 1) . 로 주어진 코퍼스가 있을 때, . TF(Term Frequency, 단어 빈도) = (용어 t가 문서에 나타나는 횟수) / (문서의 용어 수) . . TF(This, Document1) = 1/8 . TF(This, Document2) = 1/5 . TF(Messi, Document2) = 4/8 . . IDF(Inverse Document Frequency, 역문서 빈도) = log(N/n) (N: 문서의 수, n은 용어 t가 출현한 문서의 수) . . IDF(This) = log(2/2) = 0 . IDF(Messi) = log(2/1) = 0.301 . TF-IDF (This, Document1) = (1/8) * (0) = 0 . TF-IDF (This, Document2) = (1/5) * (0) = 0 . TF-IDF (Messi, Document1) = (4/8) * (0.301) = 0.15 . . Frequency based: Co-Occurrence Vector . . 유사한 단어가 함께 발생하는 경향이 있고, 비슷한 문맥을 갖는 다는 점에서 아이디어를 얻어, 단어들의 동시 발생 횟수로 행렬을 구성하는 방법 . Co-occurrence - 주어진 코퍼스에 대해 단어 쌍의 co-occurrence는 단어 w1, w2가 context window에 함께 나타난 횟수를 의미함. . Context window - co-occurrence를 측정하기 위한 범위를 의미하는 window . . ex) 다음과 같은 문장에서 context window size = 2로 주어졌을 때, . [Quick Brown Fox Jump Over] The Lazy Dog . Fox라는 단어와 동시 발생한 단어는 Quick, Brown, Jump, Over이다. . Quick Brown [Fox Jump Over The Lazy] Dog . 비슷하게 Over 와 동시 발생한 단어는 Fox, Jump, The, Lazy가 된다. . . 코퍼스의 단어 크기가 V일 때, Co-occurrence 행렬의 크기는 V x V 가 된다. 일반적으로 V x V 는 매우 크기가 크기 때문에, 불용어와 같이 불필요한 단어를 제거하여 사용한다. 그렇지만, 여전이 단어 집합의 크기가 매우 크기 때문에, PCA, SVD등의 기법을 활용하여 차원을 축소하거나, 행렬 인수 분해를 사용한다. . (Word Embedding 2 에서 계속..) .",
            "url": "https://inahjeon.github.io/devlog/nlp/2018/11/25/word-embedding1.html",
            "relUrl": "/nlp/2018/11/25/word-embedding1.html",
            "date": " • Nov 25, 2018"
        }
        
    
  
    
        ,"post28": {
            "title": "글또를 시작하며",
            "content": "글쓰기 모임을 시작했다. . 원래 8월 말부터 혼자 공부했던 내용이나, 특정 이슈에 대해서 생각을 정리하는 용도로 개인 블로그를 시작했다. 하지만 사람은 원래 게으른 동물이기 때문에(?) 미루고 미루고 미루면서 역시나 꾸준히 글을 잘 쓰지 못했다. 대략 3개월 간 거의 컨퍼런스 참여하면서 들었던 세션 요약 3편 정도만 썼던 것 같다. . 그런 와중에, 마침 확실한 데드라인과 글쓰기 습관을 만들도록 도음을 받을 수 있는 훌륭한 모임(글또)이 있어 참여해보게 되었다. . 글쓰기 모임의 룰은 10만원의 예치금을 내고 2주마다 1편 씩 글을 쓰는데, 정해진 데드라인에 글을 쓰지 않을 경우 1만원씩 차감하는 방식이다. 그리고 2018년 11월부터 2019년 5월까지 총 12편을 글을 작성하게 되며, 총 3회의 pass권이 주어진다. . 혼자서는 실천이 어려웠는데, 이렇게 모임을 하면서 다름 사람에게 목표를 공유하고 데드라인까지 있어서 이번에는 목표한 바를 이루고 진행하면서 스스로 많이 배우고 성장할 수 있는 계기가 될 것 같다. :) . 이루고 싶은 목표 . 미루고만 있었던 공부, 개인 프로젝트를 이번 기회에 하나씩 진행해서 이에 대한 주제로 글쓰기. | 내용 정리를 지금보다 더 깔끔하게 잘하고, Markdown 손에 익히기. | Pass권 쓰지않고 예치금 꼭 다 받아가기. | . 어떤 글을 쓸까? . 11월부터 내년 5월까지 6개월동안, 12편의 글을 쓰게 된다. 대략적으로 어떤 내용에 대해서 써볼 지 아이디에이션 해봤는데, 평소에 해보고 싶었던 주제들은 많이 있었어서 금방 채워지긴 했다. . 글 주제는 크게 다음 3가지 정도로 나누어 질 것 같다. . 관심있는 데이터 분석해보고 인사이트 정리한 글 . 금융 데이터 분석. 다니는 회사가 금융 도메인이다보니 자연스레 관심이 많아져서 금융 관련해서 몇 가지 주제를 잡아서 진행해보고 싶다. | 음식, 주류 데이터 평점이나 성분 데이터가 있으면 분석해보고, 나에게 추천해보면 재밌을 것 같다. | . 관심 있는 분야 공부한 내용 정리한 글 . 딥러닝 또는 강화학습 또는 자연어처리 | 개인 프로젝트로 게임 데이터 강화 학습 시키고 시뮬레이션 해보는 것 정말 해보고 싶다. | . Data Scientist로 일하면서 느꼈던 점 . 주로 하는 일과 업에 대해서 느꼈던 점을 정리하며 앞으로 어떤 방향을 가지고 일할 것인가 에 대해서 한번 회고하고 정리해보고 싶다. | 어떤 사람을 채용해야하는 것인가. 요즘 면접에 많이 참여하고 있다 보니 어떤 특성을 가진 사람을 채용헤야 하는 지에 대해 관심이 많고, 이를 잘 파악하기 위해서 고민이 많아서 이 내용도 한번 정리해보고 싶다. | . 번외: . 첫 모임에서 알게된 유용했던 꿀팁들 . 유명한 개발자 블로그를 수집해둔 awesome-dev-blog 라는 repository가 있었다. 관련 분야 블로그 리스트업하고 구독해야겠다. | 개인슬랙 활용. 슬랙은 회사에서 업무 도구로만 쓰고 있었는데, 개인 계정으로 자료들을 모아놓거나 소식을 구독할때 매우 유용하게 쓸 수 있는 툴도 될 수 있다는 것을 알았다. :+1: | github에 이미지를 첨부할 때 이미지를 github에 일일이 올려서 링크를 넣었는데, dropbox에 올려서 편하게 첨부하는 법을 알았다(!). | . 블로그 개선 . 지금은 github 블로그 테마로 https://github.com/niklasbuschmann/contrast를 사용하고 있는데, 글감을 생각해보다보니 주제 별로 나누어서 볼 수 있게 navigation이 필요하다는 생각이 들었다. 다른 테마나 블로그 설정을 좀 더 찾아봐야겠다. 직접 개발을 하고 싶은 욕심도 있지만 효율상 꾹 참고 잘 만들어진 도구를 활용해야지. .",
            "url": "https://inahjeon.github.io/devlog/diary/2018/11/11/start-geultto.html",
            "relUrl": "/diary/2018/11/11/start-geultto.html",
            "date": " • Nov 11, 2018"
        }
        
    
  
    
        ,"post29": {
            "title": "Google Cloud Summit 2018",
            "content": "2018-10-25-google-cloud-summit-review . 느낀점 . 생각보다 규모가 매우 큰 행사였다. 여러 클라우드 서비스 컨설팅 업체나 제휴사들이 많이 참석하고, 제품을 홍보하는 부스가 많았다. 구글에서도 클라우드 서비스를 홍보하거나, 교육 프로그램이 많았다. . 그 중에서 quicklab 이라는 교육 프로그램을 통해서 cloudML을 사용해서 머신러닝 모델을 학습시키고 배포하는 프로그램을 체험해봤다. 주어진 github 코드와 데이터 셋이 있어서 메뉴얼에 나온 명령어만 그대로 따라하면 되어 매우 쉬웠다. 쉬웠던 만큼 크게 머리에 남은건 없었던 것 같지만, 교육 프로그램에서 cloudML 서비스를 한번 체험시키는 목적은 달성한 것 같다. . 전체적으로 구글 클라우드 짱짱맨이니까 많이 쓰세요! 였지만, Data Scientist로써 GCP를 잘 활용하고 있고 좋아하기 때문에 어떤 것들을 도입해 볼수 있을지 product 구축 사례를 잘 들었다. . 세션: 데이터 사이언티스트를 위한 Cloud AI Platform 소개 . 한기환 - Cloud Consultant, Google Cloud . 이용운 - 팀장, nARC (netmarble AI Revolution Center) 콜럼버스실 데이터인텔리전스팀, 넷마블 . 1. 기업의 목소리 . AI 채택까지의 먼 길 . 40%이상의 기업이 AI를 통한 이익에 대해 확신이 없음 (ROI가 나올 것인가) | 20%의 기업은 AI도입을 위해 노력중 | . 누가 AI를 사용할 수 있나? . 2100만명: 개발자 | 100만명 이하: 데이터 사이언티스트 | 1000명: 딥러닝 연구원 | . 컴퓨팅 성능의 중요성 . 최근 AI발전의 80%는 컴퓨팅 성능 향상에 따른 것임. | 85%의 기업이 다중 클라우드 전략을 사용하고 있거나 채택 과정에 있음. | . 2. 구글의 비전: 구객과 파트너가 AI를 쉽고 빠르고 유용하게 사용할 수 있도록 대중화하자 . CloudAI 소개 . 초보자: Cloud ML Engine | 능숙자: BigQuery ML | 전문가: AutoML 및 CloudML API | . 숙련도에 따라 누구나 활용 가능한 서비스들이 있음. . 왜 구글을 사용해야하는지? . 규모 | 속도 | 품질 | 접근가능성 | . 데이터 서비스쪽은 google cloud 를 가장 선호했음 (통계) . 3. 서비스 소개 . Cloud Dataprep 분석 또는 ML 프로젝트를 위한 데이터를 시각적으로 준비 | . | Data Studio를 사용한 협업 BI 원클릭 시각화 | . | BigQuery ML 데이터를 이동하지 않고, 계획된 ML 실행 | 모델 선택 및 하이퍼 튜닝 자동화 | BigQuery의 SQL로 모델 튜닝을 반복하여 개발 속도 향상 | . | Cloud Datalab 빅쿼리 데이터와 연결된 notebook | . | 클라우드 ML 엔진 HyperTune 으로 모델 최적화 가속: 자동 하이퍼 파라미터 튜닝 서비스 ** | . | Kubeflow Kubernetes용 클라우드 네이티브 ML | 멀티 클라우드 전략을 사용하는 기업에서 쉽게 이동가능하도록 해줌. | 텐서플로, 파이토치 등 즉시 사용할 수 있도록 지원 | Kubernetes가 종속성, 리소스 관리 | 원하는 곳에서 실행. 스왑기능 및 확장 가능. | . | . 4. 고객 사용 사례: 넷마블 . BigQuery - 데이터레이크 . DataPrep - 데이터 전처리, 대강 데이터 간보기 . Jupyter - 피쳐 엔지니어링, 피쳐 셀렉션 진행, . Data flow - 분산처리해서 학습 데이터 생성 . Cloud Storage - 학습 데이터 저장 . ML 엔진 - 학습 정확도 모니터링, 하이퍼 파라미터 튜닝 .",
            "url": "https://inahjeon.github.io/devlog/conference/2018/11/04/google-cloud-summit-review.html",
            "relUrl": "/conference/2018/11/04/google-cloud-summit-review.html",
            "date": " • Nov 4, 2018"
        }
        
    
  
    
        ,"post30": {
            "title": "Deview 2018 Day2 세션 정리",
            "content": "1. 인공지능이 인공지능 챗봇을 만든다 (이재원님) . 슬라이드 들으면서 리모트 하느라 내용을 거의 못 들음. 아쉽.. . 2. 기계독해 QA: 검색인가, NLP인가? (서민준님) . 슬라이드 . QA: Question Answering (궁금한 것이 있을 때 답을 해줄 수 있는 시스템) . 검색으로 찾는 QA (“쿼리” 검색 시) 내용 및 제목의 관련성 =&gt; NLP와 관련! | 비슷한 검색을 한 유저가 읽은 문서 | 웹 사이트 신뢰도 등을 고려하여 알맞은 문서를 전달. | . | (활용하는 기법들) . Word Matching: 검색한 단어가 존재하는 문서를 가져옴 (ctrl-F) . 제목에만 적용할 경우 꽤 효과적임. | 쿼리에 단어가 많아질 경우 성능이 좋지 않음. | . TF-IDF: 중요 키워드(흔하지 않은 단어)에 더 가중치를 중 . LSA: Bag of word . 각 단어에 추상적인 “태그”를 달아줌 | 추상적인 태그를 통해 다른 단어끼리도 비교할 수 있게 됨. | . (검색 != 문장 독해) . 문장을 읽는 것이 아님. 문법적, 의미적 맥락을 파악못함. | 원하는 답을 곡 집어서 가져오기 힘듦. | . NLP로 읽는 QA 문서를 시스템이 읽고 중요한 정보를 요약해서 알려주는 형태 input: 질문, 관련된 문서 (검색 결과) | output: 답변 | . | 답변을 만들때) . 생성모델(Generative model) VS 추출 모델(Extractive model) . 생성모델의 문제점: 서비스 퀄리티 컨트롤이 어려움, 평가도 어려움 | . =&gt; 추출 모델을 사용 . (7 Milestones in Extractive QA) . Task definition) . Sentencel-level QA: 문장 레벨의 답변 (관련 문장들 중 알맞은 문장 선택) | Phrase-level QA: 최소한의 구문으로 답변 . | Dataset: SQuAD | . Models) . Cross-attention: 문서를 읽으면서 질문을 참고, 질문을 읽으면서 문서를 참고하는 방식 | Self-attention: 문서를 읽으면서 문서의 다른 부분을 참고 | Transfer learning: Unlabeled corpus를 활용. Wikipedia(3 billion, unlabeled) -&gt; SQuAD(2 million, labeled) 에 적용 | Super-human level: 사람보다 더 높은 성능(91%)을 내는 모델. 앙상블, NLP툴들(파서 등), 적용. BERT(93%) - google AI | . 검색과 NLP의 접점 문제) 문서를 읽는데 너무 오랜 시간이 걸림 (GPU로 6일) | 관련 문서 찾기) . Solution1: 검색해서 관련 문서를 찾고 해당 문서를 읽음 . 문제점: 검색엔진을 거쳐야 하고, 검색엔진이 잘못된 답을 내면 성능이 떨어짐. | . Solution2: 찾기와 읽기를 동시에 한다. . Q. 검색이 어떻게 문서를 빨리 찾을까, 사람은 정보를 어떻게 빨리 찾을까? . A. 문서들을 미리 범주화 해둠. . Locality-Sensitive Hashing(비슷한 아이템의 충돌을 최대화 함) 사용 | . 문서 -&gt; 구문) . 문서 d, query q가 주어졌을 때, answer a의 확률을 최대화. . 기존: 매 새로운 질문마다 F를 재계산 함. F(a, d, q) | 개선: 따로 나누어서 계산(H는 미리 계산 가능). exp(g(q)) * H(a, d) | . …그 뒤는 생략 . 3. Fast &amp; Accurate Data Annotation Pipeline for AI applications (이정권님) . 슬라이드 . AI를 이루는 세가지 요소: 하드웨어/모델링/데이터 . (어떤 데이터가 필요한가?) . 늘 쌓이는 데이터: 로그 등 | Annotation이 필요한 데이터: 영상, 이미지, 음성 등 비정형 데이터 (딥러닝의 주 어플리케이션) | . (기존의 annotation과정): 사람이 데이터를 보고 라벨링을 하여 전달 ex) 이미지: 박스를 그리고 라벨링 . 문제점: . 인식의 불일치: 사람마다, 같은 사람이라도 같은 데이터를 보고 다르게 인식할 수 있음. | 데이터의 부정확함: 사람의 실수로 인해 라벨링이 부정확하거나 빠지는 문제 | . 아이디어: . 대량생산을 위한 파이프라이닝 | AI로 작업 능률 Boost | 매 작업과정의 검수 | . (파이프라이닝) . 분업하기 . 사람의 인지단계 고려 | AI의 도움을 받을 수 있도록 | 각 단계별 검증 절차가 용이하게 | . 리콜을 높이자 . ex) 주어진 이미지에서 다음 물체들을 모두 찾으세요 VS 다음 영역에 개를 찾으세요. . …생략 . 4. Papago Internals: 모델분석과 응용기술 개발 (신중휘님) . 슬라이드 . 1) Papago MT Engine 팀 역할 소개 . (AI 연구/개발과 서비스) . 모델연구/개발 -&gt; 응용기술 연구/개발 -&gt; 앱/웹 서비스 개발 -&gt; 서버개발 ex) 기계번역 -&gt; 웹 문서 번역 -&gt; 웹문서 -&gt; 최적화 . | 집중하는 것: 품질, 편의성, 최적화 . | . 2) 실무자에게 듣는 기계번역(seq-to-seq) 모델 분석 방법 - 파파고와 함께: 학습과 수렴 (정권우님) . 파파고 기계번역 모델 소개) . Input: 나는 엄마가 좋아. | Encoder: input to vector (vectorization) | Decoder: vector to output (devectorization) | Ouput: I like mom. | . 학습과 수렴 관련 연구) . 목표: 품질 안정성 (신뢰할 수 있는 학습 모델) . 학습 / 검증 / 테스트 데이터로 분리 | 매 epoch마다 검증 데이터 자동 평가 | 개선 폭이 적어지는 지점에서 stop (수렴) | . 기계번역 학습 수렴 기준) . 정확한 수렴 지점 선별 기준을 명시하지 않음. | learning rate 기반의 수렴 기준 선택 | . 문제1) 일반화된 모델 수렴 지점 선택 방법이 없음 (=모델 학습이 어려움) . 일반적인 학습 모델 평가 방법: BLEU | 학습 과정에서 얼마나 모델이 예민하게 반응하는지 확인 (매 iteration마다 BLEU 점수 측정) | . 문제2) BLEU != 품질 . 좋은 품질의 번역이란) | 동일한 의미 (Adequacy) | 자연스러운 문장 (Fluency) | . : 전문가 평가 점수로 비교해봤을 때 BLEU점수와 품질 점수의 상관관계가 낮음. . 3) 실무자에 듣는 기계번역 응용 기술의 연구/개발 - 인공신경망을 활용한 웹사이트 번역 (김재명님) . …생략 . 5. NAVER 광고 deep click prediction: 모델링부터 서빙까지: 김윤중님 . 슬라이드 . 광고시스템: 광고자 / 광고 / 소비자 . 미션: 어떤 사용자에게 어떤 광고를 제공할 것인가? . (user, context) -&gt; ad . 좋은 광고란? . 얼마나 많이 클릭하는가 ex) CTR = clicks/impressions | . (Click Prediction Problem) . Dataset: . (u1, c1, a1, / clicked) . (u2, c1, a1, / not clicked) . .. . 이진 분류문제로 치환 . 광고문제의 특성) . 수많은 사용자들, 수많은 형태의 광고 | 높은 성능 -&gt; 매출 | 광고주, 소비자, 광고플랫폼의 utility가 다 다름 | . 목표: (user, context) -&gt; ad . (Naive Approach) Classifier 사용 . 문제점: 확장성 문제, cold start problem . 확장성 문제해결을 위해 case study) . Taboola, Youtube 등) candidate 추출 (narrow down) -&gt; ranking | google play) 질의어, candidate -&gt; ranking | . (Modeling) . Key idea) . Candidate model | Embedding (feature space가 매우 sparse함) | User, Context, Ad 모두 Embedidng 한 후, Classification 함 . Candidate model 문제: user, context =&gt; candidate Ads . preference, similarity를 loss function에 넣음 . (user, context), (ad)의 similarity . nearest neighbor문제로 품 . (Serving) . online serving에서 classifier prediction 만 수행 Encoder / Classifier 분리 | Prediction 과정 operation 감축 | Prediction input size줄이기 | . Common Request Trick) . 데이터 Prediction 시 duplicated data는 그룹으로 묶어서 prediction . (성능평가) . similarity로 candidate을 뽑았는데, 100, 500, 1000개로 테스트했을 때 CTR이 1.5배 차이남 . -&gt; similarity가 잘 동작하지 않았음. . 원인파악: 두루두루 모든 사용자들에게 인기있는 광고는 유사도가 높게 나오지 않았음. .",
            "url": "https://inahjeon.github.io/devlog/conference/2018/10/12/deview-2018-day2-review.html",
            "relUrl": "/conference/2018/10/12/deview-2018-day2-review.html",
            "date": " • Oct 12, 2018"
        }
        
    
  
    
        ,"post31": {
            "title": "캐글뽀개기 2018 세션 노트",
            "content": "기업 현장에서의 데이터 과학: 조동환님 . 1) SKT DT 조직의 데이터 인프라 raw data(정형데이터/비정형데이터/외부데이터 등) to data lake . data lake : 데이터를 가공하기 전에 모아두는 곳. | . data lake가 데이터 쓰레기장이 되지 않으려면 데이터의 형태, 데이터에 대한 설명 등 잘 관리되어 있어야함. . data ware house: 분석하기 위해 가공된 데이터 저장소. | . data lake to data services . data lake -&gt; feature store -&gt; ml engine -&gt; data analysis | data lake -&gt; fast data store (일부 기간 데이터) -&gt; data service | . 2) Data 기반 조직으로 변화 시키기 . 조직이 data 기반으로 움직이도록 하기 위해서 변화를 어떻게 이끌어 낼 것 인지가 매우 중요! . (2)번 내용이 더 중요했는데, 필기를 많이 못했음..ㅜ) . Mastering Machine Learning with Competitions: 이정윤님 . 왜 kaggle 해야하는지 . Competition 팁들 . validation set을 잘 정의하는 게 중요! | 효율화를 위해 pipeline이 있어야함 (피쳐엔지니어링 / 학습 / submission 등 단계 관리) | . Show me the Kaggle medal: 이유한님 . 캐글 진행 과정: . feature engineering ratio feature : A per B ex) 연금 / 전체수입 | product feature: A * B | Addition feature: A + B | Subtraction feature: A + B | Aggregation Feature: category와 numerial feature의 조합 | category feature: one-hot / label encoding / lightgbm (가장 성능이 좋다고함) (lightgbm-built-in: missing value는 알아서 처리함) | . | Feature Selection Feature 2300개에서 feature importance / target 과의 correlation으로 1000개 정도로 줄임. . | Training Strategy 여러 모델들 Ensemble | 각 모델들을 피쳐로 해서 다시 학습 | . | 다른 참가자와 합치기 (stacking) | 그 이후 성능을 더 올리고 싶다면? . feature generation | parameter tuning | more stacking | . Hyper parameter tuning 모델 파라미터 최적화 | Lessions learned: . Lession1 - Feature generation 과 cross-validation은 함께 해야한다. (leader board와 로컬 cv가 다를 수 있음) | . Featue generation -&gt; cv -&gt; 성능향상 되었는지? -&gt; feature set update . Lession2 - 모든 결과를 기록해야한다. 날짜 / feature / 설명 / 성능 / parameter | . 중요한 팁: . 가장 중요: stable trustworthy validation set 을 준비하는 것이 필수 ** | 잘하는 사람 커널 따라해보고 커널 만들어 보는 게 중요! | . 이후: . Google Competition: Gstore 의 유저 revenue 예측 참가중.. . 2%의 매출이 나오는 주요 고객들을 분류하는 것이 중요. | . 머신러닝을 위한 수학 - 최적화: 이규영님 . 최적화 . 어떤 제약 조건하에서 목적 함수 f(x)에 대해 함수를 최소/최대화 하는 x값을 찾는 문제 . 최적화의 분류) . deterministic vs stochastic deterministic optimization: 확정적 최적화 | . | 각 파라미터들은 고정된 값 . 머신러닝에서 사용되는 대부분의 최적화 방법들 . stochastic: 확률적 최적화 | . 각 파라미터들은 고정된 값이 아닌 확률분포를 갖는다고 가정 . 대표문제: 강화학습, 마르코프모델 . 변수의 형태: Continuous VS Discrete VS Mixed . | 함수, 제약조건의 형태: Linear Vs Non-linear . 선형(linear 1차함수) | one or more nonlinear 2차 이상 함수 | . | 변수, 함수의 형태에 따라) . Linear Programming: 목적함수와 제약식 모두가 선형인 최적화 . | Non-linear Programming: 목적함수와 제약식 중 단 하나라도 선형이 아님 (두번 미분이 가능해야함) . | . ** Convex: 증가함수 인가 . 유일한 global minima가 있음. | local minima = global minma | ex) SVM (Quadratic programming으로 최적화 문제를 풀 수 있음) | . ** Non convex . gradient descent algorithm으로 풀 수 있음 | neural network / gradient-descent / random serach / model-based search 등 . | Integer Programming: 제약조건에 정수 조건이 들어감 물류 최적화 등에 사용 . | Integer Non-linear Programming | Mixed Integer Programming | Mixed Non-linear Programming | . Stochastic optimization . 목적함수 또는 제약식에 확률분포가 들어감 | ex) 강화학습 | . [Stochastic optimization 문제를 푸는 방법들] . Stochastic gradient descent . 휴리스틱 최적화: 한정된 시간에서 최적의 답을 찾는 것이 아니라 만족할 만한 수준의 해법을 찾는것 (deterministic, stochastic / discrete or continous 모두 적용가능) . Genetic algorithm: 적자 생존의 아이디어 | 시뮬레이티드 어닐링: 물리적인 담금질 기법 | 안티콜로니시스템: 개미생태계 | . 캐글을 위한 캐글: 정권우님 . (live 캐글 대회에서 메달을 따기 위한 과거 캐글 경진대회 공부법) . [지금 무엇을 해야할까?] . 입문강의 (모두를 위한 머신러닝 / fast.ai 등)를 들었는가 . 코딩을 할 줄 아는가 . =&gt; 그럼 캐글! . [bottom up(이론부터 차근차근) VS top down(일단 그냥 먼저 해보는거)] . 경진대회 시작 / keras / VGG16 / adam optimization 사용 -&gt; leader board | 이론들을 문제를 풀기 위해 필요한 수준까지만 이해하고 바로 적용 | . Q. 무슨 경진대회부터 시작할까? . 타이타닉 해본 후 Live 경진대회 =&gt; 과거 경진대회 중 관심있는 데이터를 다루는 경진대회를 공부! | . 과거 경진대회 중 어떤 것? . 1년 내외 최근 | 참여자가 1000명 이상 | 상위 입상자 코드가 내가 사용하는 언어로 되어있는지 | . 처음 도전했던 경진대회 . 산탄데르 - Top 19% | state farm - Top 14% | 좌절 후 액션플랜) . 캐글 블로그에서 과거 경진대회의 승자 인터뷰 정독 및 번역하기 | 과거 경진대회 상위코드 재현하기 | 혼자서 과거 경진대회 참여하기 (6개월간 2개 정도 공부) | . 과거 경진대회의 장점) . 시간 제약이 없음 | Submission 제약이 없음 | 경쟁이 없음 | . 과거 경진대회 공부 방법) 1) EDA 2) Baseline Code (기초 파이프라인 개발) - 시작점 3) Experiments 4) 상위 입상자 코드 재현 . 그리고) 3번째 산탄데르 제품 추천 문제 - Top 10%, 동메달 . Q. 언제부터 팀 빌딩을 하고 문제를 같이 풀어보는게 좋을지? . 10% 정도는 스스로 할 수 있을 단계까지 먼저 도달하고, 팀 구성해서 협업해보는 것이 좋음. | .",
            "url": "https://inahjeon.github.io/devlog/conference/2018/10/07/kaggle-2018-review.html",
            "relUrl": "/conference/2018/10/07/kaggle-2018-review.html",
            "date": " • Oct 7, 2018"
        }
        
    
  
    
        ,"post32": {
            "title": "캐글뽀개기 2018 후기",
            "content": "광화문 마이크로소프트에서 진행하는 캐글뽀개기2018 컨퍼런스에 다녀왔다. . 머신러닝을 시작하고 나서 캐글을 시작해보려다가 방향을 잘 못잡고 포기해본 전적이 있어서, 다른 분들은 과연 어떻게 시작했고 또 좋은 성적을 거둘 수 있었는지 궁금했다. . 그래서 이번 캐글 컨퍼런스에서 발표하는 세션들을 많이 기대했었는데, 기대만큼 세션들이 다 너무 좋았던 것 같다. . 입상한 캐글러 분들의 시행착오, 노하우를 들으면서 처음 시작할 때 어떻게 시도해보는게 좋은지, 나중에 점수를 올리려면 어떻게 접근 해야하는지 대략적인 감이 오는 것 같다. 물론 실제로 해보면 또 엄청난 시행착오를 겪겠지만… 그래도 다시 캐글을 시작해봐야겠다는 욕구가 마구 샘솟는다. . 아침에 힘겹게 일어나서 광화문까지 갈 때는 내가 무슨 부귀영화를 누리려고 주말에도 학회를 신청했었지 했었는데, 게으름 안부리고 참석하길 잘했다. 뿌듯! 내 자신 칭찬해 :) .",
            "url": "https://inahjeon.github.io/devlog/diary/2018/10/07/diary20181008.html",
            "relUrl": "/diary/2018/10/07/diary20181008.html",
            "date": " • Oct 7, 2018"
        }
        
    
  
    
        ,"post33": {
            "title": "컨퍼런스 참가",
            "content": "최근에 개발, Data Science 관련 학회에 참석하면서 새로운 아이디어를 많이 얻고, 시야가 넓어지게 되어서 큰 규모의 컨퍼런스에는 부지런히 참석하려고 노력하는 중이다. . 그러다가 10월은 어쩌다 보니 거의 매주 컨퍼런스에 참석하게 된 것 같다. 가을에 왠지 컨퍼런스 일정들이 다 몰려있다. . 10월 7일 캐글뽀개기2018 | 10월 12일 DEVIEW2018 | 10월 19일 데이터야놀자2018 - 발표 | 10월 25일 Google Cloud Summit | 11월 10일 GDG DevFest Seoul 2018 - 발표 | . 그 중에서 데이터야놀자, DevFest는 회사 내부 기술 컨퍼런스에서 발표했던 내용을 기반으로 발표자로 한번 신청해보았는데, 뜻밖에도 둘 다 선정되어서 연사로 참여하게 되었다. ‘ㅁ’…ㄷ . 첫 외부 발표라 잘 할 수 있을까 걱정이 되는데, 동시에 나에게 너무 좋은 경험이 될 것 같아서 기대가 된다. . 알찬 내용으로 열심히 준비해야지 :) .",
            "url": "https://inahjeon.github.io/devlog/diary/2018/09/30/diary20180929.html",
            "relUrl": "/diary/2018/09/30/diary20180929.html",
            "date": " • Sep 30, 2018"
        }
        
    
  
    
        ,"post34": {
            "title": "edwith 자연어처리 강의",
            "content": "자연어처리 쪽에 관심이 많이 생겨서 공부해보려고 했는데, 마침 edwith에서 진행하는 딥러닝을 이용한 자연어처리 강의가 있길래 수강 신청했다. . 강의 내용을 보니까 자연어처리 내용 들어가기 전에 기본적인 머신러닝 이론에 대한 내용들도 있어서 까먹었던 내용들 다시 한번 복습하고 가도 좋을 것 같다. . 스탠퍼드에서 하는 자연어처리 강의를 많이 추천해서 들어보려다가 영어에 대한 부담감으로 망설이고 있었는데, 한글 강의 너무 좋다…ㅠ 다 듣고 스탠퍼드 강의 들어봐야지. . 자연어처리 말고도 관심있는 주제들이 많아서 일단 다 신청해놓았다. . 인공지능을 위한 선형대수 | 논문으로 시작하는 딥러닝 | 딥러닝을 이용한 자연어처리 | . 일단은 이 3개만 먼저 들어봐야지. .",
            "url": "https://inahjeon.github.io/devlog/diary/2018/09/26/diary20180926.html",
            "relUrl": "/diary/2018/09/26/diary20180926.html",
            "date": " • Sep 26, 2018"
        }
        
    
  
    
        ,"post35": {
            "title": "블로그 시작",
            "content": "새벽 1시, 블로그 시작 하기 좋은 갬성 넘치는 시간이다. . 이번엔 꾸준히 글 써보자. .",
            "url": "https://inahjeon.github.io/devlog/diary/2018/08/30/start.html",
            "relUrl": "/diary/2018/08/30/start.html",
            "date": " • Aug 30, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About Me . 안녕하세요? 저는 뱅크샐러드 서비스를 만들어 나가고 있는 Machine Learning Engineer 전인아입니다. . 데이터마이닝 연구실에서 빅데이터 분석 연구를 하다가, 실제 세상의 데이터를 다루고 분석해 보고싶어 레이니스트에 오게 되었습니다. . 현재는 뱅크샐러드를 사용하는 유저들의 소비패턴 분석, 자연어처리 및 머신러닝 서비스 개발을 담당하고 있습니다. . 지금보다 데이터에서 더 많은 가치를 만들어 낼 수 있기를 꿈꾸고 있습니다. . 근황 . 블로그를 꾸준히 유지하려 노력중입니다. | 맛(+술)집 리스트를 모으고 있습니다 -&gt; Michelin | 데이터 엔지니어링 및 머신러닝 서비스 인프라 구축에 관심이 많습니다. | 노트북 대신 아이패드로 일하기에 도전하고 있습니다. (실패했습니다) | 신의 물방울을 보고 와인에 빠졌습니다. | 효율화/자동화에 관심이 많습니다. | 운동을 해야할 것 같습니다. | 아침, 점심으로 랩*쉬 먹는데 맛있어요. #요거트맛 | 4K 빔 프로젝터와 PS4를 사고 싶습니다. -&gt; 질렀습니다 WoW! | 리얼포스 키보드 질렀어요. 오예! 개발은 장비빨 | 1일 1commit | 첫 마라톤 완주! #마블런2019 | . 관심사 . #와인 #디즈니영화 #비트코인 #생계졸업 #파스타요리 #스테이크잘굽는법 #그림그리기 .",
          "url": "https://inahjeon.github.io/devlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

}